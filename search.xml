<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>DETR</title>
    <url>/2022/03/04/DETR/</url>
    <content><![CDATA[<p>论文：[<a href="https://arxiv.org/abs/2010.04159">2010.04159] Deformable DETR: Deformable Transformers for End-to-End Object Detection (arxiv.org)</a></p>
<span id="more"></span>
<h1 id="前人的工作"><a href="#前人的工作" class="headerlink" title="前人的工作"></a>前人的工作</h1><h2 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h2><p>注意力机制可以理解为，计算机视觉系统在模拟人类视觉系统中可以迅速高效地关注到重点区域的特性。<br>$$<br>Attention&#x3D;f(g(x),x)<br>$$<br>g(x)表示对输入特征进行处理并产生注意力的过程，f(g(x),x)表示结合注意力对输入特征进行处理的过程</p>
<p>self-attention模型：<br>$$<br>Q,K,V&#x3D;Linear(x)\<br>g(x)&#x3D;Softmax(QK)\<br>f(g(x),x)&#x3D;g(x)V<br>$$<br>senet模型:<br>$$<br>g(x)&#x3D;Sigmoid(MLP(GAP(x)))\<br>f(g(x),x)&#x3D;g(x)x<br>$$</p>
<p>注意力又可以细分为：通道注意力、空间注意力、时间注意力、分支注意力以及两种组合注意力：通道-空间注意力、空间-时间注意力</p>
<p><strong>通道注意力：</strong>将输入的特征图，经过<strong>基于宽度与高度</strong>的global max pooling 和global average pooling，然后分别经过MLP。将MLP输出的特征进行基于element-wise的加和操作，再经过sigmoid激活操作，生成最终的channel attention featuremap。将该channel attention featuremap和input featuremap做element-wise乘法操作，生成Spatial attention模块需要的输入特征。</p>
<p><strong>空间注意力：</strong>将Channel attention模块输出的特征图作为本模块的输入特征图。首先做一个<strong>基于channel</strong>的global max pooling 和global average pooling，然后将这2个结果基于channel 做concat操作。然后经过一个卷积操作，降维为1个channel。再经过sigmoid生成spatial attention feature。最后将该feature和该模块的输入feature做乘法，得到最终生成的特征。</p>
<p>Transformers网络包含self-attention和cross-attention机制，其主要的问题是时间开销、内存开销过高。现有许多思路来解决这个问题</p>
<ol>
<li>使用预定义的稀疏注意力模式，最直接的范式就是将注意力模式限制到固定的局部窗口。而这种方法会丧失全局信息。为了补偿对全局信息的提取，可以增加关键元素的接受域，或是允许少量特殊令牌访问所有关键元素，或是添加一些预定义的稀疏注意模式，直接注意远处的关键元素</li>
<li>学习数据依赖的稀疏注意力，基于注意力的局部敏感数据哈希算法，将查询和关键元素散列到不同的容器中，或是使用k-means找到最相关的关键元素，或是学习block-wise稀疏注意力的block排序</li>
<li>探索自我注意力的低秩性质，通过尺寸维度而不是通道维度的线性投影来减少关键元素数量，或是通过内核化近似重新计算自注意力</li>
</ol>
<p>本篇论文使用的可变性注意力是受可变性卷积启发，属于第二类，只关注从查询元素的特征中预测一个小的固定采样点集合。在相同FLOPS下，变形注意力要比传统卷积略慢。</p>
<h2 id="目标检测的多尺度特征表示"><a href="#目标检测的多尺度特征表示" class="headerlink" title="目标检测的多尺度特征表示"></a>目标检测的多尺度特征表示</h2><p>在目标检测任务中，一张图像内真实对象的尺寸差别巨大，这也成为目标检测的一大困难。现代物体检测器通常利用多尺度特征来解决。FPN提出一个自顶向下路径融合多尺度特征；PANet进一步添加一条自底向上的路径到FPN顶部；或是结合通过一个全局注意力操作提取出的从所有尺寸中的特则；或是使用U型模型来融合多尺度特征。最近，NAS-FPN、Auto-FPN提出通过神经网络搜索自动设计交叉注意力联系；BiFPN是PANet的重复简化版本</p>
<p>本篇论文使用多尺度可变性的注意力模块可以通过注意力机制自然的将多尺度特征累加起来，无需借助特征金字塔网络</p>
<h2 id="Transformers中的多头检测"><a href="#Transformers中的多头检测" class="headerlink" title="Transformers中的多头检测"></a>Transformers中的多头检测</h2><p>Transformers是基于机器翻译的注意力机制的网络架构。为了使模型能够关注不同表示子空间和不同位置的内容，将不同注意力头的输出以可学习的权值线性聚合。</p>
<h2 id=""><a href="#" class="headerlink" title=""></a></h2><p>DETR基于Transformer encoder-decoder框架，合并了set-based 匈牙利算法，通过二分图匹配，强制每一个ground-truth box都有唯一的预测结果（通过该算法找优化方向，哪个ground-truth由哪个slot负责）</p>
<h2 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h2><ol>
<li>DETR训练周期长，到达收敛状态的时间长，初始化时，图上各个位置的权重相同，而在训练结束时，权重只集中在图像中出现物体的位置，这里权重的更新似乎需要经过很多轮训练才能达到收敛</li>
<li>对小目标物体检测不友好，DETR使用多尺度特征处理小目标，而高分辨率的特征图会大大提高DETR复杂度</li>
</ol>
<h2 id="关键过程"><a href="#关键过程" class="headerlink" title="关键过程"></a>关键过程</h2><ol>
<li>通过CNN骨干网络将输入特征提取出来。DETR利用标准的Transformer编码器-解码器体系结构将输入特征映射转换为一组对象查询的特征。在目标查询特征(由解码器产生)上添加一个三层前馈神经网络(FFN)和一个线性投影作为检测头。</li>
<li>对于DETR的编码器，查询和关键元素都是特征图中的像素。编码器输入是ReaNet特征图，自注意的计算复杂度为O(h^2w^2c)，随空间大小呈二次型增长。</li>
<li>对于DETR解码器，输入包括编码器中的特征图和N个由可学习位置嵌入表示的对象查询。解码器中存在两类注意模块，即<strong>交叉注意模块</strong>和<strong>自我注意模块</strong>。在交叉注意模块中，对象查询从特征映射中提取特征。查询元素是对象查询的元素，关键元素是编码器的输出特征映射的元素。复杂度随特征映射的空间大小呈线性增长。在自注意模块中，对象查询相互交互，以捕获它们之间的关系。查询和关键元素都是对象查询。因此，对于适度数量的对象查询，复杂性是可以接受的。</li>
</ol>
<h1 id="论文概述"><a href="#论文概述" class="headerlink" title="论文概述"></a>论文概述</h1><p><img src="/%5Cimages%5Cimage-20220119220603117.png" alt="image-20220119220603117"></p>
<h2 id="端到端适用于目标检测的可变性transformers模型"><a href="#端到端适用于目标检测的可变性transformers模型" class="headerlink" title="端到端适用于目标检测的可变性transformers模型"></a>端到端适用于目标检测的可变性transformers模型</h2><p>模型使用ResNet-50作预训练</p>
<h2 id="可变性注意力模块"><a href="#可变性注意力模块" class="headerlink" title="可变性注意力模块"></a>可变性注意力模块</h2><p>不同于传统注意力模块注意图像中的所有位置，可变性注意力模块只关注参考点周围一小组关键采样点，无需考虑特征图的空间大小，即为每个查询只分配少量固定数量的关键点。<br>$$<br>DeformAttn(z_q,p_q,x)&#x3D;\sum^M_{m&#x3D;1}W_m[\sum^K_{k&#x3D;1}A_{mqk}\times W^{‘}<em>mx(p_q+\Delta p</em>{mqk})],<br>$$<br>输入特征是C×H×W维的；q表示具有Zq个上下文特征、二维参考点Pq的查询元素；m表示注意力头；k表示采样的关键点的索引，K为采样关键点的总数，Δp和A分别为检测头的采样偏移量和权重</p>
<h2 id="多尺度可变性注意力模块"><a href="#多尺度可变性注意力模块" class="headerlink" title="多尺度可变性注意力模块"></a>多尺度可变性注意力模块</h2><p>$$<br>MSDeformAttn(z_q,\hat{p_q},{x^l}^L_{l&#x3D;1})&#x3D;\sum^M_{m&#x3D;1}W_m[\sum^L_{l&#x3D;1}\sum^K_{k&#x3D;1}A_{mlqk}\times W^{‘}<em>m x^l (\phi_l(\hat{p_q})+\Delta p</em>{mlqk})],<br>$$</p>
<p>x是多尺度特征输入图，l是输入特征的等级，k为采样点。多尺度变形注意与之前的单尺度版本非常相似，不同的是它从多尺度特征映射中采样LK点，而不是从单尺度特征映射中采样K点。Φ将归一化坐标重新转换为第l层的特征图。</p>
<p>可变形卷积是为单尺度输入而设计的，每个注意力头只关注一个采样点。然而，多尺度变形注意从多尺度输入中查看多个采样点。所提出的(多尺度)可变形注意模块也可以被视为Transformer注意的有效变体，其中可变形采样位置引入了<strong>预滤波机制</strong>（预先过滤不重要的点，降低计算复杂度）。当采样点遍历所有可能的位置时，所提出的注意模块相当于Transformer注意。</p>
<h2 id="可变性transformer编码器"><a href="#可变性transformer编码器" class="headerlink" title="可变性transformer编码器"></a>可变性transformer编码器</h2><p>我们用提出的多尺度可变形注意模块替换DETR中的Transformer注意模块处理特征映射。该编码器的输入和输出都是具有相同分辨率的多尺度特征图。关键元素和查询元素都是多尺度特征图中的像素。对于每个查询像素，参考点就是它本身。为了确定每个查询像素所处的特征级别，除了位置嵌入之外，我们还在特征表示中添加了尺度级嵌入(记作el)。与固定编码的位置嵌入不同，尺度级嵌入{el}Ll&#x3D;1是随机初始化并与网络联合训练的。可变形变压器编码器的参数在不同的特征层之间共享。</p>
<h2 id="可变性transformer解码器"><a href="#可变性transformer解码器" class="headerlink" title="可变性transformer解码器"></a>可变性transformer解码器</h2><p>解码器中存在交叉注意模块和自注意模块。两种类型的注意模块的查询元素都是对象查询。在交叉注意模块中，对象查询从特征映射中提取特征，其中关键元素是编码器的输出特征映射。在自注意模块中，对象查询相互交互，其中的关键元素是对象查询。（这里和DETR网络相似）由于提出的变形注意模块是为处理卷积特征映射作为关键元素而设计的，所以只将每个交叉注意模块替换为多尺度变形注意模块，而保持自我注意模块不变。</p>
<p>由于多尺度可变形注意模块提取参考点周围的图像特征，通过让检测头相对于参考点的偏移量预测边界盒，可以进一步降低优化难度。</p>
<p>通过将DETR中的Transformer注意模块替换为可变形注意模块，建立了一个高效、快速收敛的检测系统，称为可变形DETR</p>
<h2 id="迭代边界框优化"><a href="#迭代边界框优化" class="headerlink" title="迭代边界框优化"></a>迭代边界框优化</h2><p>为了提高检测性能，作者建立了一种简单有效的迭代边界盒优化机制。这里，每个解码器层都根据前一层的预测来细化边界框。</p>
<h2 id="两阶段可变性DETR"><a href="#两阶段可变性DETR" class="headerlink" title="两阶段可变性DETR"></a>两阶段可变性DETR</h2><p>在原始DETR中，解码器中的对象查询与当前图像无关。作者使用两阶段目标检测思想，第一阶段使用可变性DETR生成区域建议，生成的区域建议将作为对象查询提供给解码器以进一步细化，形成一个两阶段的可变形DETR。</p>
<p>在第一阶段，为了实现高召回建议，多尺度特征图中的每个像素都将作为对象查询。然而，直接将对象查询设置为像素会给解码器中的自注意模块带来不可接受的计算和内存开销。为了避免这个问题，去掉了解码器，并形成了一个只有编码器的可变形DETR来生成区域建议。即每个像素被赋值为一个对象查询，直接预测一个边界框。得分最高的边界框被选为区域建议。在将区域建议提交到第二阶段之前，不应用NMS。</p>
<h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p><img src="/%5Cimages%5Cimage-20220120132237764.png" alt="image-20220120132237764"></p>
]]></content>
      <tags>
        <tag>deep learning</tag>
        <tag>Object Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>Fast-RCNN</title>
    <url>/2022/02/25/Fast-RCNN/</url>
    <content><![CDATA[<p>论文：<a href="https://openaccess.thecvf.com/content_iccv_2015/html/Girshick_Fast_R-CNN_ICCV_2015_paper.html">ICCV 2015 Open Access Repository (thecvf.com)</a></p>
<span id="more"></span>

<img src="\images\image-20220105165859553.png" alt="image-20220105165859553" style="zoom:150%;" />



<h1 id="论文概述"><a href="#论文概述" class="headerlink" title="论文概述"></a>论文概述</h1><p>Fast-RCNN是将RCNN与SPPNet相融合提出的一种新的网络。</p>
<h2 id="RoI层"><a href="#RoI层" class="headerlink" title="RoI层"></a>RoI层</h2><p><img src="/%5Cimages%5Cimage-20220105171915109.png" alt="image-20220105171915109"></p>
<p>Fast-RCNN设计了一个RoI层用来对<strong>整张图像</strong>进行特征提取，再根据候选区域在原图中的位置挑选特征。</p>
<p>SPPNet提出了Spatial pyramid pooling技术可以将一张图像中卷积所得<strong>的不同尺寸的ROI映射为固定范围</strong>（H×W）的特征图，其中H、W是超参数。每个RoI都是由一个矩形窗口转化为一个卷积特征图。RoI有一个四元组（r,c,h,w）定义（r，c指定左上角坐标；h、w定义其高度和宽度）。</p>
<p><em>RoI max pooling工作原理：将h×w的RoI窗口分割为一个H×W网格，每个网格窗口大小为(h&#x2F;H)×(w&#x2F;W),每个子窗口值最大池化到相应的输出网格单元中。</em></p>
<h2 id="预训练网络"><a href="#预训练网络" class="headerlink" title="预训练网络"></a>预训练网络</h2><p>预训练网络转换为Fast-RCNN网络经过三个转变</p>
<ol>
<li>最后被最大的池化层被RoI层替代，通过设置H、W与网络的第一个全连接层兼容</li>
<li>网络最后的全连接层和softmax被一个全连接层超过K+1类别的softmax和一个特定类别的限定框回归器所代替</li>
<li>该网络接受两个数据输入：一个图像列表和这些图像中的RoI列表</li>
</ol>
<h2 id="微调检测"><a href="#微调检测" class="headerlink" title="微调检测"></a>微调检测</h2><p>在Fast-RCNN网络中，反向传播训练参数是一项重要的功能。（它客服了SPPNet不能更新空间金字塔层以下权值的问题）。由于SPPNet网络的每个训练样本（RoI）来自不同的图像时，SPP层的反向传播是非常低效的。</p>
<p>作者提出了一种更加有效的训练方法，利用了训练过程中的特征共享。即对随机梯度下降进行小批量的分级采样。首先对N幅图像进行采样，然后对每幅图像进行R&#x2F;N个RoI采样。重要的是，对于来自相同图像的RoI在向前和向后传递时共享计算和内存。不过这种训练方法存在训练收敛较慢的问题。（N&#x3D;2，R&#x3D;128时效果最好）</p>
<p>同时，在分层采样中，作者的一个精细调整阶段联合优化softmax分类器和边界框回归器，而不是用三个独立的阶段寻训练softmax分类器、支持向量机、回归器。</p>
<h2 id="多任务损失"><a href="#多任务损失" class="headerlink" title="多任务损失"></a>多任务损失</h2><p>$$<br>L(p,u,t^u,v)&#x3D;L_{cls}(p,u)+\lambda[u\ge 1]L_{loc}(t^u,v)<br>$$</p>
<h2 id="最小批次采样"><a href="#最小批次采样" class="headerlink" title="最小批次采样"></a>最小批次采样</h2><p>实验中，作者使用最小batch为128，对每张图像采样64个RoI。接着从这些proposals中获取区IoU大于等于的0.5的RoI(这些RoI中包含对象标记，u大于等于1)，剩余的RoI从IoU处于[0.1~0.5]的RoI中选取。再将IoU小于0.1的当作背景例子，标记u&#x3D;0.</p>
<p>这里也可以将IoU小于0.1的样本作为hard样本进行训练；训练过程中，图像以0.5的概率翻转，除此没有别的数据增强。</p>
<h2 id="通过RoI池层的反向传播"><a href="#通过RoI池层的反向传播" class="headerlink" title="通过RoI池层的反向传播"></a>通过RoI池层的反向传播</h2><h2 id="SGD超参数"><a href="#SGD超参数" class="headerlink" title="SGD超参数"></a>SGD超参数</h2><p>用于softmax分类和边界框回归的全连接层使用标准偏差为0.01和0.001的零均值高斯分布初始化zero-mean Gaussian distributions。所有层的加权学习率为1，偏差学习率为2，整体学习率为0.001</p>
<h2 id="尺度不变性"><a href="#尺度不变性" class="headerlink" title="尺度不变性"></a>尺度不变性</h2><p>作者探索了两种不同的尺度不变目标检测的实现，一是暴力计算，二是使用图像金字塔</p>
<ol>
<li>暴力计算：在训练和测试中，每幅图像都按照预先定义的像素大小进行处理</li>
<li>图像金字塔：在测试过程中，使用图像金字塔对每个目标建议进行近似尺度归一化，作为数据增强的一种。</li>
</ol>
<p><img src="/%5Cimages%5Cimage-20220106111519294.png" alt="image-20220106111519294"></p>
<p>作者通过实验发现，卷积网络擅长直接学习尺度不变，并且多尺度方法耗费了大量的时间成本只将mAP提升了很小一部分</p>
<h2 id="目标检测过程"><a href="#目标检测过程" class="headerlink" title="目标检测过程"></a>目标检测过程</h2><p>Fast-RCNN网络一旦完成微调，检测过程相当于运行一个前向传递（假设proposal已经提提前算好）。网络将一个图像和一个R对象列表（通常为2k左右）作为输入进行评分。当使用图像金字塔后，每个RoI接近224×224个像素</p>
<p>对于每个RoI r，前向传递输出一个类别的后验概率分布p与一组相对于r 的预测边界框位置偏移。然后使用RCNN算法对每个类单独执行非极大值抑制。</p>
<h2 id="截断SVD"><a href="#截断SVD" class="headerlink" title="截断SVD"></a><strong>截断SVD</strong></h2><p>对于整张图像的分类，全连接层的计算时间要比卷积层处理的时间更少。但是，对于检测来说，由于需要处理的RoI数量很大，几乎一半的前向传递时间都花费在计算全连接层上。因此，作者使用truncated SVD技术来加速全连接层的计算。truncated SVD可以减少30%的探测时间，而只减少0.3%的mAP，并且无需在模型压缩后执行额外的微调。在深层的网络中，对RoI池化层的训练是非常有必要的。</p>
<p>同时，作者提到在深度较浅的网络中，conv1是处理一般性的与任务独立的。因此对conv1训练的意义不大。对卷积层的更新会增加训练时间，并且导致GPU内存溢出的问题，而带来的mAP提升很少。</p>
<p><img src="/%5Cimages%5Cimage-20220106100757776.png" alt="image-20220106100757776"></p>
<p>SVD原理可参考<a href="https://www.cnblogs.com/pinard/p/6251584.html">奇异值分解(SVD)原理与在降维中的应用 - 刘建平Pinard - 博客园 (cnblogs.com)</a></p>
<h2 id="更多的训练数据有利于提升mAP"><a href="#更多的训练数据有利于提升mAP" class="headerlink" title="更多的训练数据有利于提升mAP"></a>更多的训练数据有利于提升mAP</h2><p>作者将VOC07加入VOC12后，mAP由66.9%提升到70.0%。</p>
<p>作者构建了有VOC07的训练集与测试集与VOC12的训练集组成的图像数据集，发现mAP也得到了提升。</p>
<h2 id="softmax相比于SVM的优势"><a href="#softmax相比于SVM的优势" class="headerlink" title="softmax相比于SVM的优势"></a><strong>softmax相比于SVM的优势</strong></h2><p><img src="/%5Cimages%5Cimage-20220106112058344.png" alt="image-20220106112058344"></p>
<p>实验表明softmax在S、L、M三种网络中性能均优于SVM。尽管性能提升不大，但是“one-shot”微调已经足够了。不同于一对一的SVM，softmax在评估RoI时还引入了类别间的竞争。</p>
<h2 id="proposals数量对结果的影响"><a href="#proposals数量对结果的影响" class="headerlink" title="proposals数量对结果的影响"></a><strong>proposals数量对结果的影响</strong></h2><p><img src="/%5Cimages%5Cimage-20220205113749198.png" alt="image-20220205113749198"></p>
<p>作者使用了两种目标检测方法。一是稀疏集proposal（如selective search ）；二是稠密集proposal（如DPM）。</p>
<p>对稀疏peoposal进行分类是一种级联，会先拒绝大量候选区域，只留一小部分给分类器评估。而应用DPM检测时，级联提高了检测精度。</p>
<p>实验表明，随着proposal数量增加，mAP会先上升再下降。所以过多的proposal对分类器并没有帮助。</p>
<p>作者在文章中还介绍了一种测量对象proposal质量的技术：平均召回AR。当每幅图像使用固定数量的proposal时AR与mAP有良好的相关性。但是在图像proposal数量不同时，由于更多proposal而提高的AR，并不意味着mAP会提高。但是在M网络中训练与测试只需要2.5小时，这使得fast-rcnn能够高效、直接评估对象proposal的mAP。</p>
<h1 id="做出的贡献"><a href="#做出的贡献" class="headerlink" title="做出的贡献"></a>做出的贡献</h1><ol>
<li>比RCNN与SPPNet更高的探测质量mAP</li>
<li>训练是单阶段的，使用了multi-task loss</li>
<li>训练时可以对网络中的所有层都进行更新</li>
<li>特征缓存时不需要磁盘空间，Fast-RCNN网络将特征提取器、分类器、回归器合并，使得训练过程不需要再将每阶段结果保存磁盘单独训练，可以一次性完成训练，加快了训练速度</li>
</ol>
]]></content>
      <tags>
        <tag>deep learning</tag>
        <tag>Object Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>Faster-RCNN</title>
    <url>/2022/03/04/Faster-RCNN/</url>
    <content><![CDATA[<p><strong>论文：</strong><a href="https://proceedings.neurips.cc/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks (neurips.cc)</a></p>
<span id="more"></span>
<h1 id="论文概述"><a href="#论文概述" class="headerlink" title="论文概述"></a>论文概述</h1><img src="\images\image-20220206101136285.png" alt="image-20220206101136285" style="zoom:75%;" />

<p>目前，先进的目标检测算法都是依赖于区域建议算法去假设物体位置的，区域建议的计算成为目标检测算法的瓶颈。而Faster-RCNN引入了<strong>区域建议网络RPN</strong>，该网络与检测网络共享全图像卷积特征，从而实现<strong>几乎无成本的区域建议</strong>。引入RPN网络也使得Faster-RCNN成为了一个完全的end-to-end的CNN目标检测模型。</p>
<h2 id="区域生成网络RPN"><a href="#区域生成网络RPN" class="headerlink" title="区域生成网络RPN"></a>区域生成网络RPN</h2><p>RPN网络代替了Fast-RCNN模型中的selective search（SS）。它先通过对应关系把特征图的点映射回原图，在每个对应的原图设计不同的固定尺度窗口bbox，根据该窗口的ground truth的IoU给它标记正负标签，让它学习里面是否有目标。通过一个简单的交替优化，RPN和Fast-RCNN可以共享卷积特征。为了使RPN与Fast-RCNN相统一，作者提出了一个训练方案，该方案在区域建议任务和目标检测任务间交替进行微调，同时保证proposal的稳定。这种方案收敛很快，并且使两个网络可以共享数据。而SS是使用CPU计算，因而计算时间较慢，也没法共享计算。</p>
<p>在RPN中只需要找出物体的大致地方，因此作者对bbox做了三个固定：固定尺度变化（三种尺度）、固定scale ratio变化（三种ratio），固定采样方式（只在特征图的每个点在原图中的对应RoI上采样）</p>
<p>作者在Fast-RCNN的基础上增加了两个卷积层来构建RPN：一个将每个conv map编码到一个短的特征向量上，另一个卷积层在每个conv map上输出客观性的评分以及关于k个涉及到不同的尺度和纵横比的区域建议的回归边界（通常取k为9）</p>
<h2 id="RPN工作原理"><a href="#RPN工作原理" class="headerlink" title="RPN工作原理"></a>RPN工作原理</h2><p><img src="/%5Cimages%5Cimage-20220106213651479.png" alt="image-20220106213651479"></p>
<p>RPN以一副图像（任意大小）作为输入并输出一组矩形对象建议并给出得分。为了生成区域建议，一个小network滑过最后一个共享的卷积层输出的特征图。该网络最后全连接到输入的卷积特征图的一个n×n的空间窗口中。每个滑动窗口都映射到一个低维向量上。这个向量被提供给两个同级的全连接层（边界框回归层reg、分类层cls）。</p>
<p>该体系结构由一个n×n的卷积层、和两个兄弟级的1×1层（reg、cls）并将ReLUs应用于n×n转换层的输出。</p>
<p>RPN最终输出的proposals会出现部分重叠。为了减少冗余，作者根据proposal的cls分数对proposal进行非极大抑制NMS（IoU阈值为0.7）。这使得每幅图像最终有大约2k个proposal。NMS之后，再对前n个proposal进行检测。实验证明，NMS在大幅减少proposal数量的情况下，不会损害最终的检测精度。</p>
<h2 id="Anchor"><a href="#Anchor" class="headerlink" title="Anchor"></a><strong>Anchor</strong></h2><p>Anchor是特征图中的每个点在原图中的对应位置，也就是初始检测框。</p>
<p>在每个滑动窗口位置上，作者同时预测<strong>k个区域proposal</strong>（一个点对应k个anchor），因此reg有4k个输出，来标记k个box的坐标；而cls有2k个输出来评估每个proposal是对象或不是对象的概率。这k个proposal是k个box的参数表示，即上面的Anchors。每个anchor是其滑动窗口的中心位置，并于比例和宽高比相关联。作者使用了3个尺度与3个纵横比，因此每个滑动位置放置9个anchor。一个W×H的卷积特征图就总共有W×H×k个anchor。因为anchor具有<strong>平移不变特性</strong>，对anchor的计算就是对对应该anchor的proposal的计算.和不是平移不变的 MultiBox 方法相比，该方法使参数减少了一个数量级。</p>
<p>作者对每个anchor使用128、256、512三个尺度以及一比一、一比二、二比一三个纵横比，发现即使物体比底层的接受域还要大，该算法仍然可以检测出物体。在实际中，人们也可以只通过观察物体的中间位置来感知物体。这也就作者的解决方案不需要使用多尺度特征或多尺度滑动窗口来预测大的物体，节省大量运行时间。</p>
<p>对于跨越图像边界的anchor。训练中，作者忽略所有跨越图像边界的anchor，以避免带来极大的误差，使得结果无法收敛。在测试中，对于这些anchor，作者只保留其图像中的部分</p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>作者将anchor打上正负标签。正标签：与ground-truth box重叠程度IoU最高的区域；或者IoU大于0.7的anchor。负标签:对于所有的ground-truth box其IoU都小于0.3的anchor。不属于正负样本的样本对于目标检测没有帮助。</p>
<p>根据这些定义，作者最小化Fast-RCNN中的多任务损失目标函数<br>$$</p>

L(\{p_i\},\{t_i\})=\frac{1}{N_{cls} }\sum{L_{cls}(p_i,p_i^{*})}+\lambda\frac{1}{N_{reg} }\sum{p_i^{*}L_{reg}(t_i,t_i^{*})}

<p>$$<br>此处的i为该batch中anchor的索引，pi是anchor为对象的概率，如果anchor为正pi*&#x3D;1，如果anchor为负pi*&#x3D;0；ti为预测边界框四个参数化坐标，ti*是与一个正anchor相关的ground-truth box。</p>
<p>Lcls（分类损失）：使用log loss；Lreg（回归损失）：使用smooth L1(R)，而只有anchor为正时，Lreg才会被激活。<br>$$<br>L_{reg}(t_i,t_i^{<em>})&#x3D;R(t_i,t_i^{</em>})<br>$$<br>作者使用Ncls、Nreg两项进行归一化，并使用lambda作为平衡因子。</p>
<p><img src="/%5Cimages%5Cimage-20220108211045368.png" alt="image-20220108211045368"></p>
<p>在回归时，要对4个坐标参数化。其中x,y,w,h为边界框box的中心位置、宽与高。x，xa，x*是预测box、anchor box、ground-truth box的参数。</p>
<p>不同于其他对任意大小区域进行特征池化（回归权值由所有区域大小共享）的边界回归，作者的回归特征在特征图上具有相同的大小。为了适用于不同的大小，作者设置了一组k边界回归器。每个回归器负责一个尺度和一个纵横比，并且这k个回归器不共享权重。</p>
<h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2><p>RPN网络是以图像为中心的。每个batch中都含有正负样本，对所有参数的优化会使得结果偏向负样本。因此作者随机对256个anchor进行采样，尽量使得正负样本的比例达到一比一（如果正样本小于128再用负样本填充）。</p>
<p>并且作者通过标准偏差为0.01的零均值高斯分布中提取权重来初始化所有层，所有层的初始化从一个ImageNet分类模型中获得，并且调优ZF网络、VGG的jconv3_1以节省内存</p>
<h2 id="将RPN应用于目标检测"><a href="#将RPN应用于目标检测" class="headerlink" title="将RPN应用于目标检测"></a>将RPN应用于目标检测</h2><p>作者将Fast-RCNN与RPN相结合，使得二者的卷积层计算可以共享。由于Fast-RCNN依赖于固定的对象proposal并且如果在学习 Fast R-CNN 的同时更改proposal机制，尚不清楚先验是否会收敛。因此作者没有简单的定义一个包含RPN与Fast-RCNN的网络，而是通过交替优化学习共享特征。</p>
<ol>
<li>使用ImageNet预训练模型初始化，并对区域建议任务做端到端的微调（<strong>训练RPN网络</strong>）</li>
<li>利用第一步生成的区域建议，训练一个独立的检测网络，该检测网络也使用ImageNet预训练模型初始化（<strong>训练FRCNN网络</strong>）</li>
<li><strong>使用检测网络对RPN网络进行初始化，但固定两个网络共有的层，只微调RPN网络特有的层。</strong>此时，两个网络共享卷积层（<strong>训练RPN网络</strong>）</li>
<li>固定共享的卷积层，微调Fast-RCNN的全连接层。（由此形成一个统一的网络）（<strong>训练FRCNN网络</strong>）</li>
</ol>
<p>在这里的训练过程类似于一种“迭代”的过程，不过只迭代了两次，因为迭代次数再增加不会带来性能上的提升。</p>
<h1 id="做出的贡献"><a href="#做出的贡献" class="headerlink" title="做出的贡献"></a>做出的贡献</h1><ol>
<li>设计卷积网络处理区域建议RPN，使得目标检测系统实现完全的end to end</li>
<li>使用多任务损失函数</li>
</ol>
]]></content>
      <tags>
        <tag>deep learning</tag>
        <tag>Object Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>PointNet</title>
    <url>/2022/03/04/PointNet/</url>
    <content><![CDATA[<p>论文：<a href="https://openaccess.thecvf.com/content_cvpr_2017/html/Qi_PointNet_Deep_Learning_CVPR_2017_paper.html">CVPR 2017 Open Access Repository (thecvf.com)</a></p>
<span id="more"></span>
<h1 id="前人的贡献"><a href="#前人的贡献" class="headerlink" title="前人的贡献"></a>前人的贡献</h1><p>传统的目标检测算法中对于数据格式有着严格的要求，将点云数据转换为符合要求的数据格式会使得数据规模扩大，影响计算效率。</p>
<p>点云数据由<strong>无序</strong>的数据点构成一个<strong>集合</strong>来表示。因此，在使用图像识别任务的深度学习模型处理点云数据之前，需要对点云数据进行一些处理。目前采用的方式主要有两种：</p>
<blockquote>
<p>1、将点云数据投影到二维平面。此种方式不直接处理三维的点云数据，而是先将点云投影到某些特定视角再处理，如<strong>前视视角和鸟瞰视角</strong>。同时，也可以融合使用来自相机的图像信息。通过将这些不同视角的数据相结合，来实现点云数据的认知任务。比较典型的算法有MV3D和AVOD。</p>
<p>2、将点云数据划分到有空间依赖关系的voxel。此种方式通过分割三维空间，引入空间依赖关系到点云数据中，再使用3D卷积等方式来进行处理。这种方法的精度依赖于三维空间的分割细腻度，而且<strong>3D卷积</strong>的运算复杂度也较高。</p>
</blockquote>
<h2 id="点集的性质"><a href="#点集的性质" class="headerlink" title="点集的性质"></a>点集的性质</h2><ol>
<li>无序：三维N个点的数据需要N！个排列组合</li>
<li>点之间相互作用：点云中的点不是独立存在的，模型需要提取局部信息</li>
<li>转换不变性：作为一个几何物体，对物体进行变换不应该改变物体的某些特征</li>
</ol>
<h1 id="论文概述"><a href="#论文概述" class="headerlink" title="论文概述"></a>论文概述</h1><p><img src="/%5Cimages%5Cimage-20220116120541325.png" alt="image-20220116120541325"></p>
<p>PointNet主要用于点云数据分类问题，即在点云数据中找到属于一个物体的所有点云。</p>
<p>该分类网络以n个点作为输入，进行输入和特征转换，然后通过最大池法对点特征进行聚合。输出是k个类别的分类分数。分割网络是分类网络的延伸。它连接全局和局部特征，并输出每个分数。” mlp  “表示多层感知器，括号中的数字表示层大小。Batchnorm用于所有带有ReLU的层。在分类网的最后一个mlp中使用了Dropout层。</p>
<p>PointNet更多的是为CV领域提供了一种新的研究方向，拓展了对于原始数据处理的思路，后续的Frustum PoinNet等是对于PointNet在目标检测方面的特定研究，以及其他网络会将其作为网络设计的一部分充分的发挥其网络结构简单，计算简单的特点。</p>
<h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p> 作用于无序输入的对称函数</p>
<p>不改变模型的输入排序有三种方法：将输入按正则序排序（在高维空间中很难实现）；将输入作为序列来训练RNN（会增加训练数据）；使用一个<strong>对称函数</strong>来聚合每个点的信息（如+、*）</p>
<p>作者的主要思想是通过对一个点集中的变换<br>元素应用一个对称函数来逼近一个定义在点集上的一般函数：<br>$$<br>f({x_1,x_2,…,x_n})\approx g(h(x_1),…,h(x_n))\<br>f:2^{\mathbb{R}^N}\rightarrow \mathbb{R} ,h:\mathbb{R} ^N\rightarrow \mathbb{R} ^K,g:\mathbb{R} ^K\times …\times\mathbb{R} ^K\rightarrow \mathbb{R} (g为对称函数)<br>$$<br>根据经验，使用一个多层感知器网络近似h（mlp），使用一个单变量函数和一个最大池化函数组合近似g。通过h，可以学习到f中的不同性质,最后输出一个k维向量，表示数据的全局特征。</p>
<p> 局部全局的数据增强</p>
<p>根据上面网络得到的k维向量，可以训练SVM或是多层感知分类器对具有全局特征进行分类。</p>
<p>在计算全局点云特征向量后，通过连接全局特征和每个点的特征，将其反馈给每个点。接着基于集合的点特征提取每个点的新特征（此时每个点特征同时拥有局部特征与全局特征）。</p>
<p>之后，网络可以基于局部几何特性和全局语义预测每个点的数量。</p>
<p> 联合定位网络</p>
<p>作者通过一个微型网络T-Net预测一个仿射变换矩阵并且直接将这种变换应用到输入点的坐标上。</p>
<p>这种思想同时适用于特征空间的对齐，通过在点特征上插入另一个对齐网络，并预测一个特征转换矩阵来对齐来自不同输入点云的特征。但是特征空间中的变换矩阵比空间变换矩阵的维数高很多，这会增加优化难度。因此在softmax训练损失中增加一个<strong>正则化项</strong>，将特征变换矩阵约束为接近<strong>正交矩阵</strong>，以获得了较好的效果。<br>$$<br>L_{reg}&#x3D;    \Vert I-AA^T    \Vert^2_F(A是由T-Net预测的特征对齐矩阵)<br>$$</p>
<h2 id="理论研究"><a href="#理论研究" class="headerlink" title="理论研究"></a>理论研究</h2><ol>
<li>PointNet神经网络对于连续集函数具有很好的逼近能力。即输入点集的小扰动不会对函数数值造成很大的改变。所以即使在最坏的情况下，网络也可以通过将空间划分为等大小的voxel来探索空间。</li>
</ol>
<p><img src="/%5Cimages%5Cimage-20220116233004972.png" alt="image-20220116233004972"></p>
<ol start="2">
<li>即使输入有数据被损坏或是带有噪声，模型都具有鲁棒性；关键集的数据多少由maxpooling操作输出数据的维度K给出上界</li>
</ol>
<p>因此，该网络通过稀疏的关键点集合来总结一个形状。</p>
<h2 id="关键流程"><a href="#关键流程" class="headerlink" title="关键流程"></a>关键流程</h2><ol>
<li>输入为一帧的全部点云数据的集合，表示为一个n×3的二维 tensor，其中n代表点云数量，3对应xyz坐标。</li>
<li>输入数据先通过和一个T-Net学习到的转换矩阵相乘来对齐，保证了模型的对特定空间转换的不变性。</li>
<li>通过多次mlp对各点云数据进行特征提取后，再用一个T-Net对特征进行对齐。</li>
<li>在特征的各个维度上执行maxpooling操作来得到最终的全局特征。</li>
<li>对分类任务，将全局特征通过mlp来预测最后的分类分数；对分割任务，将全局特征和之前学习到的各点云的局部特征进行串联，再通过mlp得到每个数据点的分类结果。</li>
</ol>
<h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p><img src="/%5Cimages%5Cimage-20220116234754228.png" alt="image-20220116234754228"></p>
<h1 id="做出的贡献"><a href="#做出的贡献" class="headerlink" title="做出的贡献"></a>做出的贡献</h1><ol>
<li>设计一种新的深度网络结构，适用于三维空间中的无序点集</li>
<li>展示这样的网络如何执行三维形状分类、形状部分分割和场景语义解析任务</li>
<li>对这种方法的稳定性和有效性进行深入的实证和理论分析</li>
<li>举例说明网络中选定的神经元三维特征，研究其性能的直观解释</li>
</ol>
]]></content>
      <tags>
        <tag>deep learning</tag>
        <tag>Object Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2022/02/25/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<span id="more"></span>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>RCNN</title>
    <url>/2022/02/25/RCNN/</url>
    <content><![CDATA[<p><strong>论文：<a href="https://openaccess.thecvf.com/content_cvpr_2014/html/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.html">CVPR 2014 Open Access Repository (thecvf.com)</a></strong></p>
<span id="more"></span>

<h2 id="论文概述"><a href="#论文概述" class="headerlink" title="论文概述"></a>论文概述</h2><h3 id="RCNN算法4个步骤"><a href="#RCNN算法4个步骤" class="headerlink" title="RCNN算法4个步骤"></a>RCNN算法4个步骤</h3><ol>
<li>候选区域生成： 一张图像生成1K~2K个候选区域 （采用Selective Search 方法）</li>
<li>特征提取： 对每个候选区域，使用深度卷积网络提取特征 （CNN）</li>
<li>类别判断： 特征送入每一类的SVM 分类器，判别是否属于该类</li>
<li>位置精修： 使用回归器精细修正候选框位置</li>
</ol>
<h3 id="RCNN网络结构（三个模块）"><a href="#RCNN网络结构（三个模块）" class="headerlink" title="RCNN网络结构（三个模块）"></a>RCNN网络结构（三个模块）</h3><ol>
<li>第一模块提出独立类别的区域建议，定义可供检测器使用的候选测试集</li>
<li>第二模块大型卷积神经网络，从每个区域提取固定长度的特征向量</li>
<li>第三模块一组特定类别的线性支持向量机</li>
</ol>
<h3 id="区域建议-region-proposals"><a href="#区域建议-region-proposals" class="headerlink" title="区域建议(region proposals)"></a>区域建议(region proposals)</h3><p>region proposals就是从图像中选取2k个候选区域的过程.</p>
<p>现有的生成策略独立的region proposals： objectness, selective search ,category-independent object proposals , constrained parametric min-cuts (CPMC), multiscale combinatorial grouping </p>
<p>在本篇论文中，作者使用<strong>selective search方法</strong></p>
<p><strong>主要思想</strong>：</p>
<ol>
<li>使用一种过分割手段，将图像分割成小区域 (1k~2k 个)</li>
<li>查看现有小区域，按照合并规则合并可能性最高的相邻两个区域。重复直到整张图像合并成一个区域位置</li>
<li>输出所有曾经存在过的区域，所谓候选区域</li>
</ol>
<p><strong>合并策略</strong>：优先合并以下四种区域：颜色（颜色直方图）相近的；纹理（梯度直方图）相近的；合并后总面积小的： 保证合并操作的尺度较为均匀，避免一个大区域陆续“吃掉”其他小区域；合并后，总面积在其BBOX中所占比例大的： 保证合并后形状规则。</p>
<p>在具体的测试中，对于图像中的所有得分区域，我们应用贪婪的非最大抑制（对每个类独立）。如果该区域与得分高于学习阈值的选定区域有交叉合并（IoU）重叠，则拒绝该区域。</p>
<p>由于所有CNN参数在所有类别中共享，并且与其他常见算法相比，CNN的特征向量是低维的。因此相比与诸如UVA检测系统，CNN网络计算区域建议和特征花费的时间与所耗内存都有极大的优化。并且RCNN可以扩展到数以千计的对象类，而无需借助近似技术。</p>
<h3 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h3><p>作者使用了Krizhevsky等人描述的CNN的<strong>Caffe</strong>实现，从每个区域提取一个<strong>4096维</strong>的特征向量，将一个图像去均值的227  × 227的RGB图像通过5个卷积层和2个完全连通层前向传播来计算特征。因此，必须先将该区域的图像数据转换为与CNN兼容的形式（无论候选区域大小或宽高比如何，直接转换为227*227大小,在warp前作者还会对box进行扩张,使得在wrap处的box中有p个像素）</p>
<p>在实验中,作者发现RCNN可以扩展到数以千计的对象,而无需使用近似技术.</p>
<h3 id="有监督预训练"><a href="#有监督预训练" class="headerlink" title="有监督预训练"></a>有监督预训练</h3><p>作者使用开源的Caffe CNN库来进行预训练.</p>
<h3 id="特定领域微调fine-tuning"><a href="#特定领域微调fine-tuning" class="headerlink" title="特定领域微调fine-tuning"></a>特定领域微调fine-tuning</h3><p>为了使RCNN适应新的任务和新的领域，作者的随机梯度下降SGD训练的CNN参数仅来自于VOC数据集。分类器是随机初始化的21路分类层（VOC中的20个类与背景），其他CNN架构没有改变。</p>
<p>分类器会将IoU大于等于0.5的 region proposals，视为积极的，其余则视为消极的。</p>
<p>SGD学习率为0.001，并且在每次SGD迭代中，使用32个正样本，与96个背景样本，组成一个128大小的batch。同时为了使结果更好预测正样本，采样也偏向正样本。</p>
<h3 id="对象类别分类"><a href="#对象类别分类" class="headerlink" title="对象类别分类"></a>对象类别分类</h3><p>作者在文章中用检测的汽车的例子，如果使用二分类器检测汽车，那么一个紧紧包围汽车的box是正例子，而与汽车无关的背景区域是反面例子。现在的问题在于，我们如何去标记一个部分包含汽车的例子。作者在此使用0.3的IoU阈值，只有大于0.3IoU的区域才是积极。作者同时强调，这个阈值的设置对整个算法结果的影响极大。</p>
<p>同时为了解决训练数据过大的问题，作者使用standard hard negative mining method技术，该技术在实验中，只需要遍历所有图像一次就可以使mAP停止增长。</p>
<blockquote>
<p>standard hard negative mining method：</p>
<p>用hard negative的样本反复训练，初始的样本保证一定的正负样本比例。在每次训练中，将预测为positive的负样本（即hard negative样本）加入负样本训练集中。</p>
</blockquote>
<p>作者在补充材料中说明了使用SVM作为分类器的原因，SVM与CNN对于正负样本的定义不同，导致CNN的分类效果不如SVM。</p>
<h3 id="过滤器First-layerfilters"><a href="#过滤器First-layerfilters" class="headerlink" title="过滤器First-layerfilters"></a>过滤器First-layerfilters</h3><p>作者使用了一种简单的非参数的反卷积方法捕捉有方向的边缘和对立的颜色。</p>
<p>主要思想：在网络中挑选出一个特定的单元（特征），并将其作为自身的对象检测i器。也就是，我们在一个大规模的held-out region proposals上计算单元的激活情况，并按得分由高到低排序，通过执行非极大值抑制nonmaximum suppression，使得被选中的区域“不言自明”。</p>
<h3 id="pool5层"><a href="#pool5层" class="headerlink" title="pool5层"></a>pool5层</h3><p>pool5的 feature map是9216维（6×6×256）的，从实验结果来看仅使用pool5的效果不如加入fc6、fc7效果好。作者认为这是因为目标检测的过程中，一些经过分类调整的特征与形状、纹理、颜色等在全连接层处理过后会更好的将这些特征融合学习。</p>
<p>fc6是pool5的全连接层，为了计算特征，它将4096×9216的权重矩阵乘以pool5的feature map，再添加一个偏差向量。</p>
<p>而fc7则是将fc6的输出作为输入，乘以4096×4096的权重矩阵并添加一个偏差矩阵。</p>
<p>作者还进行了多组对照实验得到了许多令人感到意外的结论：</p>
<ol>
<li>在不进行微调的情况下，fc7的结果反而不如fc6，这表明了在不降低mAP的情况下有29%的CNN参数可以被去除。并且去除了两个全连接层的结果也是可以接受的，需要注意的是，此时只使用pool5（即仅6%的参数）。说明对目标检测结果有效的参数大多数来源于卷积层，而不是全连接层。</li>
<li>进行微调的情况下，微调普遍使实验结果提高了8%，并且在全连接层上的微调要比在pool5上的微调更有效果。这说明pool5学习的特征是通用的，微调的大部分改进是通过学习特定领域的非线性分类器获得的。</li>
</ol>
<h3 id="边界框回归"><a href="#边界框回归" class="headerlink" title="边界框回归"></a>边界框回归</h3><p>作者通过训练一个线性回归模型来预测一个新的检测窗口，用于selective search的区域建议。</p>
<h2 id="做出的贡献"><a href="#做出的贡献" class="headerlink" title="做出的贡献"></a>做出的贡献</h2><p>这篇论文首次表明，与基于更简单的hog特征的系统相比，CNN可以在PASCAL VOC上带来更高的对象检测性能。并且主要解决一下两个问题</p>
<ol>
<li>将高容量卷积神经网络(cnn)应用于<strong>自底向上的区域</strong>建议，以定位和分割对象</li>
</ol>
<p>其中为了定位物体localizing onbject，传统方法一是使用回归方法（但是在实践中的表现并不好），二是构建滑动窗口检测器（但是由于网络中接受域与step过大，图像的精确定位存在困难）</p>
<p>因此作者使用“区域识别”模式解决CNN定位问题。在实验中，作者的方法会先将输入图像划分为2k个类别无关的区域，使用CNN从每个区域提取固定长度的特征向量，然后使用类别特定的线性支持向量机SVM对每个区域进行分类</p>
<ol start="2">
<li>当标注的训练数据稀缺时，对辅助任务进行有监督的预训练，然后进行领域特定的微调，可以产生显著的性能提升。</li>
</ol>
]]></content>
      <tags>
        <tag>deep learning</tag>
        <tag>Object Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>CenterPoint</title>
    <url>/2022/03/08/CenterPoint/</url>
    <content><![CDATA[<p>论文：<a href="https://arxiv.org/pdf/2006.11275.pdf">2006.11275.pdf (arxiv.org)</a></p>
<span id="more"></span>
<h1 id="论文概述"><a href="#论文概述" class="headerlink" title="论文概述"></a>论文概述</h1><p>centerpoint是通过关键点检测来查询物体的中间位置和其特征，用<strong>两阶段</strong>的目标检测，第一阶段使用经典基于雷达激光的骨干网络如：VoxelNet、PointPillars来对输入点云进行处理，然后将表示转换为<strong>鸟瞰图</strong>，并使用标准的<strong>基于图像</strong>的关键点检测器来查找<strong>对象中心</strong>。这样对于每个检测中心可以通过中心位置的点特征来回归物体诸如三维大小、速度等特征。在第二阶段是轻量级的来<strong>细化对象位置</strong>。第二阶段提取估计对象三维box的每个面的三维中心点特征（具体来说只有四个向外的面）。该算法可以恢复由于步长和受限制的视野域带来的局部几何信息的丢失，以较小的成本带来了较好的性能提升。</p>
<p><img src="/%5Cimages%5Cimage-20220121125306002.png" alt="image-20220121125306002"></p>
<p>对于<strong>每一个点</strong>，使用<strong>双线性插值</strong>从地图视角的主干网输出中提取一个特征。接着，将提取的点特征连接起来，并将他们通过一个<strong>MLP</strong>传递。第二阶段在第一阶段的预测结果上预测一个与类无关的<strong>信心分数</strong>和对于box进一步细化。</p>
<p>可信得分：<br>$$<br>I&#x3D;\min(1,\max{(0,2\times IoU_t-0.5}))<br>$$<br>IoUt是第t个proposal box与ground-truth间的IoU</p>
<p>二元交叉熵损失：<br>$$<br>L_{socre}&#x3D;-I_t\log(\hat{I_t})-(1-I_t)\log(1-\hat{I_t})<br>$$<br>It即为可信得分</p>
<p>对于<strong>框回归</strong>，模型预测在第一阶段建议之上的细化，用L1损失训练模型。两阶段CenterPoint简化并加速了之前计算复杂度较高的的基于PointNet特征提取器和RoIAlign操作的两阶段3D检测器。</p>
<h2 id="Center-heatmap-head"><a href="#Center-heatmap-head" class="headerlink" title="Center heatmap head"></a>Center heatmap head</h2><p>Center heatmap head的目标就是在任何被探测的物体中心位置产生一个<strong>热力图峰值</strong>，Center heatmap head最终会产生一个k通道的热力图，一个通道代表了K类物体中的一种。在训练过程中，它将标注的box的三维中心投影到地图视图中，以生成二维高斯目标。使用<strong>focal loss</strong>。地图视角有图像视角不具备的优势，例如在地图视角下，汽车所占的比例很小，而在image视角下所占的比例可能会很大。此外，透视投影中对深度的压缩使得物体间的中心距离比image视角下更加接近。作者还通过放大每个ground truth的中心位置的高斯峰值来增强目标热力图的正监督，来抵消CenterNet带来的监督信号稀疏问题（使得大多数位置被认为是背景）。模型可以从附近的像素中得到更密集的监督。</p>
<h2 id="Regression-heads"><a href="#Regression-heads" class="headerlink" title="Regression heads"></a>Regression heads</h2><p>对象的中心有如下几个特征：子体素o、高度h、三维大小s以及一个偏向旋转角度。子体素位置减少了骨干网的体素化和步长带来的误差。高度h帮助在三维中定位对象，并添加被地图视角删除的高度信息。方向预测使用sin、cos作为一个连续的回归目标。结合框的大小，这些Regression heads可以提供完整的物体状态信息。每个输出使用他自己的head，作者使用L1损失训练。在推理时，通过在每个对象的峰值位置对索引密集回归头输出来提取所有属性。</p>
<h2 id="Velocity-head-and-tracking"><a href="#Velocity-head-and-tracking" class="headerlink" title="Velocity head and tracking"></a>Velocity head and tracking</h2><p>为了通过时间来跟踪物体，作者使用二维速度进行估计，并作为一个额外的回归输出。同时将前一帧中的点转换并归并到当前参考帧中，并通过时间差(速度)来预测当前和过去参考帧中物体位置的差异，来构建时间点云序列。作者对于速度回归也使用L1损失进行训练。</p>
<p>在推理时，使用贪心策略通过偏移量将当前检测与过去检测关联起来。即通过应用负速度估计将当前帧中目标中心投影回前一帧，然后通过最近距离匹配将其与跟踪目标进行匹配。</p>
<h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p><img src="/%5Cimages%5Cimage-20220121125338945.png" alt="image-20220121125338945"></p>
]]></content>
      <tags>
        <tag>deep learning</tag>
        <tag>Object Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>F-PointNet</title>
    <url>/2022/03/08/F-PointNet/</url>
    <content><![CDATA[<p>论文：<a href="https://openaccess.thecvf.com/content_cvpr_2018/html/Qi_Frustum_PointNets_for_CVPR_2018_paper.html">CVPR 2018 Open Access Repository (thecvf.com)</a></p>
<span id="more"></span>

<h1 id="论文概述"><a href="#论文概述" class="headerlink" title="论文概述"></a>论文概述</h1><p><img src="/%5Cimages%5Cimage-20220117100417479.png" alt="image-20220117100417479"></p>
<p>每个对象都由一个类（k个预定义类）和一个模态三维box表示。即使对象的一部分被遮挡或是截断，模态框也会得到完整的对象。</p>
<h2 id="关键流程"><a href="#关键流程" class="headerlink" title="关键流程"></a>关键流程</h2><ol>
<li>利用一个二维CNN对象检测器提取二维区域并对内容进行分类</li>
<li>将二维区域提升到三维，并形成截锥方案</li>
<li>通过对截锥中每个点二值分类，对对象实例进行分割（截锥点云：n×c、n个点、c个通道）</li>
<li>基于分割后的点云（m×c），使用T-Net平移对齐点，使得质心接近模态盒中心</li>
<li>使用框估计网络估计得出模态三维box</li>
</ol>
<h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><h3 id="截锥建议"><a href="#截锥建议" class="headerlink" title="截锥建议"></a>截锥建议</h3><p><img src="/%5Cimages%5Cimage-20220117115726571.png" alt="image-20220117115726571"></p>
<p>由于现有的实时深度探测器得到的分辨率要低于RGB图像，因此先使用RGB图像通过二维目标检测框出目标物体的box，再通过投影矩阵，将二维box提升为三维搜索空间的截锥。接着收集截锥内部的所有点云，形成<strong>锥形体点云</strong>。还需要通过将圆台旋转到中心视图来对圆台进行<strong>归一化</strong>，使得圆台的中轴与图像平面正交（上图b）。这种归一化有利于提高算法发旋转不变性。</p>
<p>二维的目标检测网络使用在ImageNet分类数据集和COCO对象检测数据集上预先训练的模型权重，并且在KITTI二维对象检测数据集上进一步调整模型权重，以对二维box进行分类和预测。</p>
<h3 id="三维实例分割"><a href="#三维实例分割" class="headerlink" title="三维实例分割"></a>三维实例分割</h3><p>获得二维box和对应的三维截锥后，有几种可以获取对象三维坐标的方法</p>
<ol>
<li>直接通过深度图使用二维CNN网络，进行三维位置的回归。容易与遮挡物体或是背景噪声混淆</li>
<li>在三维点云中进行分割处理，使用<strong>基于PointNet的网络</strong>处理截锥内的点</li>
</ol>
<p>本篇文献采用的是第二种方法，从截锥中获取点云并预测每个点的概率分数，该分数表示该点属于某个对象的可能性有多大。而一个点只可能属于一个特定的对象，此时其他点属于无关点。同时，该网路还学习遮挡与噪声对目标检测的影响。</p>
<p>在多类检测任务中，还利用二维检测器的信息提供更好的实例分割。例如，二维检测器检测出对象是行人后，网络会特别的针对类似人的特征进行检测。实际中，是通过将语义类别编码为一个one-hot的k维类向量（代表希望检测的k个对象），并将这个向量连接到中间点云特征中。</p>
<p>三维实例分割出来后，提取出来被分类为感兴趣的对象的点。对这些点再进一步归一化（上图c），以提高坐标的平移不变性。需要注意的是，此处没有进行点云的缩放，因为对象的大小对于框的估计也有重要的作用，</p>
<h3 id="模态三维盒估计"><a href="#模态三维盒估计" class="headerlink" title="模态三维盒估计"></a>模态三维盒估计</h3><p>尽管使用了特征对齐，该网路仍存在坐标系的原点仍然离模态盒中心很远。在使用一个轻量化的PointNet（T-Net）来重新估计完整对象的真实中心，然后转换坐标，使预测中心成为原点。这里的T-Net可以看作为新的一种空间transformer网络，并且明确的监督平移网络来预测mask坐标原点到真实物体中心的<strong>中心残差</strong>。该网路结构与PointNet和PointNet++相似，不过输出的是三维box的参数。</p>
<p>估计box中心时使用残差方法，将box估计网络预测的中心和T-Net以及遮挡点形心得到的预测中心相结合获得绝对中心<br>$$<br>C_{pred}&#x3D;C_{mask}+\Delta C_{T-Net}+\Delta C_{box-net}<br>$$<br>对于box的尺寸和朝向，使用Faster r-cnn的预测朝向方法来进行预测。即预定义了NS个模板和NH个等分割角度的box。网络将这些分类到预定义的类别中，并预测每个类别的参差数</p>
<h2 id="多任务训练损失"><a href="#多任务训练损失" class="headerlink" title="多任务训练损失"></a>多任务训练损失</h2><p><img src="/%5Cimages%5Cimage-20220118222715021.png" alt="image-20220118222715021"></p>
<p><strong>角损</strong>对于网络结果的影响还是很大的，本质上，角损时预测框和ground truth box八个角的距离之和。由于角点位置由中心、大小和方向共同决定，因此角点损失能够对这些参数的多任务训练进行正则化。<br>$$</p>

L_{corner}=\sum^{NS}_{i=1}\sum^{NH}_{j=1}\delta_{ij}\min{\{\sum^8_{k=1}\parallel p_k^{ij}-p^{*}_k \parallel,\sum^8_{k=1}\parallel p_k^{ij}-p^{**}_k \parallel\} }

<p>$$<br>构造NS×NH个包含所有尺寸和朝向的anchors,取值时使用原始和翻转情况最小的值，避免由于翻转航向造成较大损失。δ是ground truth的大小&#x2F;朝向类，是一个二维掩码用来选择距离项。</p>
<h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p><img src="/%5Cimages%5Cimage-20220117114100396.png" alt="image-20220117114100396"></p>
<p>该网络的优势：</p>
<ol>
<li>对于合理距离内的非遮挡物体的情况，可以得出非常精确的三维box</li>
<li>并且对于部分获取到的点数很少的物体（平行停放的汽车）也可以正确的预测三维box</li>
<li>在二维box相互重叠的情况下，转换到三维空间后处理起来就容易了许多。</li>
</ol>
<p>同时，实验也暴露出该网路存在的一些问题：</p>
<ol>
<li>由于稀疏点云，造成不准确的姿态和大小估计。这个可以通过对图像特征的进一步提取解决</li>
<li>如果截锥中有同一类别的多个实例（两个人站在一起），因为网络假设的是一个截锥只有一个对象，所以出现多个对象的时候，可能会产生混淆，从而输出混合分割结果。可以通过在每个截锥中设置多个三维box缓解</li>
<li>二维检测器会因为湖南的灯光或强遮挡错过目标，如果二位检测没有检测到物体，转换到三维空间时自然会忽略该物体。因此可以借助与BEV图像缓解。</li>
</ol>
<h1 id="做出的贡献"><a href="#做出的贡献" class="headerlink" title="做出的贡献"></a>做出的贡献</h1><ol>
<li>提出一种基于RGB-D数据的三维目标检测算法</li>
<li>提供广泛的定量评估来验证该算法的设计选择，以及丰富的定性结果来理解该方法的优势和局限性</li>
<li>展示了如何在该框架下训练3D对象检测器，并在标准的3D对象检测基准上实现最先进的性能。</li>
</ol>
<p>[^预测朝向方法]: A. Mousavian, D. Anguelov, J. Flynn, and J. Kosecka. 3d bounding box estimation using deep learning and geometry.、Faster r-cnn: Towards real-time object detection with region proposal networks.</p>
]]></content>
      <tags>
        <tag>deep learning</tag>
        <tag>Object Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>PointPillars</title>
    <url>/2022/03/08/PointPillars/</url>
    <content><![CDATA[<p>论文：<a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Lang_PointPillars_Fast_Encoders_for_Object_Detection_From_Point_Clouds_CVPR_2019_paper.html">CVPR 2019 Open Access Repository (thecvf.com)</a></p>
<span id="more"></span>
<p><img src="/%5Cimages%5Cimage-20220114222458551.png" alt="image-20220114222458551"></p>
<h1 id="论文概述"><a href="#论文概述" class="headerlink" title="论文概述"></a>论文概述</h1><p>本篇文献主要解决将点云编码为适合更适合目标检测的格式的问题。作者没有预训练网络，所有权值随机使用均匀分布。</p>
<h2 id="特征解码网络"><a href="#特征解码网络" class="headerlink" title="特征解码网络"></a>特征解码网络</h2><p>将点云转换为稀疏的伪图像。</p>
<p>不同于voxelNet将整个点云分割为许多的voxel，PointPillar只在xOy面进行划分，整个点云空间被分割为若干个高度为点云图高度（在z方向上近似是无限的，因此不需要超参数控制z维度）的柱体。</p>
<p>与voxel类似，大部分的柱体都是空的。而对于包含点数过多N的柱体采用随机采样，对于包含点数过小的柱体应用零填充。并通过限制非空柱体的数量P，将整个点云用一个尺寸为(D,P,N)的密集张量。</p>
<p>接着使用简化的PointNet，对于每个点应用一个线性层（张量的1×1卷积）、一个BN层、一个ReLU层，产生一个尺寸为(C,P,N)的张量。（C&#x3D;64）然后对其进行max操作，创建一个尺寸为(C,P)的输出张量（将三维点云转换为二维数据）。因为作者是使用的柱体而非voxel，所以可以在卷积中间层避免三维卷积，极大的提升计算效率。</p>
<p>编码后特征被分散到原始位置，以创建一个大小为(C,H,W)的伪图像（H，W为画布的高度和宽度）    </p>
<h2 id="二维卷积骨干网络"><a href="#二维卷积骨干网络" class="headerlink" title="二维卷积骨干网络"></a>二维卷积骨干网络</h2><p>将伪图像转换为高层的表示。该网络又可以分解为两个小的网络</p>
<ol>
<li>自上而下网络：在越来越小的空间分辨率上产生特征</li>
<li>对自上而下的特征进行上采样和连接</li>
</ol>
<p>作者使用Block(S,L,F)来表示自顶向下的主干，每个block的操作步长为S（与初始的伪图像大小有关），每块都有一个二维卷积层L和输出通道F，以及一个BN层、一个ReLU层。</p>
<p>第一层卷积步长为S&#x2F;Sin，<em>以确保block在执行步长为Sin卷积操作后还可以执行步长为S的操作。</em>（对于汽车S&#x3D;2，对于行人、自行车S&#x3D;1）（The first convolution inside the layer has stride S&#x2F;Sin to ensure the block operates on tride S after receiving an input blob of stride Sin）。后续block上的卷积步长为1.</p>
<p>最终从每个自顶向下block得到的特征通过一定的上采样和拼接进行组合：</p>
<ol>
<li>对特征进行上采样Up(Sin,Sout,F)，初始步长Sin，最终步长Sout，利用转置二维卷积和F得到特征</li>
<li>使用 BatchNorm和ReLU 应用上采样特征，最终的输出特征是从不同步长级联得到的特征</li>
</ol>
<h2 id="Detection-Head"><a href="#Detection-Head" class="headerlink" title="Detection Head"></a>Detection Head</h2><p>检测三维box，并进行回归。这里的 Detection Head是模块化的，即对于不同的任务可以使用不同的 Detection Head。就像使用不同的镜头来拍摄不同的照片。</p>
<p>作者这里使用 Single Shot Detector (SSD)以来处理三维目标检测。同时，使用二维的IoU将ground truth与先验box相匹配。而将box高度与高度elevation作为额外的回归目标</p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>回归残差：<br>$$</p>

\Delta x=\frac{x^{gt}-x^a}{d^a},
\Delta y=\frac{y^{gt}-y^a}{d^a},
\Delta z=\frac{z^{gt}-z^a}{d^a}\quad(d=\sqrt{(w^a)^2+(l^a)^2});\\
\Delta l=\log{\frac{l^{gt} }{l^a} },
\Delta w=\log{\frac{w^{gt} }{w^a} },
\Delta h=\log{\frac{h^{gt} }{h^a} },
\Delta\theta=\theta^{gt}-\theta^a;

<p>$$<br>定位损失：由于角度定位无法区分翻转的box，在离散化方向上使用softmax分类损失Ldir学习车辆前进方向<br>$$<br>L_{loc}&#x3D;\sum_{b\in(x,y,z,w,l,\theta)}SmoothL1(\Delta b)<br>$$<br>目标分类损失：p^a是锚点是类的概率，α&#x3D;0.25，γ&#x3D;2<br>$$<br>L_{cls}&#x3D;-\alpha_a(1-p^a)^\gamma\log{p^a},<br>$$</p>
<p>总损失函数：Npos为正锚点的数量、βloc&#x3D;2、βcls&#x3D;1、βdir&#x3D;0.2<br>$$</p>

L=\frac{1}{N_{pos} }(\beta_{loc}L_{loc}+\beta_{cls}L_{cls}+\beta_{dir}L_{dir}),

<p>$$<br>损失函数是以哦那个初始学习率为0.0002的Adam进行优化，每15个epoch减少0.8倍。用于验证与测试的epoch个数分别为160、320，batch大小分别为2、4.</p>
<h2 id="超参数设置"><a href="#超参数设置" class="headerlink" title="超参数设置"></a>超参数设置</h2><ul>
<li>xy分辨率：0.16m</li>
<li>点柱最大数量P：12000</li>
<li>点柱内最多点数N：100</li>
<li>轴对齐非极大抑制NMS的IoU阈值：0.5</li>
</ul>
<h3 id="汽车检测任务："><a href="#汽车检测任务：" class="headerlink" title="汽车检测任务："></a>汽车检测任务：</h3><ul>
<li><p>x,y,z检测范围：[(0, 70.4), (-40, 40), (-3, 1)]</p>
</li>
<li><p>锚点宽、长、高：(1.6, 3.9, 1.5)m，z中心：-1m</p>
</li>
<li><p>匹配正负阈值：0.6、0.45</p>
</li>
</ul>
<h3 id="行人、自行车检测任务："><a href="#行人、自行车检测任务：" class="headerlink" title="行人、自行车检测任务："></a>行人、自行车检测任务：</h3><ul>
<li><p>x,y,z检测范围：[(0, 48), (-20, 20), (-2.5, 0.5)]</p>
</li>
<li><p>行人锚点宽、长、高：(0.6, 0.8, 1.73)m，z中心：-0.6m</p>
</li>
<li><p>自行车锚点宽、长、高：(0.6, 1.76, 1.73)m，z中心：-0.6m</p>
</li>
<li><p>匹配正负阈值：0.5，0.35</p>
</li>
</ul>
<h2 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h2><ol>
<li>类似SECOND，创建一个查找表，其中包含所有类别的ground truth 3D boxes以及box中相关联的点云；对于每个样本随即选择若干个汽车、行人、自行车的真实样本将其放入当前环境中以提升对于不同环境下目标检测的能力</li>
<li>对所有 <strong>ground truth boxes独立的旋转、转换</strong>，进一步丰富训练集。</li>
<li>执行两个全局增强集合：随机沿x轴翻转并执行<strong>全局旋转和放缩</strong>、使用N(0,0,2)<strong>模拟噪声</strong>对x、y、z坐标转换</li>
</ol>
<h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p><img src="/%5Cimages%5Cimage-20220115183322884.png" alt="image-20220115183322884"></p>
<p>实验发现该网络对于行人的检测仍有一些不足，行人与自行车会被误认为彼此。行人也容易被混淆为狭窄的带有垂直特性的物体，如树干、电线杆。</p>
<p>推理速度快也是该网络的一大优势，总的运行时间：16.2ms。主要推理过程如下：（ Intel i7 CPU and a 1080ti GPU）</p>
<ol>
<li>根据图像的可见性、范围加载、过滤点云1.4ms</li>
<li>将点分配到点柱并进行处理2.7ms</li>
<li>将点柱张量加载进GPU2.9ms、<strong>编码1.3ms</strong>、分散为伪图像0.1ms</li>
<li>由卷积骨干网、检测头处理7.7ms</li>
<li>NMS处理0.1ms（使用CPU）</li>
</ol>
<p><strong>编码阶段</strong>是该网络运行时间少的关键，VoxelNet的解码时间190ms、SECOND的编码时间50ms。同时该网络只使用一个PointNet网络进行编码，将pytorch的运行时间减少了2.5ms。将第一个block的尺寸缩减到64以匹配解码输出的尺寸，并将上采样特征层输出尺寸减半到128，这些都大幅减少了运行时间。</p>
<p>实验证明，当推理速度达到105Hz时，准确率只减少了一点。相比之下，激光雷达的工作频率为20Hz。但是需要注意的是，现在的实验是使用桌面级GPU，如果应用于实际，使用嵌入式GPU，计算效率会下降；一个可操作的AV需要查看完整的环境并处理全部点云，而实验使用的KITTI数据集中，只会使用10%的点云数据。实际中需要计算的数据量有很大区别。</p>
<h3 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h3><p>作者通过一系列消融实验得到以下结论：</p>
<ol>
<li>好的编码器明显优于固定的编码器，特别是对于更大的分辨率。</li>
<li>box增强并不会带来更大提升，反而在检测行人方面导致性能下降</li>
<li>更小的点柱使得定位更准确以及学习的特征更多，更大的点柱计算速度更快（更少的非空点柱）</li>
</ol>
<h1 id="做出的贡献"><a href="#做出的贡献" class="headerlink" title="做出的贡献"></a>做出的贡献</h1><ol>
<li>提出一个新的基于点云解码器和网络PointPliiar，适用于端到端的基于点云的三维目标检测网络的训练</li>
<li>将对点云的柱上的计算变为密集的二维卷积，使得推理速率到达62Hz</li>
<li>在KITTI数据集上的实验，该网络表现出对于汽车、自行车、行人检测最先进的结果</li>
<li>通过消融实验 ablation studies，发现对检测性能起到关键影响的因素</li>
<li>作者提出的点柱偏移Xp、Yp以及簇偏移Xc、Yc、Zc带来更好的检测效果</li>
</ol>
]]></content>
      <tags>
        <tag>deep learning</tag>
        <tag>Object Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>PointRCNN</title>
    <url>/2022/03/08/PointRCNN/</url>
    <content><![CDATA[<p>论文：<a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Shi_PointRCNN_3D_Object_Proposal_Generation_and_Detection_From_Point_Cloud_CVPR_2019_paper.html">CVPR 2019 Open Access Repository (thecvf.com)</a></p>
<span id="more"></span>
<h1 id="论文概述"><a href="#论文概述" class="headerlink" title="论文概述"></a>论文概述</h1><p><img src="/%5Cimages%5Cimage-20220218103739679.png" alt="image-20220218103739679"></p>
<p>本篇论文使用两阶段的三维目标检测框架，并且直接应用于三维点云，实现了强大和准确的三位检测性能。</p>
<h2 id="第一阶段"><a href="#第一阶段" class="headerlink" title="第一阶段"></a>第一阶段</h2><p>第一阶段是生成自底向上的三维包围盒方案，分割前景，同时从分割点生成数量较少的box proposal，节省大量的计算量。具体来说，通过学习逐点特征来分割原始点云，并同时从分割的前景点生成3D提案。</p>
<p>对于训练集中的每个3D点云场景，我们从每个场景中抽取16,384个点作为输入。对于点数小于16,384的场景，我们随机重复这些点数，得到16,384点。对于stage-1子网络，我们遵循[28]的网络结构，其中使用四个具有多尺度分组的集抽象层，将点分组为大小为4096、1024、256、64的组。然后使用四个特征传播层来获取点特征向量，用于分割和生成建议。</p>
<p>鉴于骨干点云网络编码的逐点特征，通过附加一个分割头用于估计前景掩模和一个框回归头用于生成3D提案。对于点分割，ground-truth分割蒙版自然是由3D  ground-truth box提供的。对于大型户外场景，前景点的数量一般要比背景点的数量少得多。因此，我们使用焦点损失[19]来处理类不平衡问题<br>$$<br>L_{focal}(p_t)&#x3D;-\alpha_t(1-p_t)^\gamma\log(p_t),\<br>p_t&#x3D;\begin{cases} p \qquad for forground point \1-p \qquad otherwise\end{cases}<br>$$</p>
<h2 id="第二阶段"><a href="#第二阶段" class="headerlink" title="第二阶段"></a>第二阶段</h2><p>第二阶段进行规范的三维box框细化，生成proposal后，采用点云池化，将第一阶段学习到的点进行池化。</p>
<p>在LiDAR坐标系中，三维包围框表示为(x, y, z, h, w, l， θ)，其中(x, y, z)为目标中心位置，(h, w,  l)为目标大小，θ为从鸟瞰目标方向。为了约束生成的3D框建议，提出了基于bin的回归损失来估计对象的3D边界框。</p>
<p><img src="/%5Cimages%5Cimage-20220218183400653.png" alt="image-20220218183400653"></p>
<p>Fcls为交叉分类损失；Freg为平滑L1损失</p>
<p>同时为了消除冗余提案，通过基于鸟瞰图进行非最大抑制(non - maximum suppression,  NMS)，生成少量高质量提案。在训练方面，使用0.85作为IoU阈值，在NMS之后，保留stage-2子网培训建议的top  300的proposals。推理过程中，采用NMS，IoU阈值为0.8，只保留前100个建议对阶段2子网进行细化</p>
<p>对于框提案细化子网络，网络从每个提案的集合区域随机抽取512个点作为细化子网络的输入。使用三个单尺度分组集合抽象层(分组大小分别为128、32、1)生成单个特征向量，用于对象置信度分类和建议位置优化。</p>
<h2 id="点云区域池化"><a href="#点云区域池化" class="headerlink" title="点云区域池化"></a>点云区域池化</h2><p>在获得3D包围盒提案后，目标是在之前生成的box proposal的基础上细化box的位置和方向。为了了解每个方案更具体的局部特征，建议根据每个3D方案的位置，从stage-1集合3D点及其对应的点特征。</p>
<p>对于每个三维box，作者都会稍微放大尺寸得到一个新的三维box，从上下文编码额外的信息</p>
<p>对于每个点，通过内外测试确定点是否再扩大的box proposal中。</p>
<h2 id="规范的3D-box细化"><a href="#规范的3D-box细化" class="headerlink" title="规范的3D box细化"></a>规范的3D box细化</h2><h3 id="正则变换"><a href="#正则变换" class="headerlink" title="正则变换"></a>正则变换</h3><p>正则变换遵守如下规则：</p>
<ol>
<li>原点位于方框的中心</li>
<li>局部的X‘和Z’轴近似平行于地平面，X‘指向提案的头部方向，另一个Z‘轴垂直于X’</li>
<li>Y ’轴与激光雷达坐标系保持一致。</li>
</ol>
<p><img src="/%5Cimages%5Cimage-20220220120505467.png" alt="image-20220220120505467"></p>
<h3 id="改进box-proposal的特征学习"><a href="#改进box-proposal的特征学习" class="headerlink" title="改进box proposal的特征学习"></a>改进box proposal的特征学习</h3><p>细化子网络结合了变换后的局部空间点(特征)以及从阶段1进行进一步的盒和置信度细化得到的全局语义特征。</p>
<p>虽然正则变换能够实现鲁棒的局部空间特征学习，但它不可避免地会丢失每个对象的深度信息。为了补偿丢失的深度信息，将点到传感器的距离特征加入特征点p中。</p>
<p>对于每个提议，其关联点的局部空间特征和额外的特征首先连接并馈送给几个全连接层，将其局部特征编码为相同维的全局特征。然后将局部特征和全局特征串联并馈送到一个pointNet++结构的网络中，得到一个判别特征向量，用于后续的置信度分类和盒体细化。</p>
<h3 id="改进box-proposal的损失函数"><a href="#改进box-proposal的损失函数" class="headerlink" title="改进box proposal的损失函数"></a>改进box proposal的损失函数</h3><p>使用基于bin的回归损失来改进proposal，如果IoU大于0.55，则将ground-truth box 分配给三维box proposal，用于学习box的改进。（三维box proposal及其对应的ground-truth box 都被转换为标准坐标系）<br>$$</p>

L_{refine}=\frac{1}{\vert\vert B\vert\vert}\sum_{i\in B}F_{cls}(prob_i,label_i)+\frac{1}{\vert\vert B_{pos}\vert\vert}\sum_{i\in B_{pos} }(\hat{L}^{(i)}_{bin}+\hat{L}^{(i)}_{res})

<p>$$</p>
<p>B是阶段1的3D提案集合，Bpos存储回归的正提案，probi是bi的估计置信度，labeli是相应的标签，</p>
<p>对于box偏转方向，则将ground-truth box 与三维box proposal的IoU阈值为0.55。</p>
<h1 id="做出的贡献"><a href="#做出的贡献" class="headerlink" title="做出的贡献"></a>做出的贡献</h1><ol>
<li>提出了一种基于点云的自底向上的三维包围盒提案生成算法，该算法通过将点云分割成前景对象和背景，生成少量高质量的三维提案。从分割中学习到的点表示不仅擅长于提议的生成，而且对后续的框细化也有帮助。</li>
<li>所提出的规范3D包围盒细化利用了从阶段1生成的高召回量盒建议，并学会了在规范坐标中预测基于稳健盒基损耗的盒坐标细化。</li>
<li>提出的三维检测框架PointRCNN在仅使用点云作为输入的情况下，显著优于目前最先进的方法</li>
</ol>
]]></content>
      <tags>
        <tag>deep learning</tag>
        <tag>Object Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>second</title>
    <url>/2022/03/08/second/</url>
    <content><![CDATA[<p>论文：<a href="https://www.mdpi.com/1424-8220/18/10/3337">Sensors | Free Full-Text | SECOND: Sparsely Embedded Convolutional Detection (mdpi.com)</a></p>
<span id="more"></span>
<h1 id="前人贡献"><a href="#前人贡献" class="headerlink" title="前人贡献"></a>前人贡献</h1><p>使用<strong>RGB-D数据</strong>的二维表示的方法分为基于鸟瞰图、基于前景两种。</p>
<h2 id="Front-View-and-Image-Based-Methods"><a href="#Front-View-and-Image-Based-Methods" class="headerlink" title="Front-View- and Image-Based Methods"></a>Front-View- and Image-Based Methods</h2><p>在一般的<strong>基于图像</strong>的方法中，先生成二维box类语义、实例语义，再使用手工方法生成特征图。另一种方法使用CNN从图像中估计3Dbox，并使用专门设计的离散连续CNN估计物体运动方向。</p>
<p>对于基于激光雷达数据的方法包括将点云转换为前景的2D map，并应用2D探测器对前景视图中的图像进行定位、和其他方法相比，这些方法在BEV检测和三维检测方面都做得很差。</p>
<p>代表论文：<a href="https://openaccess.thecvf.com/content_cvpr_2017/html/Mousavian_3D_Bounding_Box_CVPR_2017_paper.html">CVPR 2017 Open Access Repository (thecvf.com)</a>、[<a href="https://arxiv.org/abs/1608.07916">1608.07916] Vehicle Detection from 3D Lidar Using Fully Convolutional Network (arxiv.org)</a></p>
<h2 id="Bird’s-Eye-View-Based-Methods"><a href="#Bird’s-Eye-View-Based-Methods" class="headerlink" title="Bird’s-Eye-View-Based Methods"></a>Bird’s-Eye-View-Based Methods</h2><p>这种方法将点云数据转换为<strong>多个切片</strong>得到height maps（按不同高度划分），再将height maps与intensity map、density map 结合得到多通道特征。这种方法的问题是在生成BEV图时，许多数据点被丢弃，导致垂直轴上信息损失很大，这种信息丢失会严重影响在3Dbox回归中的性能</p>
<p>如MV3D（首个将点云数据转换为BEV的方法）；ComplexYOLO使用YOLO网络和复杂角度编码方法来提高速度和定位性能、但在预测3D边界框时只能固定高度）；</p>
<p>代表文章：<a href="https://openaccess.thecvf.com/content_CVPRW_2019/html/WAD/Simon_Complexer-YOLO_Real-Time_3D_Object_Detection_and_Tracking_on_Semantic_Point_CVPRW_2019_paper.html">CVPR 2019 Open Access Repository (thecvf.com)</a>、<a href="https://openaccess.thecvf.com/content_CVPRW_2019/html/WAD/Simon_Complexer-YOLO_Real-Time_3D_Object_Detection_and_Tracking_on_Semantic_Point_CVPRW_2019_paper.html">CVPR 2019 Open Access Repository (thecvf.com)</a></p>
<h2 id="3D-Based-Methods"><a href="#3D-Based-Methods" class="headerlink" title="3D-Based Methods"></a>3D-Based Methods</h2><p>多数的3D-based方法或者<strong>直接使用</strong>点云数据、或者将数据转换为3Dvoxel（而不是BEV），然后采用一种<strong>卷积式的投票算法</strong>进行检测。这种方法利用点云数据的稀疏性，以特征中心的投票方案提高计算速度。但是是使用<strong>手工制作</strong>特征方式，无法适应自动驾驶的复杂环境。</p>
<p>之后又有人提出使用<strong>CNN网络、k-领域</strong>等方法从点云中学习局部空间信息。但是这些方法不能应用于大规模的点，需要用图像检测结果对原始数据点进行滤波。</p>
<p>CNN网络应用到点云也是目前的研究热门，其基本思想是基于CNN的检测器将点云转换为voxel，有下列一些方向：</p>
<ol>
<li>将点云数据离散为二值的voxel，然后进行三维卷积</li>
<li>将点云数据分组为voxel，提取voxel特征，再将这些特征转换为密集张量，利用3D或2D卷积网络进行处理</li>
</ol>
<p>这种方法的主要问题是3D CNN的高计算成本，而且3D CNN的计算复杂度随着voxel分辨率的增加而增加。因此，使用稀疏结构的卷积网络会降低计算复杂度。而 <a href="https://openaccess.thecvf.com/content_cvpr_2018/html/Graham_3D_Semantic_Segmentation_CVPR_2018_paper.html">CVPR 2018 Open Access Repository (thecvf.com)</a>提出了一种空间结构不变的3D CNN。这种网络已经应用于三维语义分割任务，但是还没有利用稀疏卷积进行检测的方法。</p>
<p>代表论文：[<a href="https://arxiv.org/abs/1505.02890">1505.02890] Sparse 3D convolutional neural networks (arxiv.org)</a>、<a href="https://openaccess.thecvf.com/content_cvpr_2018/html/Graham_3D_Semantic_Segmentation_CVPR_2018_paper.html">CVPR 2018 Open Access Repository (thecvf.com)</a></p>
<h2 id="Fusion-Based-Methods"><a href="#Fusion-Based-Methods" class="headerlink" title="Fusion-Based Methods"></a>Fusion-Based Methods</h2><p>这种方法将<strong>相机图像与点云</strong>相结合。</p>
<ol>
<li>使用3维的RPN两个尺度不同的接受域产生三维proposal，然后将每个三维proposal的深度数据反馈到三维CNN并且将相应的二维的颜色补充到二维CNN网络来预测最终结果。</li>
<li>将点云数据转换为一个正视图和一个BEV，再从这两个图中提取特征图与图像特征图融合。但是它含有三个CNN网络并不适用于小心对象</li>
<li>将图像与BEV结合，使用一种新的结构生成高分辨率的特征图的三位对象proposal</li>
<li>使用二维检测结果过滤点云，PointNet就可以应用于三维box</li>
</ol>
<p>这些方法需要处理大量的数据，因此基于融合的方法运行缓慢。并且它对激光雷达的时间同步和校准摄像机的额外要求限制这种方法的使用环境，降低了鲁棒性。</p>
<p>代表论文：<a href="https://openaccess.thecvf.com/content_cvpr_2017/html/Chen_Multi-View_3D_Object_CVPR_2017_paper.html">CVPR 2017 Open Access Repository (thecvf.com)</a></p>
<h1 id="论文概述"><a href="#论文概述" class="headerlink" title="论文概述"></a>论文概述</h1><h2 id="SECOND-Detector"><a href="#SECOND-Detector" class="headerlink" title="SECOND Detector"></a>SECOND Detector</h2><p>SECOND Detector以原始点云作为输入，将其转换为voxel特征和坐标，并应用两个VFE层和一个线性层。然后使用稀疏CNN。最后应用RPN生成检测结果。</p>
<p><img src="/%5Cimages%5Cimage-20220111213034747.png" alt="image-20220111213034747"></p>
<p> 作者的Point Cloud Grouping、Voxel-wise Feature Extractor与VoxelNet的处理相同此处便不再赘述。</p>
<h3 id="稀疏卷积网络"><a href="#稀疏卷积网络" class="headerlink" title="稀疏卷积网络"></a>稀疏卷积网络</h3><p>作者的主要改进体现在引入了<strong>稀疏卷积网络</strong>，替代voxelNet中的三维卷积提取特征图。常规的稀疏卷积是如果没有相关的输入点，则不计算输出点。子簇卷积（常规卷积网络的替代）限制当且仅当相应的输入位置处于活动状态是，输出位置才处于活动状态，这可以避免生成过多的活动点，提升卷积速度。</p>
<h3 id="稀疏卷积算法"><a href="#稀疏卷积算法" class="headerlink" title="稀疏卷积算法"></a>稀疏卷积算法</h3><ol>
<li><p>将稀疏的<strong>输入特征</strong>通过gather操作获得<strong>密集的gather特征；</strong></p>
</li>
<li><p>然后使用GEMM对<strong>密集的gather特征</strong>进行卷积操作，获得<strong>密集的输出特征；</strong></p>
</li>
<li><p>通过预先构建的<strong>输入-输出索引规则矩阵</strong>，将<strong>密集的输出特征</strong>映射到<strong>稀疏的输出特征</strong>。</p>
</li>
</ol>
<p><img src="/%5Cimages%5Cimage-20220112105038074.png" alt="image-20220112105038074"></p>
<p>二维密集卷积算法中，W表示过滤元素，D表示图像元素。函数P(x,y)需要根据输出位置来计算输入位置。因此，卷积输出Y计算如下：<br>$$</p>

Y_{x,y,m}=\sum_{u,v\in P(x,y)}{\sum_{l}{W_{u-u_0,v-v_0,l,m}D_{u,v,l} } }\quad(1)

<p>$$<br>基于<strong>矩阵乘法GEMM算法</strong>可用于收集全部用于构建矩阵的数据，并执行GEMM本身。<br>$$</p>

Y_{x,y,m}={\sum_{l}{W_{*,l,m}\tilde{D}_{P(x,y),l} } }\quad(2)
{% rendrawaw %}
<p>$$<br>此处的W与上式的W相同，只是<strong>使用GEMM形式</strong>。对于稀疏数据D‘和相关联的输出Y’直接计算算法如下：<br>$$</p>
{% raw %}
Y_{x,y,m}=\sum_{i\in P'(j)}{\sum_{l}{W_{k,l,m}D'_{i,l} } }\quad(3)
{% endraw %}
<p>$$<br>其中p‘是获取输入索引和滤波器偏移量的函数。<strong>基于GEMM的版本</strong>为<br>$$</p>
{% raw %}
Y’_{j,m}={\sum_{l}{W_{*,l,m}\tilde{D'}_{P'(j),l} } }\quad(4)
{% endraw %}
<p>$$<br>因为D’中含有大量的零不用参与计算，因此引入<strong>规则矩阵R</strong>，指定输入索引i给出核偏移量k和输出索引j，公式如下：<br>$$</p>
{% raw %}
Y’_{j,m}=\sum_k{\sum_{l}{W_{k,l,m}\tilde{D'}_{R_{k,j},k,l} } }\quad(5)
{% endraw %}
<p>$$<br>而5式的inner sum无法通过GEMM的计算，因此还需要收集足够的数据构建矩阵来执行GEMM，再将数据分散回去。实际中可以利用预先构造的<strong>输入-输出索引规则矩阵</strong>从原始稀疏矩阵数据中收集数据</p>
<h3 id="生成规则算法"><a href="#生成规则算法" class="headerlink" title="生成规则算法"></a>生成规则算法</h3><p>常见的哈希表规则生成算法是基于CPU的，速度较慢，并且需要再CPU和GPU间进行数据传输。另一种方法是<strong>迭代输入点</strong>，找到每个输入点相关的输出，并将相应的索引存储到规则中。在迭代的过程中，需要使用一张表检查每个输出位置的存在性以决定是否使用全局输出索引计数器来累加数据，这也是制约并行计算在算法中使用的最大挑战。</p>
<p>作者设计了一种<strong>基于GPU的规则生成算法</strong>。</p>
<ol>
<li><strong>收集输入的索引和对应的空间索引</strong>而非输出索引（此阶段会重复获得输出索引）</li>
<li>在空间索引数据上使用一种独特的<strong>并行算法</strong>，以获得输出索引以及相关的空间索引。</li>
<li>根据前两步的结果生成一个<strong>与稀疏数据空间维度相同的缓冲区</strong>，用于下一步的表查找</li>
<li>对规则进行<strong>迭代</strong>，并使用存储的空间索引来获取每个输入索引的输出索引。</li>
</ol>
<p><img src="https://pic2.zhimg.com/80/v2-40a7e08f7a00a6e25ac4ff33a25fb849_1440w.jpg" alt="img"></p>
<h3 id="稀疏卷积中间提取器"><a href="#稀疏卷积中间提取器" class="headerlink" title="稀疏卷积中间提取器"></a>稀疏卷积中间提取器</h3><p>中间提取器用于学习z轴信息，并将稀疏的三维数据转换为二维BEV图像。它包含了稀疏卷积的两个阶段。每个阶段都有几个子流形卷积层和一个正常的稀疏卷积，用于在z轴进行下采样。在 z 维被下采样到一维或二维后，稀疏数据被转换为密集特征图。 然后，将数据简单地重新整形为类似图像的 2D 数据。</p>
<p><img src="/%5Cimages%5Cimage-20220112212134401.png" alt="image-20220112212134401"></p>
<blockquote>
<p>黄色表示稀疏卷积，白色表示子流形卷积，红色表示稀疏到密集层，图的上半部分是稀疏数据的空间维数。</p>
</blockquote>
<h3 id="Anchors与目标"><a href="#Anchors与目标" class="headerlink" title="Anchors与目标"></a>Anchors与目标</h3><p>作者的anchor size与<strong>VoxelNet</strong>中anchor size，对正负锚点的阈值选择都是一样。作者同时为每个锚点分配一个以分类为目标的one-hot向量、一个边界框回归为目标的7维向量、一个以方向分类为目标的one-hot向量。</p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><h3 id="Sine-Error-Loss-for-Angle-Regression（方向回归）"><a href="#Sine-Error-Loss-for-Angle-Regression（方向回归）" class="headerlink" title="Sine-Error Loss for Angle Regression（方向回归）"></a>Sine-Error Loss for Angle Regression（方向回归）</h3><p>作者在RPN中增加了一个direction classifer分支，将车头是否区分正确直接通过一个softmax loss来进行约束。如果θ&gt;0则为正，θ&lt;0则为负，将其转换为了一个简单的二分类问题。<br>$$<br>L_{\theta}&#x3D;SmoothL1(\sin{(\theta_p-\theta_t)})<br>$$</p>
<p>它可以很好的解决0和Π两个角度的对抗样本问题，也可以根据角度偏移对IoU进行建模。</p>
<h3 id="Focal-Loss-for-Classification"><a href="#Focal-Loss-for-Classification" class="headerlink" title="Focal Loss for Classification"></a>Focal Loss for Classification</h3><p>该网络产生的约70k个锚点中，只有约4k~6k是有用的。作者引入RetinaNet中的单级损失single-stage loss，即focal loss<br>$$<br>FL(p_t)&#x3D;-\alpha_t(1-p_t)^{\gamma}\log(p_t)<br>$$<br>pt是模型的估计概率，α&#x3D;0.25，γ&#x3D;2.</p>
<h3 id="总训练损失"><a href="#总训练损失" class="headerlink" title="总训练损失"></a>总训练损失</h3><p>$$<br>L_{total}&#x3D;\beta_1L_{cls}+\beta_2(L_{reg-\theta}+L_{reg-other})+\beta_3L_{dir}<br>$$</p>
<p>第一个损失函数是分类损失，第二个损失函数是新角度损失，第三个损失函数是位置和尺寸回归损失，第四个损失函数是方向分类损失。β1&#x3D;1.0、β2&#x3D;2.0、β3&#x3D;0.2（将β3使用较小的值，避免网络难以识别物体发方向情况）</p>
<h2 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h2><h3 id="从数据库中采样Ground-Truths"><a href="#从数据库中采样Ground-Truths" class="headerlink" title="从数据库中采样Ground Truths"></a>从数据库中采样Ground Truths</h3><ol>
<li>从训练数据集生成一个包含所有Ground Truths及其相关点云数据（ground truths的三维box中所有点）</li>
<li>从数据库中随机选中几个ground truths，通过串联方式引入当前训练的点云中（可以增加训练中ground truths点的数量，以模拟不同环境中的物体）</li>
<li>进行碰撞测试，删除任何与其他物体碰撞的采样对象</li>
</ol>
<h3 id="目标噪音"><a href="#目标噪音" class="headerlink" title="目标噪音"></a>目标噪音</h3><p>作者使用voxelNet方法对每个ground truth与其中点云独立、随机的进行转变。</p>
<h3 id="全局旋转和放缩"><a href="#全局旋转和放缩" class="headerlink" title="全局旋转和放缩"></a>全局旋转和放缩</h3><p>作者对全部点云以及所有ground truth box进行全局放缩和旋转。从[0.95,1.05]的均匀分布提取局部噪音，从[-Π&#x2F;4,Π&#x2F;4]提取全局噪音</p>
<h2 id="网络细节"><a href="#网络细节" class="headerlink" title="网络细节"></a>网络细节</h2><p>作者使用一大一小两个网络，在摄像机视野外的点被舍弃。</p>
<h3 id="汽车检测任务"><a href="#汽车检测任务" class="headerlink" title="汽车检测任务"></a>汽车检测任务</h3><p>在SECOND中使用两个VFE层，即大型网络的VFE(32)和VFE(128)，较小的网络的VFE(32)和VFE(64)，在线性(128)层之后。因此，输出稀疏张量的维数对于大型网络为128  × 10 × 400 × 352，对于小型网络为128 × 10 × 320 ×  264。然后，我们使用两阶段稀疏CNN进行特征提取和降维。每个卷积层遵循一个BatchNorm层和一个ReLU层。所有稀疏卷积层都有一个64-output  feature map，核大小为(3,1,1)核大小，stride为(2,1,1)。对于大型网络，中间块的输出维数为64 × 2 × 400 ×  352。一旦输出被重塑为128 × 400 × 352，就可以应用RPN网络。我们使用Conv2D(cout, k,  s)来表示con2d - batchnorm - relu层，使用DeConv2D(cout, k, s)来表示DeConv2D- batchnorm -  relu层，其中cout为输出通道数，k为内核大小，s为stride。因为所有层在所有维度上都有相同的大小，所以我们对k和s使用标量值。所有Conv2D层都有相同的填充，所有DeConv2D层都有零填充。在我们的RPN的第一阶段，应用了三个Conv2D(128,  3,1(2))层。然后，在第二阶段和第三阶段分别应用5个Conv2D(128, 3, 1(2))层和5个Conv2D(256, 3,  1(2))层。在每一阶段中，只有第一卷积层的s &#x3D; 2;否则，s &#x3D; 1。我们对每个阶段的最后一次卷积应用一个单一的DeConv2D(128, 3,  s)层，三个阶段的s依次为1、2和4。</p>
<p><img src="/%5Cimages%5Cimage-20220112220946220.png" alt="image-20220112220946220"></p>
<h3 id="行人和骑自行车者检测任务"><a href="#行人和骑自行车者检测任务" class="headerlink" title="行人和骑自行车者检测任务"></a>行人和骑自行车者检测任务</h3><p>与汽车检测方面唯一的区别是RPN中第一个卷积层的步幅为1而不是2。</p>
<h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p>在KITTI的验证集上，该网络无论大小网络都具有极高的平均准确度，以及极快的处理速度。同时该网络的角度编码速度、收敛速度也是非常快的。</p>
<p><img src="/%5Cimages%5Cimage-20220112222654032.png" alt="image-20220112222654032"></p>
<h3 id="汽车检测任务-1"><a href="#汽车检测任务-1" class="headerlink" title="汽车检测任务"></a>汽车检测任务</h3><p>该网络在检测汽车时展示出了极强的性能，尤其是该网络可以有效的检测被遮挡的汽车。但是对于获得数据量较少的汽车任然无法做到准确检测，尤其是对于<strong>点数小于10的车辆</strong></p>
<h3 id="行人和骑自行车者检测任务-1"><a href="#行人和骑自行车者检测任务-1" class="headerlink" title="行人和骑自行车者检测任务"></a>行人和骑自行车者检测任务</h3><p>对行人和自行车的检测出现了更多的<strong>假阳性与假阴性</strong>，一些预测甚至出现在不合理的位置。这些问题可能归因于行人和自行车的实例包含的点更少，<strong>容易与其他点或是噪音混淆</strong>。此外，行人和自行车数量相对较少，导致包含他们的voxel数量较少，训练效果也就较差。过滤不相关信息并基于二维检测结果确定目标位置，应该会解决这个问题。</p>
<h1 id="做出的贡献"><a href="#做出的贡献" class="headerlink" title="做出的贡献"></a>做出的贡献</h1><ol>
<li>将稀疏卷积应用于基于激光雷达的目标检测</li>
<li>提出了一种改进的稀疏卷积方法，显著提升训练与推理的速度</li>
<li>引入一种新的<strong>角度损失回归</strong>方法</li>
<li>，提高收敛速度和性能</li>
</ol>
]]></content>
      <tags>
        <tag>deep learning</tag>
        <tag>Object Detection</tag>
      </tags>
  </entry>
</search>
