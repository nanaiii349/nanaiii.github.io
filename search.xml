<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>F-PointNet</title>
    <url>/2022/03/08/F-PointNet/</url>
    <content><![CDATA[<p>论文：<a href="https://openaccess.thecvf.com/content_cvpr_2018/html/Qi_Frustum_PointNets_for_CVPR_2018_paper.html">CVPR 2018 Open Access Repository (thecvf.com)</a></p>
<span id="more"></span>

<h1 id="论文概述"><a href="#论文概述" class="headerlink" title="论文概述"></a>论文概述</h1><img src="\images\image-20220117100417479.png" alt="image-20220105165859553" style="zoom:100%;" />


<p>每个对象都由一个类（k个预定义类）和一个模态三维box表示。即使对象的一部分被遮挡或是截断，模态框也会得到完整的对象。</p>
<h2 id="关键流程"><a href="#关键流程" class="headerlink" title="关键流程"></a>关键流程</h2><ol>
<li>利用一个二维CNN对象检测器提取二维区域并对内容进行分类</li>
<li>将二维区域提升到三维，并形成截锥方案</li>
<li>通过对截锥中每个点二值分类，对对象实例进行分割（截锥点云：n×c、n个点、c个通道）</li>
<li>基于分割后的点云（m×c），使用T-Net平移对齐点，使得质心接近模态盒中心</li>
<li>使用框估计网络估计得出模态三维box</li>
</ol>
<h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><h3 id="截锥建议"><a href="#截锥建议" class="headerlink" title="截锥建议"></a>截锥建议</h3><img src="\images\image-20220117115726571.png" alt="image-20220105165859553" style="zoom:100%;" />

<p>由于现有的实时深度探测器得到的分辨率要低于RGB图像，因此先使用RGB图像通过二维目标检测框出目标物体的box，再通过投影矩阵，将二维box提升为三维搜索空间的截锥。接着收集截锥内部的所有点云，形成<strong>锥形体点云</strong>。还需要通过将圆台旋转到中心视图来对圆台进行<strong>归一化</strong>，使得圆台的中轴与图像平面正交（上图b）。这种归一化有利于提高算法发旋转不变性。</p>
<p>二维的目标检测网络使用在ImageNet分类数据集和COCO对象检测数据集上预先训练的模型权重，并且在KITTI二维对象检测数据集上进一步调整模型权重，以对二维box进行分类和预测。</p>
<h3 id="三维实例分割"><a href="#三维实例分割" class="headerlink" title="三维实例分割"></a>三维实例分割</h3><p>获得二维box和对应的三维截锥后，有几种可以获取对象三维坐标的方法</p>
<ol>
<li>直接通过深度图使用二维CNN网络，进行三维位置的回归。容易与遮挡物体或是背景噪声混淆</li>
<li>在三维点云中进行分割处理，使用<strong>基于PointNet的网络</strong>处理截锥内的点</li>
</ol>
<p>本篇文献采用的是第二种方法，从截锥中获取点云并预测每个点的概率分数，该分数表示该点属于某个对象的可能性有多大。而一个点只可能属于一个特定的对象，此时其他点属于无关点。同时，该网路还学习遮挡与噪声对目标检测的影响。</p>
<p>在多类检测任务中，还利用二维检测器的信息提供更好的实例分割。例如，二维检测器检测出对象是行人后，网络会特别的针对类似人的特征进行检测。实际中，是通过将语义类别编码为一个one-hot的k维类向量（代表希望检测的k个对象），并将这个向量连接到中间点云特征中。</p>
<p>三维实例分割出来后，提取出来被分类为感兴趣的对象的点。对这些点再进一步归一化（上图c），以提高坐标的平移不变性。需要注意的是，此处没有进行点云的缩放，因为对象的大小对于框的估计也有重要的作用，</p>
<h3 id="模态三维盒估计"><a href="#模态三维盒估计" class="headerlink" title="模态三维盒估计"></a>模态三维盒估计</h3><p>尽管使用了特征对齐，该网路仍存在坐标系的原点仍然离模态盒中心很远。在使用一个轻量化的PointNet（T-Net）来重新估计完整对象的真实中心，然后转换坐标，使预测中心成为原点。这里的T-Net可以看作为新的一种空间transformer网络，并且明确的监督平移网络来预测mask坐标原点到真实物体中心的<strong>中心残差</strong>。该网路结构与PointNet和PointNet++相似，不过输出的是三维box的参数。</p>
<p>估计box中心时使用残差方法，将box估计网络预测的中心和T-Net以及遮挡点形心得到的预测中心相结合获得绝对中心<br>$$<br>C_{pred}&#x3D;C_{mask}+\Delta C_{T-Net}+\Delta C_{box-net}<br>$$<br>对于box的尺寸和朝向，使用Faster r-cnn的预测朝向方法来进行预测。即预定义了NS个模板和NH个等分割角度的box。网络将这些分类到预定义的类别中，并预测每个类别的参差数</p>
<h2 id="多任务训练损失"><a href="#多任务训练损失" class="headerlink" title="多任务训练损失"></a>多任务训练损失</h2><img src="\images\image-20220118222715021.png" alt="image-20220105165859553" style="zoom:100%;" />


<p><strong>角损</strong>对于网络结果的影响还是很大的，本质上，角损时预测框和ground truth box八个角的距离之和。由于角点位置由中心、大小和方向共同决定，因此角点损失能够对这些参数的多任务训练进行正则化。<br>$$</p>

L_{corner}=\sum^{NS}_{i=1}\sum^{NH}_{j=1}\delta_{ij}\min{\{\sum^8_{k=1}\parallel p_k^{ij}-p^{*}_k \parallel,\sum^8_{k=1}\parallel p_k^{ij}-p^{**}_k \parallel\} }

<p>$$<br>构造NS×NH个包含所有尺寸和朝向的anchors,取值时使用原始和翻转情况最小的值，避免由于翻转航向造成较大损失。δ是ground truth的大小&#x2F;朝向类，是一个二维掩码用来选择距离项。</p>
<h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><img src="\images\image-20220117114100396.png" alt="image-20220105165859553" style="zoom:100%;" />


<p>该网络的优势：</p>
<ol>
<li>对于合理距离内的非遮挡物体的情况，可以得出非常精确的三维box</li>
<li>并且对于部分获取到的点数很少的物体（平行停放的汽车）也可以正确的预测三维box</li>
<li>在二维box相互重叠的情况下，转换到三维空间后处理起来就容易了许多。</li>
</ol>
<p>同时，实验也暴露出该网路存在的一些问题：</p>
<ol>
<li>由于稀疏点云，造成不准确的姿态和大小估计。这个可以通过对图像特征的进一步提取解决</li>
<li>如果截锥中有同一类别的多个实例（两个人站在一起），因为网络假设的是一个截锥只有一个对象，所以出现多个对象的时候，可能会产生混淆，从而输出混合分割结果。可以通过在每个截锥中设置多个三维box缓解</li>
<li>二维检测器会因为湖南的灯光或强遮挡错过目标，如果二位检测没有检测到物体，转换到三维空间时自然会忽略该物体。因此可以借助与BEV图像缓解。</li>
</ol>
<h1 id="做出的贡献"><a href="#做出的贡献" class="headerlink" title="做出的贡献"></a>做出的贡献</h1><ol>
<li>提出一种基于RGB-D数据的三维目标检测算法</li>
<li>提供广泛的定量评估来验证该算法的设计选择，以及丰富的定性结果来理解该方法的优势和局限性</li>
<li>展示了如何在该框架下训练3D对象检测器，并在标准的3D对象检测基准上实现最先进的性能。</li>
</ol>
<p>[^预测朝向方法]: A. Mousavian, D. Anguelov, J. Flynn, and J. Kosecka. 3d bounding box estimation using deep learning and geometry.、Faster r-cnn: Towards real-time object detection with region proposal networks.</p>
]]></content>
      <tags>
        <tag>论文阅读</tag>
        <tag>deep learning</tag>
        <tag>Object Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>CenterPoint</title>
    <url>/2022/03/08/CenterPoint/</url>
    <content><![CDATA[<p>论文：<a href="https://arxiv.org/pdf/2006.11275.pdf">2006.11275.pdf (arxiv.org)</a></p>
<span id="more"></span>
<h1 id="论文概述"><a href="#论文概述" class="headerlink" title="论文概述"></a>论文概述</h1><p>centerpoint是通过关键点检测来查询物体的中间位置和其特征，用<strong>两阶段</strong>的目标检测，第一阶段使用经典基于雷达激光的骨干网络如：VoxelNet、PointPillars来对输入点云进行处理，然后将表示转换为<strong>鸟瞰图</strong>，并使用标准的<strong>基于图像</strong>的关键点检测器来查找<strong>对象中心</strong>。这样对于每个检测中心可以通过中心位置的点特征来回归物体诸如三维大小、速度等特征。在第二阶段是轻量级的来<strong>细化对象位置</strong>。第二阶段提取估计对象三维box的每个面的三维中心点特征（具体来说只有四个向外的面）。该算法可以恢复由于步长和受限制的视野域带来的局部几何信息的丢失，以较小的成本带来了较好的性能提升。<br><img src="\images\image-20220121125306002.png" alt="image-20220105165859553" style="zoom:100%;" /></p>
<p>对于<strong>每一个点</strong>，使用<strong>双线性插值</strong>从地图视角的主干网输出中提取一个特征。接着，将提取的点特征连接起来，并将他们通过一个<strong>MLP</strong>传递。第二阶段在第一阶段的预测结果上预测一个与类无关的<strong>信心分数</strong>和对于box进一步细化。</p>
<p>可信得分：<br>$$<br>I&#x3D;\min(1,\max{(0,2\times IoU_t-0.5}))<br>$$<br>IoUt是第t个proposal box与ground-truth间的IoU</p>
<p>二元交叉熵损失：<br>$$<br>L_{socre}&#x3D;-I_t\log(\hat{I_t})-(1-I_t)\log(1-\hat{I_t})<br>$$<br>It即为可信得分</p>
<p>对于<strong>框回归</strong>，模型预测在第一阶段建议之上的细化，用L1损失训练模型。两阶段CenterPoint简化并加速了之前计算复杂度较高的的基于PointNet特征提取器和RoIAlign操作的两阶段3D检测器。</p>
<h2 id="Center-heatmap-head"><a href="#Center-heatmap-head" class="headerlink" title="Center heatmap head"></a>Center heatmap head</h2><p>Center heatmap head的目标就是在任何被探测的物体中心位置产生一个<strong>热力图峰值</strong>，Center heatmap head最终会产生一个k通道的热力图，一个通道代表了K类物体中的一种。在训练过程中，它将标注的box的三维中心投影到地图视图中，以生成二维高斯目标。使用<strong>focal loss</strong>。地图视角有图像视角不具备的优势，例如在地图视角下，汽车所占的比例很小，而在image视角下所占的比例可能会很大。此外，透视投影中对深度的压缩使得物体间的中心距离比image视角下更加接近。作者还通过放大每个ground truth的中心位置的高斯峰值来增强目标热力图的正监督，来抵消CenterNet带来的监督信号稀疏问题（使得大多数位置被认为是背景）。模型可以从附近的像素中得到更密集的监督。</p>
<h2 id="Regression-heads"><a href="#Regression-heads" class="headerlink" title="Regression heads"></a>Regression heads</h2><p>对象的中心有如下几个特征：子体素o、高度h、三维大小s以及一个偏向旋转角度。子体素位置减少了骨干网的体素化和步长带来的误差。高度h帮助在三维中定位对象，并添加被地图视角删除的高度信息。方向预测使用sin、cos作为一个连续的回归目标。结合框的大小，这些Regression heads可以提供完整的物体状态信息。每个输出使用他自己的head，作者使用L1损失训练。在推理时，通过在每个对象的峰值位置对索引密集回归头输出来提取所有属性。</p>
<h2 id="Velocity-head-and-tracking"><a href="#Velocity-head-and-tracking" class="headerlink" title="Velocity head and tracking"></a>Velocity head and tracking</h2><p>为了通过时间来跟踪物体，作者使用二维速度进行估计，并作为一个额外的回归输出。同时将前一帧中的点转换并归并到当前参考帧中，并通过时间差(速度)来预测当前和过去参考帧中物体位置的差异，来构建时间点云序列。作者对于速度回归也使用L1损失进行训练。</p>
<p>在推理时，使用贪心策略通过偏移量将当前检测与过去检测关联起来。即通过应用负速度估计将当前帧中目标中心投影回前一帧，然后通过最近距离匹配将其与跟踪目标进行匹配。</p>
<h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><img src="\images\image-20220121125338945.png" alt="image-20220105165859553" style="zoom:100%;" />
]]></content>
      <tags>
        <tag>论文阅读</tag>
        <tag>deep learning</tag>
        <tag>Object Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>DETR</title>
    <url>/2022/03/04/DETR/</url>
    <content><![CDATA[<p>论文：[<a href="https://arxiv.org/abs/2010.04159">2010.04159] Deformable DETR: Deformable Transformers for End-to-End Object Detection (arxiv.org)</a></p>
<span id="more"></span>
<h1 id="前人的工作"><a href="#前人的工作" class="headerlink" title="前人的工作"></a>前人的工作</h1><h2 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h2><p>注意力机制可以理解为，计算机视觉系统在模拟人类视觉系统中可以迅速高效地关注到重点区域的特性。<br>$$<br>Attention&#x3D;f(g(x),x)<br>$$<br>g(x)表示对输入特征进行处理并产生注意力的过程，f(g(x),x)表示结合注意力对输入特征进行处理的过程</p>
<p>self-attention模型：<br>$$<br>Q,K,V&#x3D;Linear(x)\<br>g(x)&#x3D;Softmax(QK)\<br>f(g(x),x)&#x3D;g(x)V<br>$$<br>senet模型:<br>$$<br>g(x)&#x3D;Sigmoid(MLP(GAP(x)))\<br>f(g(x),x)&#x3D;g(x)x<br>$$</p>
<p>注意力又可以细分为：通道注意力、空间注意力、时间注意力、分支注意力以及两种组合注意力：通道-空间注意力、空间-时间注意力</p>
<p><strong>通道注意力：</strong>将输入的特征图，经过<strong>基于宽度与高度</strong>的global max pooling 和global average pooling，然后分别经过MLP。将MLP输出的特征进行基于element-wise的加和操作，再经过sigmoid激活操作，生成最终的channel attention featuremap。将该channel attention featuremap和input featuremap做element-wise乘法操作，生成Spatial attention模块需要的输入特征。</p>
<p><strong>空间注意力：</strong>将Channel attention模块输出的特征图作为本模块的输入特征图。首先做一个<strong>基于channel</strong>的global max pooling 和global average pooling，然后将这2个结果基于channel 做concat操作。然后经过一个卷积操作，降维为1个channel。再经过sigmoid生成spatial attention feature。最后将该feature和该模块的输入feature做乘法，得到最终生成的特征。</p>
<p>Transformers网络包含self-attention和cross-attention机制，其主要的问题是时间开销、内存开销过高。现有许多思路来解决这个问题</p>
<ol>
<li>使用预定义的稀疏注意力模式，最直接的范式就是将注意力模式限制到固定的局部窗口。而这种方法会丧失全局信息。为了补偿对全局信息的提取，可以增加关键元素的接受域，或是允许少量特殊令牌访问所有关键元素，或是添加一些预定义的稀疏注意模式，直接注意远处的关键元素</li>
<li>学习数据依赖的稀疏注意力，基于注意力的局部敏感数据哈希算法，将查询和关键元素散列到不同的容器中，或是使用k-means找到最相关的关键元素，或是学习block-wise稀疏注意力的block排序</li>
<li>探索自我注意力的低秩性质，通过尺寸维度而不是通道维度的线性投影来减少关键元素数量，或是通过内核化近似重新计算自注意力</li>
</ol>
<p>本篇论文使用的可变性注意力是受可变性卷积启发，属于第二类，只关注从查询元素的特征中预测一个小的固定采样点集合。在相同FLOPS下，变形注意力要比传统卷积略慢。</p>
<h2 id="目标检测的多尺度特征表示"><a href="#目标检测的多尺度特征表示" class="headerlink" title="目标检测的多尺度特征表示"></a>目标检测的多尺度特征表示</h2><p>在目标检测任务中，一张图像内真实对象的尺寸差别巨大，这也成为目标检测的一大困难。现代物体检测器通常利用多尺度特征来解决。FPN提出一个自顶向下路径融合多尺度特征；PANet进一步添加一条自底向上的路径到FPN顶部；或是结合通过一个全局注意力操作提取出的从所有尺寸中的特则；或是使用U型模型来融合多尺度特征。最近，NAS-FPN、Auto-FPN提出通过神经网络搜索自动设计交叉注意力联系；BiFPN是PANet的重复简化版本</p>
<p>本篇论文使用多尺度可变性的注意力模块可以通过注意力机制自然的将多尺度特征累加起来，无需借助特征金字塔网络</p>
<h2 id="Transformers中的多头检测"><a href="#Transformers中的多头检测" class="headerlink" title="Transformers中的多头检测"></a>Transformers中的多头检测</h2><p>Transformers是基于机器翻译的注意力机制的网络架构。为了使模型能够关注不同表示子空间和不同位置的内容，将不同注意力头的输出以可学习的权值线性聚合。</p>
<h2 id=""><a href="#" class="headerlink" title=""></a></h2><p>DETR基于Transformer encoder-decoder框架，合并了set-based 匈牙利算法，通过二分图匹配，强制每一个ground-truth box都有唯一的预测结果（通过该算法找优化方向，哪个ground-truth由哪个slot负责）</p>
<h2 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h2><ol>
<li>DETR训练周期长，到达收敛状态的时间长，初始化时，图上各个位置的权重相同，而在训练结束时，权重只集中在图像中出现物体的位置，这里权重的更新似乎需要经过很多轮训练才能达到收敛</li>
<li>对小目标物体检测不友好，DETR使用多尺度特征处理小目标，而高分辨率的特征图会大大提高DETR复杂度</li>
</ol>
<h2 id="关键过程"><a href="#关键过程" class="headerlink" title="关键过程"></a>关键过程</h2><ol>
<li>通过CNN骨干网络将输入特征提取出来。DETR利用标准的Transformer编码器-解码器体系结构将输入特征映射转换为一组对象查询的特征。在目标查询特征(由解码器产生)上添加一个三层前馈神经网络(FFN)和一个线性投影作为检测头。</li>
<li>对于DETR的编码器，查询和关键元素都是特征图中的像素。编码器输入是ReaNet特征图，自注意的计算复杂度为O(h^2w^2c)，随空间大小呈二次型增长。</li>
<li>对于DETR解码器，输入包括编码器中的特征图和N个由可学习位置嵌入表示的对象查询。解码器中存在两类注意模块，即<strong>交叉注意模块</strong>和<strong>自我注意模块</strong>。在交叉注意模块中，对象查询从特征映射中提取特征。查询元素是对象查询的元素，关键元素是编码器的输出特征映射的元素。复杂度随特征映射的空间大小呈线性增长。在自注意模块中，对象查询相互交互，以捕获它们之间的关系。查询和关键元素都是对象查询。因此，对于适度数量的对象查询，复杂性是可以接受的。</li>
</ol>
<h1 id="论文概述"><a href="#论文概述" class="headerlink" title="论文概述"></a>论文概述</h1><img src="\images\image-20220119220603117.png" alt="image-20220105165859553" style="zoom:100%;" />


<h2 id="端到端适用于目标检测的可变性transformers模型"><a href="#端到端适用于目标检测的可变性transformers模型" class="headerlink" title="端到端适用于目标检测的可变性transformers模型"></a>端到端适用于目标检测的可变性transformers模型</h2><p>模型使用ResNet-50作预训练</p>
<h2 id="可变性注意力模块"><a href="#可变性注意力模块" class="headerlink" title="可变性注意力模块"></a>可变性注意力模块</h2><p>不同于传统注意力模块注意图像中的所有位置，可变性注意力模块只关注参考点周围一小组关键采样点，无需考虑特征图的空间大小，即为每个查询只分配少量固定数量的关键点。<br>$$<br>DeformAttn(z_q,p_q,x)&#x3D;\sum^M_{m&#x3D;1}W_m[\sum^K_{k&#x3D;1}A_{mqk}\times W^{‘}<em>mx(p_q+\Delta p</em>{mqk})],<br>$$<br>输入特征是C×H×W维的；q表示具有Zq个上下文特征、二维参考点Pq的查询元素；m表示注意力头；k表示采样的关键点的索引，K为采样关键点的总数，Δp和A分别为检测头的采样偏移量和权重</p>
<h2 id="多尺度可变性注意力模块"><a href="#多尺度可变性注意力模块" class="headerlink" title="多尺度可变性注意力模块"></a>多尺度可变性注意力模块</h2><p>$$<br>MSDeformAttn(z_q,\hat{p_q},{x^l}^L_{l&#x3D;1})&#x3D;\sum^M_{m&#x3D;1}W_m[\sum^L_{l&#x3D;1}\sum^K_{k&#x3D;1}A_{mlqk}\times W^{‘}<em>m x^l (\phi_l(\hat{p_q})+\Delta p</em>{mlqk})],<br>$$</p>
<p>x是多尺度特征输入图，l是输入特征的等级，k为采样点。多尺度变形注意与之前的单尺度版本非常相似，不同的是它从多尺度特征映射中采样LK点，而不是从单尺度特征映射中采样K点。Φ将归一化坐标重新转换为第l层的特征图。</p>
<p>可变形卷积是为单尺度输入而设计的，每个注意力头只关注一个采样点。然而，多尺度变形注意从多尺度输入中查看多个采样点。所提出的(多尺度)可变形注意模块也可以被视为Transformer注意的有效变体，其中可变形采样位置引入了<strong>预滤波机制</strong>（预先过滤不重要的点，降低计算复杂度）。当采样点遍历所有可能的位置时，所提出的注意模块相当于Transformer注意。</p>
<h2 id="可变性transformer编码器"><a href="#可变性transformer编码器" class="headerlink" title="可变性transformer编码器"></a>可变性transformer编码器</h2><p>我们用提出的多尺度可变形注意模块替换DETR中的Transformer注意模块处理特征映射。该编码器的输入和输出都是具有相同分辨率的多尺度特征图。关键元素和查询元素都是多尺度特征图中的像素。对于每个查询像素，参考点就是它本身。为了确定每个查询像素所处的特征级别，除了位置嵌入之外，我们还在特征表示中添加了尺度级嵌入(记作el)。与固定编码的位置嵌入不同，尺度级嵌入{el}Ll&#x3D;1是随机初始化并与网络联合训练的。可变形变压器编码器的参数在不同的特征层之间共享。</p>
<h2 id="可变性transformer解码器"><a href="#可变性transformer解码器" class="headerlink" title="可变性transformer解码器"></a>可变性transformer解码器</h2><p>解码器中存在交叉注意模块和自注意模块。两种类型的注意模块的查询元素都是对象查询。在交叉注意模块中，对象查询从特征映射中提取特征，其中关键元素是编码器的输出特征映射。在自注意模块中，对象查询相互交互，其中的关键元素是对象查询。（这里和DETR网络相似）由于提出的变形注意模块是为处理卷积特征映射作为关键元素而设计的，所以只将每个交叉注意模块替换为多尺度变形注意模块，而保持自我注意模块不变。</p>
<p>由于多尺度可变形注意模块提取参考点周围的图像特征，通过让检测头相对于参考点的偏移量预测边界盒，可以进一步降低优化难度。</p>
<p>通过将DETR中的Transformer注意模块替换为可变形注意模块，建立了一个高效、快速收敛的检测系统，称为可变形DETR</p>
<h2 id="迭代边界框优化"><a href="#迭代边界框优化" class="headerlink" title="迭代边界框优化"></a>迭代边界框优化</h2><p>为了提高检测性能，作者建立了一种简单有效的迭代边界盒优化机制。这里，每个解码器层都根据前一层的预测来细化边界框。</p>
<h2 id="两阶段可变性DETR"><a href="#两阶段可变性DETR" class="headerlink" title="两阶段可变性DETR"></a>两阶段可变性DETR</h2><p>在原始DETR中，解码器中的对象查询与当前图像无关。作者使用两阶段目标检测思想，第一阶段使用可变性DETR生成区域建议，生成的区域建议将作为对象查询提供给解码器以进一步细化，形成一个两阶段的可变形DETR。</p>
<p>在第一阶段，为了实现高召回建议，多尺度特征图中的每个像素都将作为对象查询。然而，直接将对象查询设置为像素会给解码器中的自注意模块带来不可接受的计算和内存开销。为了避免这个问题，去掉了解码器，并形成了一个只有编码器的可变形DETR来生成区域建议。即每个像素被赋值为一个对象查询，直接预测一个边界框。得分最高的边界框被选为区域建议。在将区域建议提交到第二阶段之前，不应用NMS。</p>
<h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><img src="\images\image-20220120132237764.png" alt="image-20220105165859553" style="zoom:100%;" />]]></content>
      <tags>
        <tag>论文阅读</tag>
        <tag>deep learning</tag>
        <tag>Object Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>Pandas学习（一）</title>
    <url>/2020/05/10/Learning-pandas-01/</url>
    <content><![CDATA[<h1 id="pandas"><a href="#pandas" class="headerlink" title="pandas"></a>pandas</h1><p><strong>Pandas</strong> 是python的一个数据分析包，最初由AQR Capital<br>Management于2008年4月开发，并于2009年底开源出来，目前由专注于Python数据包开发的PyData开发team继续开发和维护，属于PyData项目的一部分。Pandas最初被作为金融数据分析工具而开发出来，因此，pandas为**<br>时间序列分析**提供了很好的支持。</p>
<p><strong>Pandas官网</strong>：<a href="https://pandas.pydata.org/">https://pandas.pydata.org/</a></p>
<p><strong>Pandas源码</strong>：<a href="https://github.com/pandas-dev/pandas.git">https://github.com/pandas-dev/pandas.git</a></p>
<span id="more"></span>

<h2 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h2><ul>
<li><strong>Series</strong>:<br>一维数组，与Numpy中的一维array类似。二者与Python基本的数据结构List也很相近。Series如今能保存不同种数据类型，字符串、boolean值、数字等都能保存在Series中。对于一个Series，其中最常用的属性为值（values），索引（index），名字（name），类型（dtype）。我们用<code>s = pd.Series(np.random.randn(5),index=[&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;,&#39;e&#39;],name=&#39;这是一个Series&#39;,dtype=&#39;float64&#39;)</code><br>这样的方式来创建一个Series。并使用<code>s.values;s.index;s.name;s.dtype</code><br>来访问Series的属性。同时Series可以调用相当多的方法，会保留NumPy的数组操作（用布尔数组过滤数据，标量乘法，以及使用数学函数），并同时保持引用的使用</li>
<li>Time- Series：以时间为索引的Series。</li>
<li><strong>DataFrame</strong><br>：二维的表格型数据结构。很多功能与R中的data.frame类似。可以将DataFrame理解为Series的容器。我们用<code>df = pd.DataFrame(&#123;&#39;col1&#39;:list(&#39;abcde&#39;),&#39;col2&#39;:range(5,10),&#39;col3&#39;:[1.3,2.5,3.6,4.6,5.8]&#125;,index=list(&#39;一二三四五&#39;))</code><br>来创建一个DataFrame。<strong>DataFrame取出一列变成Series</strong></li>
<li>Panel ：三维的数组，可以理解为DataFrame的容器。</li>
<li>Panel4D：是像Panel一样的4维数据容器。</li>
<li>PanelND：拥有factory集合，可以创建像Panel4D一样N维命名容器的模块。</li>
</ul>
<h2 id="文件的读取，写入"><a href="#文件的读取，写入" class="headerlink" title="文件的读取，写入"></a>文件的读取，写入</h2><p>pandas支持csv格式，txt格式，xls格式等诸多数据文件的读写。<br><img src="/images/pandas_01_01.png" alt="avatar"></p>
<p>在此注重介绍csv格式的读取。</p>
<h3 id="pandas-read-csv"><a href="#pandas-read-csv" class="headerlink" title="pandas.read_csv"></a>pandas.read_csv</h3><p><strong>pandas.read_csv(filepath_or_buffer: Union[str, pathlib.Path, IO[~AnyStr]], sep&#x3D;’,’, delimiter&#x3D;None, header&#x3D;’infer’,<br>names&#x3D;None, index_col&#x3D;None, usecols&#x3D;None, squeeze&#x3D;False, prefix&#x3D;None, mangle_dupe_cols&#x3D;True, dtype&#x3D;None, engine&#x3D;None,<br>converters&#x3D;None, true_values&#x3D;None, false_values&#x3D;None, skipinitialspace&#x3D;False, skiprows&#x3D;None, skipfooter&#x3D;0, nrows&#x3D;None,<br>na_values&#x3D;None, keep_default_na&#x3D;True, na_filter&#x3D;True, verbose&#x3D;False, skip_blank_lines&#x3D;True, parse_dates&#x3D;False,<br>infer_datetime_format&#x3D;False, keep_date_col&#x3D;False, date_parser&#x3D;None, dayfirst&#x3D;False, cache_dates&#x3D;True, iterator&#x3D;False,<br>chunksize&#x3D;None, compression&#x3D;’infer’, thousands&#x3D;None, decimal: str &#x3D; ‘.’, lineterminator&#x3D;None, quotechar&#x3D;’”‘, quoting&#x3D;0,<br>doublequote&#x3D;True, escapechar&#x3D;None, comment&#x3D;None, encoding&#x3D;None, dialect&#x3D;None, error_bad_lines&#x3D;True, warn_bad_lines&#x3D;True,<br>delim_whitespace&#x3D;False, low_memory&#x3D;True, memory_map&#x3D;False, float_precision&#x3D;None)</strong></p>
<p>这个函数参数太多，我第一眼看见的时候也是吓了一跳。不过在实际使用时，大家也只是用<code>df=pandas.read_csv(&#39;XXX.csv&#39;)</code>使用第一个参数(filepath_or_buffer)<br>：任何有效的字符串路径都是可以接受的，也可以是URL。</p>
<p>其他请访问官方文档：<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html</a></p>
<h2 id="基本函数"><a href="#基本函数" class="headerlink" title="基本函数"></a>基本函数</h2><p>在平时的数据处理中下列函数被大量的使用，提前整理如下：</p>
<h3 id="head和tail"><a href="#head和tail" class="headerlink" title="head和tail"></a>head和tail</h3><p><strong>DataFrame.head（self：〜FrameOrSeries，n：int &#x3D; 5 ） →〜FrameOrSeries</strong></p>
<p>此函数根据位置返回对象的前n行。这对于快速测试对象中的数据类型是否正确非常有用。</p>
<p>对于n的负值，此函数返回除最后n行之外的所有行，等效于df[:-n]</p>
<p><strong>DataFrame.tail(self: ~FrameOrSeries, n: int &#x3D; 5) → ~FrameOrSeries</strong></p>
<p>此函数与head大同小异，只是返回数据后n列。</p>
<p>对于n的负值，此函数返回除最前n行之外的所有行，等效于df[n:]</p>
<h3 id="unique和nunique"><a href="#unique和nunique" class="headerlink" title="unique和nunique"></a>unique和nunique</h3><p><strong>DataFrame.nunique(self, axis&#x3D;0, dropna&#x3D;True) → pandas.core.series.Series</strong></p>
<ul>
<li><p>axis:要使用的axis。行为0或’index’，列为1或’columns’。</p>
</li>
<li><p>dropna bool：默认为True，是否将NaN包括在内。</p>
</li>
</ul>
<p>该函数返回有多少个唯一值。</p>
<p><strong>Series.unique(self)</strong></p>
<p>该函数返回Series对象的唯一值</p>
<h3 id="count和value-counts"><a href="#count和value-counts" class="headerlink" title="count和value_counts"></a>count和value_counts</h3><p><strong>DataFrame.count(self, axis&#x3D;0, level&#x3D;None, numeric_only&#x3D;False)</strong></p>
<ul>
<li><p>axis：{0或’index’，1或’columns’}，默认0。如果为0或“索引”，则为每列生成计数 s。如果为1或“列”，则为每行生成计数 s 。</p>
</li>
<li><p>level：int或str，可选。如果axis是MultiIndex（分层，有多个索引），则沿特定级别计数，并折叠为DataFrame。一个str指定级别名称。</p>
</li>
</ul>
<p>该函数返回非缺失值元素个数。</p>
<p><strong>Series.value_counts(self, normalize&#x3D;False, sort&#x3D;True, ascending&#x3D;False, bins&#x3D;None, dropna&#x3D;True)</strong></p>
<ul>
<li><p>normalize boolean：将normalize设置为True时，通过将所有值除以值的总和来返回相对频率。</p>
</li>
<li><p>bins int：可选。不是对值进行计数，而是将它们分组到半开箱中，便于在pd.cut帮助下处理数据。</p>
</li>
</ul>
<p>该函数返回一个包含唯一值计数的Series。</p>
<h3 id="describe和info"><a href="#describe和info" class="headerlink" title="describe和info"></a>describe和info</h3><p><strong>DataFrame.describe(self: ~FrameOrSeries, percentiles&#x3D;None, include&#x3D;None, exclude&#x3D;None) → ~FrameOrSeries</strong></p>
<ul>
<li><p>percentiles：类似于数字的列表，可选。要包含在输出中的百分比。所有值都应介于0和1之间。默认值为 ，它返回第25、50和75个百分位数。[.25, .5, .75]</p>
</li>
<li><p>include ：“all”，类似dtype的列表或无（默认），可选。要包括在结果中的数据类型的白名单。<em>忽略Series。</em></p>
</li>
</ul>
<p>以下是include选项：</p>
<ul>
<li><p>‘all’：输入的所有列将包含在输出中。</p>
</li>
<li><p>类似于dtypes的列表：将结果限制为提供的数据类型。要将结果限制为数字类型，请提交 numpy.number。要将其限制为对象列，请提交numpy.object数据类型。字符串也可以select_dtypes（例如）的样式使用<br>。要选择熊猫分类列，请使用df.describe(include&#x3D;[‘O’])’category’</p>
</li>
<li><p>None（默认）：结果将包括所有数字列</p>
</li>
</ul>
<blockquote>
<p><em><strong>Notes：</strong></em></p>
<p><em>对于数字数据，则结果的指数将包括count， mean，std，min，max以及下，50和上百分。默认情况下，较低的百分比是25，较高的百分比是75。该50百分比是一样的中位数。</em></p>
<p><em>为对象的数据（例如字符串或时间戳），则结果的指数将包括count，unique，top，和freq。该top 是最常见的值。该freq是最常见的值的频率。时间戳记还包括first和last项目。</em></p>
<p><em>如果多个对象值具有最高的计数，则将 从具有最高计数的那些中任意选择count和top结果。</em></p>
<p><em>对于通过提供的混合数据类型DataFrame，默认值为仅返回对数字列的分析。如果数据框仅由对象和类别数据组成，而没有任何数字列，则默认值为返回对对象和类别列的分析。如果include&#x3D;’all’作为选项提供，则结果将包括每种类型的属性的并集。</em></p>
</blockquote>
<p><strong>DataFrame.info(self, verbose&#x3D;None, buf&#x3D;None, max_cols&#x3D;None, memory_usage&#x3D;None, null_counts&#x3D;None) → None</strong></p>
<p>此方法打印信息息有关包括索引D型和列dtypes，非空值和存储器使用量的数据帧。</p>
<ul>
<li><p>verbose boolean 是否打印完整的摘要。</p>
</li>
<li><p>buf 可写缓冲区，默认为sys.stdout。将输出发送到哪里。默认情况下，输出将打印到sys.stdout。如果需要进一步处理输出，请传递可写缓冲区。</p>
</li>
<li><p>memory_usage bool，str，可选。指定是否应显示DataFrame元素（包括索引）的总内存使用情况。<em>True始终显示内存使用情况。False永远不会显示内存使用情况。“<br>deep”的值等效于“真正的内省”。内存使用情况以可读单位（以2为基数的表示形式）显示。</em></p>
</li>
<li><p>null_counts bool，可选。是否显示非空计数。默认情况下，仅当框架小等于时显示。值为True始终显示计数，而值为False则不显示计数。</p>
</li>
</ul>
]]></content>
      <tags>
        <tag>计算机</tag>
      </tags>
  </entry>
  <entry>
    <title>Faster-RCNN</title>
    <url>/2022/03/04/Faster-RCNN/</url>
    <content><![CDATA[<p><strong>论文：</strong><a href="https://proceedings.neurips.cc/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks (neurips.cc)</a></p>
<span id="more"></span>
<h1 id="论文概述"><a href="#论文概述" class="headerlink" title="论文概述"></a>论文概述</h1><img src="\images\image-20220206101136285.png" alt="image-20220206101136285" style="zoom:75%;" />

<p>目前，先进的目标检测算法都是依赖于区域建议算法去假设物体位置的，区域建议的计算成为目标检测算法的瓶颈。而Faster-RCNN引入了<strong>区域建议网络RPN</strong>，该网络与检测网络共享全图像卷积特征，从而实现<strong>几乎无成本的区域建议</strong>。引入RPN网络也使得Faster-RCNN成为了一个完全的end-to-end的CNN目标检测模型。</p>
<h2 id="区域生成网络RPN"><a href="#区域生成网络RPN" class="headerlink" title="区域生成网络RPN"></a>区域生成网络RPN</h2><p>RPN网络代替了Fast-RCNN模型中的selective search（SS）。它先通过对应关系把特征图的点映射回原图，在每个对应的原图设计不同的固定尺度窗口bbox，根据该窗口的ground truth的IoU给它标记正负标签，让它学习里面是否有目标。通过一个简单的交替优化，RPN和Fast-RCNN可以共享卷积特征。为了使RPN与Fast-RCNN相统一，作者提出了一个训练方案，该方案在区域建议任务和目标检测任务间交替进行微调，同时保证proposal的稳定。这种方案收敛很快，并且使两个网络可以共享数据。而SS是使用CPU计算，因而计算时间较慢，也没法共享计算。</p>
<p>在RPN中只需要找出物体的大致地方，因此作者对bbox做了三个固定：固定尺度变化（三种尺度）、固定scale ratio变化（三种ratio），固定采样方式（只在特征图的每个点在原图中的对应RoI上采样）</p>
<p>作者在Fast-RCNN的基础上增加了两个卷积层来构建RPN：一个将每个conv map编码到一个短的特征向量上，另一个卷积层在每个conv map上输出客观性的评分以及关于k个涉及到不同的尺度和纵横比的区域建议的回归边界（通常取k为9）</p>
<h2 id="RPN工作原理"><a href="#RPN工作原理" class="headerlink" title="RPN工作原理"></a>RPN工作原理</h2><img src="\images\image-20220106213651479.png" alt="image-20220206101136285" style="zoom:100%;" />

<p>RPN以一副图像（任意大小）作为输入并输出一组矩形对象建议并给出得分。为了生成区域建议，一个小network滑过最后一个共享的卷积层输出的特征图。该网络最后全连接到输入的卷积特征图的一个n×n的空间窗口中。每个滑动窗口都映射到一个低维向量上。这个向量被提供给两个同级的全连接层（边界框回归层reg、分类层cls）。</p>
<p>该体系结构由一个n×n的卷积层、和两个兄弟级的1×1层（reg、cls）并将ReLUs应用于n×n转换层的输出。</p>
<p>RPN最终输出的proposals会出现部分重叠。为了减少冗余，作者根据proposal的cls分数对proposal进行非极大抑制NMS（IoU阈值为0.7）。这使得每幅图像最终有大约2k个proposal。NMS之后，再对前n个proposal进行检测。实验证明，NMS在大幅减少proposal数量的情况下，不会损害最终的检测精度。</p>
<h2 id="Anchor"><a href="#Anchor" class="headerlink" title="Anchor"></a><strong>Anchor</strong></h2><p>Anchor是特征图中的每个点在原图中的对应位置，也就是初始检测框。</p>
<p>在每个滑动窗口位置上，作者同时预测<strong>k个区域proposal</strong>（一个点对应k个anchor），因此reg有4k个输出，来标记k个box的坐标；而cls有2k个输出来评估每个proposal是对象或不是对象的概率。这k个proposal是k个box的参数表示，即上面的Anchors。每个anchor是其滑动窗口的中心位置，并于比例和宽高比相关联。作者使用了3个尺度与3个纵横比，因此每个滑动位置放置9个anchor。一个W×H的卷积特征图就总共有W×H×k个anchor。因为anchor具有<strong>平移不变特性</strong>，对anchor的计算就是对对应该anchor的proposal的计算.和不是平移不变的 MultiBox 方法相比，该方法使参数减少了一个数量级。</p>
<p>作者对每个anchor使用128、256、512三个尺度以及一比一、一比二、二比一三个纵横比，发现即使物体比底层的接受域还要大，该算法仍然可以检测出物体。在实际中，人们也可以只通过观察物体的中间位置来感知物体。这也就作者的解决方案不需要使用多尺度特征或多尺度滑动窗口来预测大的物体，节省大量运行时间。</p>
<p>对于跨越图像边界的anchor。训练中，作者忽略所有跨越图像边界的anchor，以避免带来极大的误差，使得结果无法收敛。在测试中，对于这些anchor，作者只保留其图像中的部分</p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>作者将anchor打上正负标签。正标签：与ground-truth box重叠程度IoU最高的区域；或者IoU大于0.7的anchor。负标签:对于所有的ground-truth box其IoU都小于0.3的anchor。不属于正负样本的样本对于目标检测没有帮助。</p>
<p>根据这些定义，作者最小化Fast-RCNN中的多任务损失目标函数<br>$$</p>

L(\{p_i\},\{t_i\})=\frac{1}{N_{cls} }\sum{L_{cls}(p_i,p_i^{*})}+\lambda\frac{1}{N_{reg} }\sum{p_i^{*}L_{reg}(t_i,t_i^{*})}

<p>$$<br>此处的i为该batch中anchor的索引，pi是anchor为对象的概率，如果anchor为正pi*&#x3D;1，如果anchor为负pi*&#x3D;0；ti为预测边界框四个参数化坐标，ti*是与一个正anchor相关的ground-truth box。</p>
<p>Lcls（分类损失）：使用log loss；Lreg（回归损失）：使用smooth L1(R)，而只有anchor为正时，Lreg才会被激活。<br>$$<br>L_{reg}(t_i,t_i^{<em>})&#x3D;R(t_i,t_i^{</em>})<br>$$<br>作者使用Ncls、Nreg两项进行归一化，并使用lambda作为平衡因子。<br><img src="\images\image-20220108211045368.png" alt="image-20220206101136285" style="zoom:100%;" /></p>
<p>在回归时，要对4个坐标参数化。其中x,y,w,h为边界框box的中心位置、宽与高。x，xa，x*是预测box、anchor box、ground-truth box的参数。</p>
<p>不同于其他对任意大小区域进行特征池化（回归权值由所有区域大小共享）的边界回归，作者的回归特征在特征图上具有相同的大小。为了适用于不同的大小，作者设置了一组k边界回归器。每个回归器负责一个尺度和一个纵横比，并且这k个回归器不共享权重。</p>
<h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2><p>RPN网络是以图像为中心的。每个batch中都含有正负样本，对所有参数的优化会使得结果偏向负样本。因此作者随机对256个anchor进行采样，尽量使得正负样本的比例达到一比一（如果正样本小于128再用负样本填充）。</p>
<p>并且作者通过标准偏差为0.01的零均值高斯分布中提取权重来初始化所有层，所有层的初始化从一个ImageNet分类模型中获得，并且调优ZF网络、VGG的jconv3_1以节省内存</p>
<h2 id="将RPN应用于目标检测"><a href="#将RPN应用于目标检测" class="headerlink" title="将RPN应用于目标检测"></a>将RPN应用于目标检测</h2><p>作者将Fast-RCNN与RPN相结合，使得二者的卷积层计算可以共享。由于Fast-RCNN依赖于固定的对象proposal并且如果在学习 Fast R-CNN 的同时更改proposal机制，尚不清楚先验是否会收敛。因此作者没有简单的定义一个包含RPN与Fast-RCNN的网络，而是通过交替优化学习共享特征。</p>
<ol>
<li>使用ImageNet预训练模型初始化，并对区域建议任务做端到端的微调（<strong>训练RPN网络</strong>）</li>
<li>利用第一步生成的区域建议，训练一个独立的检测网络，该检测网络也使用ImageNet预训练模型初始化（<strong>训练FRCNN网络</strong>）</li>
<li><strong>使用检测网络对RPN网络进行初始化，但固定两个网络共有的层，只微调RPN网络特有的层。</strong>此时，两个网络共享卷积层（<strong>训练RPN网络</strong>）</li>
<li>固定共享的卷积层，微调Fast-RCNN的全连接层。（由此形成一个统一的网络）（<strong>训练FRCNN网络</strong>）</li>
</ol>
<p>在这里的训练过程类似于一种“迭代”的过程，不过只迭代了两次，因为迭代次数再增加不会带来性能上的提升。</p>
<h1 id="做出的贡献"><a href="#做出的贡献" class="headerlink" title="做出的贡献"></a>做出的贡献</h1><ol>
<li>设计卷积网络处理区域建议RPN，使得目标检测系统实现完全的end to end</li>
<li>使用多任务损失函数</li>
</ol>
]]></content>
      <tags>
        <tag>论文阅读</tag>
        <tag>deep learning</tag>
        <tag>Object Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>Fast-RCNN</title>
    <url>/2022/02/25/Fast-RCNN/</url>
    <content><![CDATA[<p>论文：<a href="https://openaccess.thecvf.com/content_iccv_2015/html/Girshick_Fast_R-CNN_ICCV_2015_paper.html">ICCV 2015 Open Access Repository (thecvf.com)</a></p>
<span id="more"></span>

<img src="\images\image-20220105165859553.png" alt="image-20220105165859553" style="zoom:150%;" />



<h1 id="论文概述"><a href="#论文概述" class="headerlink" title="论文概述"></a>论文概述</h1><p>Fast-RCNN是将RCNN与SPPNet相融合提出的一种新的网络。</p>
<h2 id="RoI层"><a href="#RoI层" class="headerlink" title="RoI层"></a>RoI层</h2><img src="\images\image-20220105171915109.png" alt="image-20220105165859553" style="zoom:100%;" />


<p>Fast-RCNN设计了一个RoI层用来对<strong>整张图像</strong>进行特征提取，再根据候选区域在原图中的位置挑选特征。</p>
<p>SPPNet提出了Spatial pyramid pooling技术可以将一张图像中卷积所得<strong>的不同尺寸的ROI映射为固定范围</strong>（H×W）的特征图，其中H、W是超参数。每个RoI都是由一个矩形窗口转化为一个卷积特征图。RoI有一个四元组（r,c,h,w）定义（r，c指定左上角坐标；h、w定义其高度和宽度）。</p>
<p><em>RoI max pooling工作原理：将h×w的RoI窗口分割为一个H×W网格，每个网格窗口大小为(h&#x2F;H)×(w&#x2F;W),每个子窗口值最大池化到相应的输出网格单元中。</em></p>
<h2 id="预训练网络"><a href="#预训练网络" class="headerlink" title="预训练网络"></a>预训练网络</h2><p>预训练网络转换为Fast-RCNN网络经过三个转变</p>
<ol>
<li>最后被最大的池化层被RoI层替代，通过设置H、W与网络的第一个全连接层兼容</li>
<li>网络最后的全连接层和softmax被一个全连接层超过K+1类别的softmax和一个特定类别的限定框回归器所代替</li>
<li>该网络接受两个数据输入：一个图像列表和这些图像中的RoI列表</li>
</ol>
<h2 id="微调检测"><a href="#微调检测" class="headerlink" title="微调检测"></a>微调检测</h2><p>在Fast-RCNN网络中，反向传播训练参数是一项重要的功能。（它客服了SPPNet不能更新空间金字塔层以下权值的问题）。由于SPPNet网络的每个训练样本（RoI）来自不同的图像时，SPP层的反向传播是非常低效的。</p>
<p>作者提出了一种更加有效的训练方法，利用了训练过程中的特征共享。即对随机梯度下降进行小批量的分级采样。首先对N幅图像进行采样，然后对每幅图像进行R&#x2F;N个RoI采样。重要的是，对于来自相同图像的RoI在向前和向后传递时共享计算和内存。不过这种训练方法存在训练收敛较慢的问题。（N&#x3D;2，R&#x3D;128时效果最好）</p>
<p>同时，在分层采样中，作者的一个精细调整阶段联合优化softmax分类器和边界框回归器，而不是用三个独立的阶段寻训练softmax分类器、支持向量机、回归器。</p>
<h2 id="多任务损失"><a href="#多任务损失" class="headerlink" title="多任务损失"></a>多任务损失</h2><p>$$<br>L(p,u,t^u,v)&#x3D;L_{cls}(p,u)+\lambda[u\ge 1]L_{loc}(t^u,v)<br>$$</p>
<h2 id="最小批次采样"><a href="#最小批次采样" class="headerlink" title="最小批次采样"></a>最小批次采样</h2><p>实验中，作者使用最小batch为128，对每张图像采样64个RoI。接着从这些proposals中获取区IoU大于等于的0.5的RoI(这些RoI中包含对象标记，u大于等于1)，剩余的RoI从IoU处于[0.1~0.5]的RoI中选取。再将IoU小于0.1的当作背景例子，标记u&#x3D;0.</p>
<p>这里也可以将IoU小于0.1的样本作为hard样本进行训练；训练过程中，图像以0.5的概率翻转，除此没有别的数据增强。</p>
<h2 id="通过RoI池层的反向传播"><a href="#通过RoI池层的反向传播" class="headerlink" title="通过RoI池层的反向传播"></a>通过RoI池层的反向传播</h2><h2 id="SGD超参数"><a href="#SGD超参数" class="headerlink" title="SGD超参数"></a>SGD超参数</h2><p>用于softmax分类和边界框回归的全连接层使用标准偏差为0.01和0.001的零均值高斯分布初始化zero-mean Gaussian distributions。所有层的加权学习率为1，偏差学习率为2，整体学习率为0.001</p>
<h2 id="尺度不变性"><a href="#尺度不变性" class="headerlink" title="尺度不变性"></a>尺度不变性</h2><p>作者探索了两种不同的尺度不变目标检测的实现，一是暴力计算，二是使用图像金字塔</p>
<ol>
<li>暴力计算：在训练和测试中，每幅图像都按照预先定义的像素大小进行处理</li>
<li>图像金字塔：在测试过程中，使用图像金字塔对每个目标建议进行近似尺度归一化，作为数据增强的一种。</li>
</ol>
<img src="\images\image-20220106111519294.png" alt="image-20220105165859553" style="zoom:100%;" />

<p>作者通过实验发现，卷积网络擅长直接学习尺度不变，并且多尺度方法耗费了大量的时间成本只将mAP提升了很小一部分</p>
<h2 id="目标检测过程"><a href="#目标检测过程" class="headerlink" title="目标检测过程"></a>目标检测过程</h2><p>Fast-RCNN网络一旦完成微调，检测过程相当于运行一个前向传递（假设proposal已经提提前算好）。网络将一个图像和一个R对象列表（通常为2k左右）作为输入进行评分。当使用图像金字塔后，每个RoI接近224×224个像素</p>
<p>对于每个RoI r，前向传递输出一个类别的后验概率分布p与一组相对于r 的预测边界框位置偏移。然后使用RCNN算法对每个类单独执行非极大值抑制。</p>
<h2 id="截断SVD"><a href="#截断SVD" class="headerlink" title="截断SVD"></a><strong>截断SVD</strong></h2><p>对于整张图像的分类，全连接层的计算时间要比卷积层处理的时间更少。但是，对于检测来说，由于需要处理的RoI数量很大，几乎一半的前向传递时间都花费在计算全连接层上。因此，作者使用truncated SVD技术来加速全连接层的计算。truncated SVD可以减少30%的探测时间，而只减少0.3%的mAP，并且无需在模型压缩后执行额外的微调。在深层的网络中，对RoI池化层的训练是非常有必要的。</p>
<p>同时，作者提到在深度较浅的网络中，conv1是处理一般性的与任务独立的。因此对conv1训练的意义不大。对卷积层的更新会增加训练时间，并且导致GPU内存溢出的问题，而带来的mAP提升很少。</p>
<img src="\images\image-20220106100757776.png" alt="哎呀，图片不见了" style="zoom:100%;" />

<p>SVD原理可参考<a href="https://www.cnblogs.com/pinard/p/6251584.html">奇异值分解(SVD)原理与在降维中的应用 - 刘建平Pinard - 博客园 (cnblogs.com)</a></p>
<h2 id="更多的训练数据有利于提升mAP"><a href="#更多的训练数据有利于提升mAP" class="headerlink" title="更多的训练数据有利于提升mAP"></a>更多的训练数据有利于提升mAP</h2><p>作者将VOC07加入VOC12后，mAP由66.9%提升到70.0%。</p>
<p>作者构建了有VOC07的训练集与测试集与VOC12的训练集组成的图像数据集，发现mAP也得到了提升。</p>
<h2 id="softmax相比于SVM的优势"><a href="#softmax相比于SVM的优势" class="headerlink" title="softmax相比于SVM的优势"></a><strong>softmax相比于SVM的优势</strong></h2><img src="\images\image-20220106112058344.png" alt="哎呀，图片不见了" style="zoom:100%;" />

<p>实验表明softmax在S、L、M三种网络中性能均优于SVM。尽管性能提升不大，但是“one-shot”微调已经足够了。不同于一对一的SVM，softmax在评估RoI时还引入了类别间的竞争。</p>
<h2 id="proposals数量对结果的影响"><a href="#proposals数量对结果的影响" class="headerlink" title="proposals数量对结果的影响"></a><strong>proposals数量对结果的影响</strong></h2><img src="\images\image-20220205113749198.png" alt="哎呀，图片不见了" style="zoom:100%;" />

<p>作者使用了两种目标检测方法。一是稀疏集proposal（如selective search ）；二是稠密集proposal（如DPM）。</p>
<p>对稀疏peoposal进行分类是一种级联，会先拒绝大量候选区域，只留一小部分给分类器评估。而应用DPM检测时，级联提高了检测精度。</p>
<p>实验表明，随着proposal数量增加，mAP会先上升再下降。所以过多的proposal对分类器并没有帮助。</p>
<p>作者在文章中还介绍了一种测量对象proposal质量的技术：平均召回AR。当每幅图像使用固定数量的proposal时AR与mAP有良好的相关性。但是在图像proposal数量不同时，由于更多proposal而提高的AR，并不意味着mAP会提高。但是在M网络中训练与测试只需要2.5小时，这使得fast-rcnn能够高效、直接评估对象proposal的mAP。</p>
<h1 id="做出的贡献"><a href="#做出的贡献" class="headerlink" title="做出的贡献"></a>做出的贡献</h1><ol>
<li>比RCNN与SPPNet更高的探测质量mAP</li>
<li>训练是单阶段的，使用了multi-task loss</li>
<li>训练时可以对网络中的所有层都进行更新</li>
<li>特征缓存时不需要磁盘空间，Fast-RCNN网络将特征提取器、分类器、回归器合并，使得训练过程不需要再将每阶段结果保存磁盘单独训练，可以一次性完成训练，加快了训练速度</li>
</ol>
]]></content>
      <tags>
        <tag>论文阅读</tag>
        <tag>deep learning</tag>
        <tag>Object Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>Pandas学习（三）</title>
    <url>/2020/05/21/Learning-pandas-03/</url>
    <content><![CDATA[<h1 id="分组"><a href="#分组" class="headerlink" title="分组"></a>分组</h1><h2 id="SAC过程"><a href="#SAC过程" class="headerlink" title="SAC过程"></a>SAC过程</h2><blockquote>
<p>SAC指的是分组操作中的split-apply-combine过程。<em>split指基于某一些规则，将数据拆成若干组，apply是指对每一组独立地使用函数，combine指将每一组的结果组合成某一类数据结构</em></p>
</blockquote>
<span id="more"></span>

<p>以解决如下问题：</p>
<ul>
<li>整合（Aggregation）——即分组计算统计量（如求均值、求每组元素个数）</li>
<li>变换（Transformation）——即分组对每个单元的数据进行操作（如元素标准化）</li>
<li>过滤（Filtration）——即按照某些规则筛选出一些组（如选出组内某一指标小于50的组）</li>
<li>上述三种问题的综合</li>
</ul>
<h2 id="groupby-函数"><a href="#groupby-函数" class="headerlink" title="groupby 函数"></a>groupby 函数</h2><p>**DataFrame.groupby(self, by&#x3D;None, axis&#x3D;0, level&#x3D;None, as_index: bool &#x3D; True, sort: bool &#x3D; True, group_keys: bool &#x3D;<br>True, squeeze: bool &#x3D; False, observed: bool &#x3D; False) → ‘groupby_generic.DataFrameGroupBy’**使用映射器或按一系列列对DataFrame进行分组.</p>
<ul>
<li><p>by: mapping, function, label, or list of labels 用于确定分组依据的分组。如果by是函数，则在对象索引的每个值上调用它。如果by为dict或Series，则将使用Series或dict<br>VALUES来确定组（将Series的值首先对齐；请参见.align()方法）。如果by为ndarray，则按原样使用这些值来确定组。标签或标签列表可以按中的列传递给分组self。注意，元组被解释为（单个）键。</p>
</li>
<li><p>level: int, level name, or sequence of such, default None 如果轴是MultiIndex（分层），则按一个或多个特定级别分组。</p>
</li>
</ul>
<p><em>经过groupby后会生成一个groupby对象，该对象本身不会返回任何东西，只有当相应的方法被调用才会起作用</em></p>
<h2 id="聚合、过滤和变换"><a href="#聚合、过滤和变换" class="headerlink" title="聚合、过滤和变换"></a>聚合、过滤和变换</h2><h3 id="1-聚合（Aggregation）"><a href="#1-聚合（Aggregation）" class="headerlink" title="1. 聚合（Aggregation）"></a>1. 聚合（Aggregation）</h3><h4 id="常用聚合函数"><a href="#常用聚合函数" class="headerlink" title="常用聚合函数"></a>常用聚合函数</h4><p>所谓聚合就是把一堆数，变成一个标量，因此mean&#x2F;sum&#x2F;size&#x2F;count&#x2F;std&#x2F;var&#x2F;sem&#x2F;describe&#x2F;first&#x2F;last&#x2F;nth&#x2F;min&#x2F;max都是聚合函数</p>
<h4 id="同时使用多个聚合函数-AGG-方法"><a href="#同时使用多个聚合函数-AGG-方法" class="headerlink" title="同时使用多个聚合函数 AGG()方法"></a>同时使用多个聚合函数 AGG()方法</h4><p>**DataFrame.agg(self, func, axis&#x3D;0, *args, * *kwargs)**使用指定轴上的一项或多项操作来进行Agg求和</p>
<p>func: function, str, list or dict 功能供AGG regating数据。如果是函数，则必须在传递DataFrame或传递给DataFrame.apply时起作用。</p>
<p>可接受的组合为：</p>
<ul>
<li>功能</li>
<li>字符串函数名称</li>
<li>功能和&#x2F;或功能名称列表，例如 [np.sum, ‘mean’]</li>
<li>轴标签的字典-&gt;函数，函数名称或此类列表。</li>
</ul>
<p>拓展用法:</p>
<ol>
<li><p>利用元组进行重命名 <code>group_m.agg([(&#39;rename_sum&#39;,&#39;sum&#39;),(&#39;rename_mean&#39;,&#39;mean&#39;)])</code></p>
</li>
<li><p>指定哪些函数作用哪些列<code>grouped_mul.agg(&#123;&#39;Math&#39;:[&#39;mean&#39;,&#39;max&#39;],&#39;Height&#39;:&#39;var&#39;&#125;)</code></p>
</li>
<li><p>使用自定义函数<code>grouped_single[&#39;Math&#39;].agg(lambda x:print(x.head(),&#39;间隔&#39;))</code></p>
</li>
<li><p>带参数的聚合函数</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">s,low,high</span>):</span><br><span class="line">    <span class="keyword">return</span> s.between(low,high).<span class="built_in">max</span>()</span><br><span class="line"><span class="comment">#判断是否组内数学分数至少有一个值在50-52之间</span></span><br><span class="line">grouped_single[<span class="string">&#x27;Math&#x27;</span>].agg(f,<span class="number">50</span>,<span class="number">52</span>)</span><br></pre></td></tr></table></figure>

<h3 id="2-过滤（Filteration）"><a href="#2-过滤（Filteration）" class="headerlink" title="2. 过滤（Filteration）"></a>2. 过滤（Filteration）</h3><p>**DataFrameGroupBy.filter(self, func, dropna&#x3D;True, <em>args, * <em>kwargs)</em></em> 返回一个DataFrame的副本，该副本中不包含不满足func指定的布尔条件的组中的元素</p>
<p>filter 函数是用来筛选某些组的（务必记住结果是组的全体），因此传入的值应当是布尔标量</p>
<ul>
<li><p>f function 应用于每个子帧的功能。应该返回True或False。</p>
</li>
<li><p>dropna 删除未通过过滤器的组。默认为真；如果为False，则评估为False的组将填充NaN</p>
</li>
</ul>
<h3 id="3-变换（Transformation）"><a href="#3-变换（Transformation）" class="headerlink" title="3. 变换（Transformation）"></a>3. 变换（Transformation）</h3><p><em><em>DataFrame.transform（self，func，axis &#x3D; 0，</em> args，</em> * kwargs ） →’DataFrame’**</p>
<p>func: function, str, list or dict 用于转换数据的功能。如果是函数，则必须在传递DataFrame或传递给DataFrame.apply时起作用。</p>
<p>可接受的组合为：</p>
<ul>
<li>功能</li>
<li>字符串函数名称</li>
<li>功能和&#x2F;或功能名称列表，例如 [np.exp. ‘sqrt’]</li>
<li>轴标签的字典-&gt;函数，函数名称或此类列表。</li>
</ul>
<h2 id="apply函数"><a href="#apply函数" class="headerlink" title="apply函数"></a>apply函数</h2><p>**GroupBy.apply(self, func, *args, * *kwargs)**使用fun函数并将结果组合在一起.</p>
<p>apply函数的灵活性很大程度来源于其返回值的多样性:</p>
<ol>
<li><p>标量返回值 <code>df.groupby(&#39;School&#39;).apply(lambda x:x.max())</code></p>
</li>
<li><p>列表返回值 <code>df.groupby(&#39;School&#39;).apply(lambda x:x-x.min()).head()</code></p>
</li>
<li><p>数据框返回值</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[[<span class="string">&#x27;School&#x27;</span>,<span class="string">&#x27;Math&#x27;</span>,<span class="string">&#x27;Height&#x27;</span>]].groupby(<span class="string">&#x27;School&#x27;</span>)\</span><br><span class="line">    .apply(<span class="keyword">lambda</span> x:pd.DataFrame(&#123;<span class="string">&#x27;col1&#x27;</span>:x[<span class="string">&#x27;Math&#x27;</span>]-x[<span class="string">&#x27;Math&#x27;</span>].<span class="built_in">max</span>(),</span><br><span class="line">                                  <span class="string">&#x27;col2&#x27;</span>:x[<span class="string">&#x27;Math&#x27;</span>]-x[<span class="string">&#x27;Math&#x27;</span>].<span class="built_in">min</span>(),</span><br><span class="line">                                  <span class="string">&#x27;col3&#x27;</span>:x[<span class="string">&#x27;Height&#x27;</span>]-x[<span class="string">&#x27;Height&#x27;</span>].<span class="built_in">max</span>(),</span><br><span class="line">                                  <span class="string">&#x27;col4&#x27;</span>:x[<span class="string">&#x27;Height&#x27;</span>]-x[<span class="string">&#x27;Height&#x27;</span>].<span class="built_in">min</span>()&#125;)).head()</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>助OrderedDict工具同时统计多个指标</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">df</span>):</span><br><span class="line">    data = OrderedDict()</span><br><span class="line">    data[<span class="string">&#x27;M_sum&#x27;</span>] = df[<span class="string">&#x27;Math&#x27;</span>].<span class="built_in">sum</span>()</span><br><span class="line">    data[<span class="string">&#x27;W_var&#x27;</span>] = df[<span class="string">&#x27;Weight&#x27;</span>].var()</span><br><span class="line">    data[<span class="string">&#x27;H_mean&#x27;</span>] = df[<span class="string">&#x27;Height&#x27;</span>].mean()</span><br><span class="line">    <span class="keyword">return</span> pd.Series(data)</span><br><span class="line">grouped_single.apply(f)</span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>计算机</tag>
      </tags>
  </entry>
  <entry>
    <title>Pandas学习（二）</title>
    <url>/2020/05/13/Learning-pandas-02/</url>
    <content><![CDATA[<h1 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h1><h2 id="单级索引"><a href="#单级索引" class="headerlink" title="单级索引"></a>单级索引</h2><h3 id="loc方法"><a href="#loc方法" class="headerlink" title="loc方法"></a>loc方法</h3><p><strong>DataFrame.loc</strong>通过标签或布尔数组访问一组行和列。</p>
<p>允许的输入为：</p>
<ul>
<li><p>单个标签，例如5或’a’，（请注意，它5被解释为索引的标签，而不是沿索引的整数位置）。</p>
</li>
<li><p>标签列表或数组，例如。[‘a’, ‘b’, ‘c’]</p>
</li>
<li><p>带有标签的切片对象，例如’a’:’f’。<em>警告：需要注意的是违背了普通的Python片，都开始和停止都包括</em></p>
</li>
<li><p>与要切片的轴长度相同的布尔数组，例如。[True, False, True]</p>
</li>
<li><p>一个callable带有一个参数的函数（调用Series或DataFrame），并返回有效的输出以进行索引（上述之​​一）</p>
</li>
</ul>
<blockquote>
<p>Note：当loc使用[] 索引全部index时返回Series，而使用 [[ ]]则返回DataFrame</p>
</blockquote>
<span id="more"></span> 

<h3 id="iloc方法"><a href="#iloc方法" class="headerlink" title="iloc方法"></a>iloc方法</h3><p><strong>DataFrame.iloc</strong>基于位置的纯基于整数位置的索引。</p>
<p>允许的输入为：</p>
<ul>
<li><p>整数，例如5。</p>
</li>
<li><p>整数列表或数组，例如。[4, 3, 0]</p>
</li>
<li><p>具有整数的切片对象，例如1:7。<em>与loc不同切片不包含右端点。</em></p>
</li>
<li><p>布尔数组</p>
</li>
<li><p>一个callable具有一个参数的函数（调用Series或DataFrame），并且返回用于索引的有效输出（上述之一）。当您没有对调用对象的引用，但希望基于某个值进行选择时，这在方法链中很有用。</p>
</li>
</ul>
<h3 id="操作符"><a href="#操作符" class="headerlink" title="[ ]操作符"></a>[ ]操作符</h3><h4 id="Series的-操作"><a href="#Series的-操作" class="headerlink" title="Series的[]操作"></a>Series的[]操作</h4><ul>
<li><p>单元素索引： <code>df[&#39;name&#39;]</code></p>
</li>
<li><p>多行索引： <code>df[0:4]</code></p>
</li>
<li><p>函数式索引： <code>df[lambda x: f(x)]</code> .f(x)应返回一个元素切片。<br><code>df[f(df)]</code> .f(df)应返回一个与轴长相同大小的布尔数组。</p>
</li>
</ul>
<h4 id="DataFrame的-操作"><a href="#DataFrame的-操作" class="headerlink" title="DataFrame的[]操作"></a>DataFrame的[]操作</h4><ul>
<li>单行，多行索引。单列索引要获得某个元素时，先使用get_loc方法,如：</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">row = df.index.get_loc(<span class="string">&quot;XXX&quot;</span>) </span><br><span class="line"><span class="comment">#将此处的&quot;XXX&quot;换为想索引的元素</span></span><br><span class="line">df[row:row+<span class="number">1</span>]</span><br></pre></td></tr></table></figure>

<ul>
<li><p>单列，多列索引</p>
</li>
<li><p>函数式索引</p>
</li>
<li><p>布尔索引</p>
</li>
</ul>
<blockquote>
<p>Note: 一般来说，[]操作符常用于列选择或布尔选择，尽量避免行的选择</p>
</blockquote>
<blockquote>
<p>布尔符号：’&amp;’, ‘|’, ‘~’：分别代表和and，或or，取反not.</p>
</blockquote>
<blockquote>
<p>布尔索引的操作一般在loc和[]中进行。</p>
</blockquote>
<h3 id="isin方法"><a href="#isin方法" class="headerlink" title="isin方法"></a>isin方法</h3><p>**DataFrame.isin(self, values) → ‘DataFrame’**返回一个显示DataFrame中的每个元素是否包含在值中的DataFrame(values为boolean).</p>
<ul>
<li><p>当values是列表时，请检查列表中是否存在DataFrame中的每个值</p>
</li>
<li><p>当values是dict时，我们可以传递值以分别检查每一列：</p>
</li>
<li><p>当values是Series或DataFrame时，索引和列必须匹配。</p>
</li>
</ul>
<h3 id="at和iat方法"><a href="#at和iat方法" class="headerlink" title="at和iat方法"></a>at和iat方法</h3><p>当只需要取一个元素时，at和iat方法能够提供更快的实现.</p>
<h3 id="interval-range方法"><a href="#interval-range方法" class="headerlink" title="interval_range方法"></a>interval_range方法</h3><p>**pandas.interval_range(start&#x3D;None, end&#x3D;None, periods&#x3D;None, freq&#x3D;None, name&#x3D;None, closed&#x3D;’right’)**返回一个fixed<br>frequency的IntervalIndex。</p>
<ul>
<li><p>closed参数可选’left’’right’’both’’neither’，默认左开右闭</p>
</li>
<li><p>periods参数控制区间个数</p>
</li>
<li><p>freq控制步长</p>
</li>
<li><p>name该IntervalIndex的名称</p>
</li>
</ul>
<h3 id="cut方法"><a href="#cut方法" class="headerlink" title="cut方法"></a>cut方法</h3><p><strong>pandas.cut(x, bins, right: bool &#x3D; True, labels&#x3D;None, retbins: bool &#x3D; False, precision: int &#x3D; 3, include_lowest: bool &#x3D;<br>False, duplicates: str &#x3D; ‘raise’)</strong><br>利用cut将数值列转为区间为元素的分类变量，</p>
<ul>
<li><p>x: 输入数组, 必须是一维的</p>
</li>
<li><p>bins:(int，sequence of scalars或IntervalIndex)    int：定义x范围内的等宽bin数。x的范围在每一侧都扩展了0.1％，以包括x的最小值和最大值。 sequence of<br>scalars：定义bin边缘以允许非均匀宽度。x的范围没有扩展。 IntervalIndex：定义要使用的确切容器。请注意，bin的 IntervalIndex 必须不重叠。</p>
</li>
<li><p>right (Boolean): 是否包括右端点</p>
</li>
</ul>
<h2 id="多级索引-MultiIndex"><a href="#多级索引-MultiIndex" class="headerlink" title="多级索引 MultiIndex"></a>多级索引 MultiIndex</h2><h3 id="1-创建多级索引"><a href="#1-创建多级索引" class="headerlink" title="1. 创建多级索引"></a>1. 创建多级索引</h3><ul>
<li>先创建元组(zip方法), Array等, 再调用MultiIndex中from_tuple, from_arrays, from_product.</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">arrays = [[<span class="string">&#x27;A&#x27;</span>,<span class="string">&#x27;a&#x27;</span>],[<span class="string">&#x27;A&#x27;</span>,<span class="string">&#x27;b&#x27;</span>],[<span class="string">&#x27;B&#x27;</span>,<span class="string">&#x27;a&#x27;</span>],[<span class="string">&#x27;B&#x27;</span>,<span class="string">&#x27;b&#x27;</span>]]</span><br><span class="line">mul_index = pd.MultiIndex.from_tuples(arrays, names=(<span class="string">&#x27;Upper&#x27;</span>, <span class="string">&#x27;Lower&#x27;</span>))</span><br><span class="line">pd.DataFrame(&#123;<span class="string">&#x27;Score&#x27;</span>:[<span class="string">&#x27;perfect&#x27;</span>,<span class="string">&#x27;good&#x27;</span>,<span class="string">&#x27;fair&#x27;</span>,<span class="string">&#x27;bad&#x27;</span>]&#125;,index=mul_index)</span><br></pre></td></tr></table></figure>

<ul>
<li>直接调用set_index方法, 指定df中的列创建.</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df_using_mul = df.set_index([<span class="string">&#x27;Class&#x27;</span>,<span class="string">&#x27;Address&#x27;</span>])</span><br></pre></td></tr></table></figure>

<h3 id="2-多层索引切片"><a href="#2-多层索引切片" class="headerlink" title="2. 多层索引切片"></a>2. 多层索引切片</h3><p>切片前要对索引进行排序, 否则报出性能警告或直接报错.</p>
<h3 id="3-多层索引中的slice对象"><a href="#3-多层索引中的slice对象" class="headerlink" title="3. 多层索引中的slice对象"></a>3. 多层索引中的slice对象</h3><p>通过 <code>idx=pd.IndexSlice</code> 获取slice对象, 以更轻松地执行多索引切片.</p>
<h3 id="4-索引层的交换"><a href="#4-索引层的交换" class="headerlink" title="4. 索引层的交换"></a>4. 索引层的交换</h3><ul>
<li><p><strong>DataFrame.swaplevel(self, i&#x3D;-2, j&#x3D;-1, axis&#x3D;0) → ‘DataFrame’</strong> 在特定轴上的MultiIndex中交换级别i和j.</p>
</li>
<li><p><strong>DataFrame.reorder_levels(self, order, axis&#x3D;0) → ‘DataFrame’</strong> 使用输入顺序重新排列索引级别。</p>
</li>
</ul>
<h2 id="索引设定"><a href="#索引设定" class="headerlink" title="索引设定"></a>索引设定</h2><h3 id="1-index-col参数"><a href="#1-index-col参数" class="headerlink" title="1. index_col参数"></a>1. index_col参数</h3><p>index_col是read_csv中的一个参数，而不是某一个方法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pd.read_csv(<span class="string">&#x27;data/table.csv&#x27;</span>,index_col=[<span class="string">&#x27;Address&#x27;</span>,<span class="string">&#x27;School&#x27;</span>]).head()</span><br></pre></td></tr></table></figure>

<h3 id="2-reindex与reindex-like方法"><a href="#2-reindex与reindex-like方法" class="headerlink" title="2. reindex与reindex_like方法"></a>2. reindex与reindex_like方法</h3><p>**DataFrame.reindex(self, labels&#x3D;None, index&#x3D;None, columns&#x3D;None, axis&#x3D;None, method&#x3D;None, copy&#x3D;True, level&#x3D;None,<br>fill_value&#x3D;nan, limit&#x3D;None, tolerance&#x3D;None)**使用可选的填充逻辑使DataFrame符合新索引。</p>
<p><strong>DataFrame.reindex_like(self: ~FrameOrSeries, other, method: Union[str, NoneType] &#x3D; None, copy: bool &#x3D; True,<br>limit&#x3D;None, tolerance&#x3D;None) → ~FrameOrSeries</strong>为生成一个横纵索引完全与参数列表一致的DataFrame，数据使用被调用的表.</p>
<ul>
<li>method:{None, ‘backfill’&#x2F;’bfill’, ‘pad’&#x2F;’ffill’, ‘nearest’}</li>
</ul>
<p>用于在重新索引的 DataFrame中填充孔的方法。请注意：这仅适用于具有单调递增&#x2F;递减索引的DataFrames &#x2F; Series。</p>
<ol>
<li><p>None（默认）：不填补空白</p>
</li>
<li><p>backfill&#x2F;bfill：将上一个有效观察值向前传播到下一个有效值。</p>
</li>
<li><p>pad&#x2F;ffill：使用下一个有效观察值填充空白。</p>
</li>
<li><p>nearest：使用最近的有效观测值来填补空白。</p>
</li>
</ol>
<ul>
<li>fill_value: 标量，默认为np. NaN. 用于填充缺失值的值。默认为NaN，但可以是任何“兼容”值。</li>
</ul>
<h3 id="3-set-index与reset-index方法"><a href="#3-set-index与reset-index方法" class="headerlink" title="3. set_index与reset_index方法"></a>3. set_index与reset_index方法</h3><p>**DataFrame.set_index(self, keys, drop&#x3D;True, append&#x3D;False, inplace&#x3D;False, verify_integrity&#x3D;False)**使用现有列设置DataFrame索引。</p>
<p>**DataFrame.reset_index(self, level: Union[Hashable, Sequence[Hashable], NoneType] &#x3D; None, drop: bool &#x3D; False, inplace:<br>bool &#x3D; False, col_level: Hashable &#x3D; 0, col_fill: Union[Hashable, NoneType] &#x3D; ‘’) →<br>Union[ForwardRef(‘DataFrame’), NoneType]**重置索引或索引的级别。</p>
<ul>
<li><p>inplace: bool, default False 就地修改DataFrame(不创建新的对象)</p>
</li>
<li><p>dropbool, default False 不要尝试将索引插入数据框列。这会将索引重置为默认的整数索引</p>
</li>
<li><p>col_levelint or str, default 0 如果列有多个级别，请确定将标签插入到哪个级别。默认情况下，它被插入第一级。</p>
</li>
<li><p>col_fillobject, default ‘’ 如果列具有多个级别，请确定如何命名其他级别。如果为None，则重复索引名称。</p>
</li>
<li><p>appendbool, default False 是否将列追加到现有索引。</p>
</li>
<li><p>verify_integrity bool，default False 检查新索引是否重复。否则，将检查推迟到必要时进行。设置为False将提高此方法的性能。</p>
</li>
</ul>
<h3 id="4-rename-axis和rename方法"><a href="#4-rename-axis和rename方法" class="headerlink" title="4. rename_axis和rename方法"></a>4. rename_axis和rename方法</h3><p><strong>DataFrame.rename_axis(self, mapper&#x3D;None, index&#x3D;None, columns&#x3D;None, axis&#x3D;None, copy&#x3D;True, inplace&#x3D;False)</strong><br>rename_axis是针对多级索引的方法，作用是修改某一层的索引名，而不是索引标签.</p>
<p>*<em>DataFrame.rename(mapper&#x3D;None, <em>, index&#x3D;None, columns&#x3D;None, axis&#x3D;None, copy&#x3D;True, inplace&#x3D;False, level&#x3D;None,<br>errors&#x3D;’ignore’) -&gt; DataFrame</em></em><br>rename方法用于修改列或者行索引标签，而不是索引名</p>
<ul>
<li><p>index, columnsscalar, list-like, dict-like or function, optional 标量，类似于列表，类似于dict或函数的转换，以应用于该轴的值.<br>使用mapper和axis可以使用mapper或index 和&#x2F;或指定要指定的轴columns。</p>
</li>
<li><p>axis{0 or ‘index’, 1 or ‘columns’}, default 0 重命名的轴</p>
</li>
</ul>
<h2 id="常用索引型函数"><a href="#常用索引型函数" class="headerlink" title="常用索引型函数"></a>常用索引型函数</h2><h3 id="1-where方法"><a href="#1-where方法" class="headerlink" title="1.where方法"></a>1.where方法</h3><p><strong>DataFrame.where(self, cond, other&#x3D;nan, inplace&#x3D;False, axis&#x3D;None, level&#x3D;None, errors&#x3D;’raise’, try_cast&#x3D;False)</strong></p>
<ul>
<li><p>condbool Series&#x2F;DataFrame, array-like, or callable 如果 cond为True，请保留原始值。如果为 False，则用other中的相应值替换。如果cond是可调用的，则它是在Series<br>&#x2F; DataFrame上计算的，并且应返回布尔Series &#x2F; DataFrame或数组。可调用对象不得更改输入Series &#x2F; DataFrame(尽管pandas不会对其进行检查)。</p>
</li>
<li><p>otherscalar, Series&#x2F;DataFrame, or callable 如果other是可调用的，则在Series &#x2F; DataFrame上对其进行计算，并应返回标量或Series &#x2F;<br>DataFrame。可调用对象不得更改输入Series &#x2F; DataFrame</p>
</li>
</ul>
<h3 id="2-mask函数"><a href="#2-mask函数" class="headerlink" title="2. mask函数"></a>2. mask函数</h3><p>mask函数与where功能上相反，其余完全一致，即对条件为True的单元进行填充</p>
<h3 id="3-query函数"><a href="#3-query函数" class="headerlink" title="3. query函数"></a>3. query函数</h3><p>**DataFrame.query(self, expr, inplace&#x3D;False, <strong>kwargs)</strong><br>使用布尔表达式查询 DataFrame的列</p>
<blockquote>
<p>query函数中的布尔表达式中，下面的符号都是合法的：行列索引名、字符串、and&#x2F;not&#x2F;or&#x2F;&amp;&#x2F;|&#x2F;~&#x2F;not in&#x2F;in&#x2F;&#x3D;&#x3D;&#x2F;!&#x3D;、四则运算符</p>
</blockquote>
<h2 id="重复元素处理"><a href="#重复元素处理" class="headerlink" title="重复元素处理"></a>重复元素处理</h2><h3 id="1-duplicated方法"><a href="#1-duplicated方法" class="headerlink" title="1. duplicated方法"></a>1. duplicated方法</h3><p>**DataFrame.duplicated(self, subset: Union[Hashable, Sequence[Hashable], NoneType] &#x3D; None, keep: Union[str, bool] &#x3D; ‘<br>first’) → ‘Series’**返回表示重复行的布尔系列。</p>
<ul>
<li>keep{‘first’, ‘last’, False}, default ‘first’ Determines which duplicates (if any) to mark.</li>
</ul>
<ol>
<li><p>first : 首次出现设为不重复</p>
</li>
<li><p>last : 最后一次设为不重复</p>
</li>
<li><p>False : 则所有重复项为False</p>
</li>
</ol>
<h3 id="2-drop-duplicates方法"><a href="#2-drop-duplicates方法" class="headerlink" title="2. drop_duplicates方法"></a>2. drop_duplicates方法</h3><p><strong>DataFrame.drop_duplicates(self, subset: Union[Hashable, Sequence[Hashable], NoneType] &#x3D; None, keep: Union[str, bool]<br>&#x3D; ‘first’, inplace: bool &#x3D; False, ignore_index: bool &#x3D; False) → Union[ForwardRef(‘DataFrame’), NoneType]</strong><br>返回已删除重复行的DataFrame。</p>
<ul>
<li>ignore_index bool，默认为False 如果为True，则结果轴将标记为0、1，…，n-1。</li>
</ul>
<h2 id="抽样函数-sample方法"><a href="#抽样函数-sample方法" class="headerlink" title="抽样函数 sample方法"></a>抽样函数 sample方法</h2><p><strong>DataFrame.sample(self: ~FrameOrSeries, n&#x3D;None, frac&#x3D;None, replace&#x3D;False, weights&#x3D;None, random_state&#x3D;None, axis&#x3D;None)<br>→ ~FrameOrSeries</strong><br>从对象轴返回随机的项目样本</p>
<ul>
<li><p>n int，optional 从轴返回的项目数。不能与frac一起使用。如果frac &#x3D; None，则默认&#x3D; 1</p>
</li>
<li><p>frac float, optional 要返回的抽样比。不能与n一起使用</p>
</li>
<li><p>replace bool, default False 允许或不允许对同一行进行多次采样。<em>frac大于1时, replace需为True.</em></p>
</li>
<li><p>weightsstr or ndarray-like, optional 样本权重，自动归一化</p>
</li>
</ul>
]]></content>
      <tags>
        <tag>计算机</tag>
      </tags>
  </entry>
  <entry>
    <title>Pandas学习（四）</title>
    <url>/2020/07/17/Learning-pandas-04/</url>
    <content><![CDATA[<h1 id="变形"><a href="#变形" class="headerlink" title="变形"></a>变形</h1><span id="more"></span>

<h2 id="透视表"><a href="#透视表" class="headerlink" title="透视表"></a>透视表</h2><h3 id="1-pivot"><a href="#1-pivot" class="headerlink" title="1.pivot"></a>1.pivot</h3><p>一般状态下，数据在DataFrame会以压缩（stacked）状态存放.</p>
<p>pivot函数可将某一列作为新的cols.</p>
<ul>
<li><p>1.index：用于制作新框架索引的列。如果为None，则使用现有索引。</p>
</li>
<li><p>2.columns：用于制作新框架列的列。</p>
</li>
<li><p>3.values：用于填充新框架值的列。如果未指定，将使用所有剩余的列，并且结果将具有按层次结构索引的列。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df = pd.read_csv(<span class="string">&#x27;data/table.csv&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df.head()</span><br><span class="line">  School Class    ID Gender   Address  Height  Weight  Math Physics</span><br><span class="line"><span class="number">0</span>    S_1   C_1  <span class="number">1101</span>      M  street_1     <span class="number">173</span>      <span class="number">63</span>  <span class="number">34.0</span>      A+        </span><br><span class="line"><span class="number">1</span>    S_1   C_1  <span class="number">1102</span>      F  street_2     <span class="number">192</span>      <span class="number">73</span>  <span class="number">32.5</span>      B+        </span><br><span class="line"><span class="number">2</span>    S_1   C_1  <span class="number">1103</span>      M  street_2     <span class="number">186</span>      <span class="number">82</span>  <span class="number">87.2</span>      B+        </span><br><span class="line"><span class="number">3</span>    S_1   C_1  <span class="number">1104</span>      F  street_2     <span class="number">167</span>      <span class="number">81</span>  <span class="number">80.4</span>      B-        </span><br><span class="line"><span class="number">4</span>    S_1   C_1  <span class="number">1105</span>      F  street_4     <span class="number">159</span>      <span class="number">64</span>  <span class="number">84.8</span>      B+        </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df.pivot(index=<span class="string">&#x27;ID&#x27;</span>,columns=<span class="string">&#x27;Gender&#x27;</span>,values=<span class="string">&#x27;Height&#x27;</span>).head()</span><br><span class="line">Gender      F      M</span><br><span class="line">ID</span><br><span class="line"><span class="number">1101</span>      NaN  <span class="number">173.0</span></span><br><span class="line"><span class="number">1102</span>    <span class="number">192.0</span>    NaN</span><br><span class="line"><span class="number">1103</span>      NaN  <span class="number">186.0</span></span><br><span class="line"><span class="number">1104</span>    <span class="number">167.0</span>    NaN</span><br><span class="line"><span class="number">1105</span>    <span class="number">159.0</span>    NaN</span><br></pre></td></tr></table></figure>

<h3 id="2-pivot-table"><a href="#2-pivot-table" class="headerlink" title="2.pivot_table"></a>2.pivot_table</h3><p><em>pivot_table比pivot功能更强大，但是也更耗时。</em></p>
<ul>
<li>1.aggfunc：对组内进行聚合统计，可传入各类函数，默认为’mean’</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>pd.pivot_table(df,index=<span class="string">&#x27;School&#x27;</span>,columns=<span class="string">&#x27;Gender&#x27;</span>,values=<span class="string">&#x27;Height&#x27;</span>,aggfunc=[<span class="string">&#x27;mean&#x27;</span>,<span class="string">&#x27;sum&#x27;</span>]).head()</span><br><span class="line">              mean               <span class="built_in">sum</span>      </span><br><span class="line">Gender           F           M     F     M</span><br><span class="line">School</span><br><span class="line">S_1     <span class="number">173.125000</span>  <span class="number">178.714286</span>  <span class="number">1385</span>  <span class="number">1251</span></span><br><span class="line">S_2     <span class="number">173.727273</span>  <span class="number">172.000000</span>  <span class="number">1911</span>  <span class="number">1548</span></span><br></pre></td></tr></table></figure>

<ul>
<li>2.margins：汇总边际状态</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>pd.pivot_table(df,index=<span class="string">&#x27;School&#x27;</span>,columns=<span class="string">&#x27;Gender&#x27;</span>,values=<span class="string">&#x27;Height&#x27;</span>,aggfunc=[<span class="string">&#x27;mean&#x27;</span>,<span class="string">&#x27;sum&#x27;</span>],margins=<span class="literal">True</span>,margins_name=<span class="string">&#x27;all&#x27;</span>).head()</span><br><span class="line"><span class="comment"># margins_name可以设置名字，默认为&#x27;All&#x27;</span></span><br><span class="line">              mean                           <span class="built_in">sum</span></span><br><span class="line">Gender           F           M         <span class="built_in">all</span>     F     M   <span class="built_in">all</span></span><br><span class="line">School</span><br><span class="line">S_1     <span class="number">173.125000</span>  <span class="number">178.714286</span>  <span class="number">175.733333</span>  <span class="number">1385</span>  <span class="number">1251</span>  <span class="number">2636</span></span><br><span class="line">S_2     <span class="number">173.727273</span>  <span class="number">172.000000</span>  <span class="number">172.950000</span>  <span class="number">1911</span>  <span class="number">1548</span>  <span class="number">3459</span></span><br><span class="line"><span class="built_in">all</span>     <span class="number">173.473684</span>  <span class="number">174.937500</span>  <span class="number">174.142857</span>  <span class="number">3296</span>  <span class="number">2799</span>  <span class="number">6095</span></span><br></pre></td></tr></table></figure>

<ul>
<li>3.行、列、值都可以为多级</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>pd.pivot_table(df,index=[<span class="string">&#x27;School&#x27;</span>,<span class="string">&#x27;Class&#x27;</span>],</span><br><span class="line"><span class="meta">... </span>               columns=[<span class="string">&#x27;Gender&#x27;</span>,<span class="string">&#x27;Address&#x27;</span>],</span><br><span class="line"><span class="meta">... </span>               values=[<span class="string">&#x27;Height&#x27;</span>,<span class="string">&#x27;Weight&#x27;</span>])</span><br><span class="line">               Height                    ...   Weight</span><br><span class="line">Gender              F                    ...        M</span><br><span class="line">Address      street_1 street_2 street_4  ... street_5 street_6 street_7    </span><br><span class="line">School Class                             ...</span><br><span class="line">S_1    C_1        NaN    <span class="number">179.5</span>    <span class="number">159.0</span>  ...      NaN      NaN      NaN    </span><br><span class="line">       C_2        NaN      NaN    <span class="number">176.0</span>  ...     <span class="number">68.0</span>     <span class="number">53.0</span>      NaN    </span><br><span class="line">       C_3      <span class="number">175.0</span>      NaN      NaN  ...      NaN      NaN     <span class="number">82.0</span>    </span><br><span class="line">S_2    C_1        NaN      NaN      NaN  ...      NaN      NaN     <span class="number">84.0</span>    </span><br><span class="line">       C_2        NaN      NaN      NaN  ...    <span class="number">100.0</span>      NaN      NaN    </span><br><span class="line">       C_3        NaN      NaN    <span class="number">157.0</span>  ...     <span class="number">88.0</span>      NaN      NaN    </span><br><span class="line">       C_4        NaN    <span class="number">176.0</span>      NaN  ...      NaN      NaN     <span class="number">82.0</span>    </span><br><span class="line"></span><br><span class="line">[<span class="number">7</span> rows x <span class="number">24</span> columns]</span><br></pre></td></tr></table></figure>

<h3 id="3-crosstab"><a href="#3-crosstab" class="headerlink" title="3.crosstab"></a>3.crosstab</h3><p>交叉表是一种特殊的透视表，典型的用途如分组统计</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 统计关于街道和性别分组的频数</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pd.crosstab(index=df[<span class="string">&#x27;Address&#x27;</span>],columns=df[<span class="string">&#x27;Gender&#x27;</span>])</span><br><span class="line">Gender    F  M</span><br><span class="line">Address</span><br><span class="line">street_1  <span class="number">1</span>  <span class="number">2</span></span><br><span class="line">street_2  <span class="number">4</span>  <span class="number">2</span></span><br><span class="line">street_4  <span class="number">3</span>  <span class="number">5</span></span><br><span class="line">street_5  <span class="number">3</span>  <span class="number">3</span></span><br><span class="line">street_6  <span class="number">5</span>  <span class="number">1</span></span><br><span class="line">street_7  <span class="number">3</span>  <span class="number">3</span></span><br></pre></td></tr></table></figure>

<ul>
<li>1.values、aggfunc：分组对某些数据进行聚合操作，这两个参数必须成对出现</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>pd.crosstab(index=df[<span class="string">&#x27;Address&#x27;</span>],columns=df[<span class="string">&#x27;Gender&#x27;</span>],</span><br><span class="line">            values=np.random.randint(<span class="number">1</span>,<span class="number">20</span>,df.shape[<span class="number">0</span>]),aggfunc=<span class="string">&#x27;min&#x27;</span>)</span><br><span class="line"><span class="comment"># 默认参数等于如下方法：</span></span><br><span class="line"><span class="comment"># pd.crosstab(index=df[&#x27;Address&#x27;],columns=df[&#x27;Gender&#x27;],values=1,aggfunc=&#x27;count&#x27;)</span></span><br><span class="line"></span><br><span class="line">Gender     F   M</span><br><span class="line">Address</span><br><span class="line">street_1  <span class="number">16</span>   <span class="number">3</span></span><br><span class="line">street_2   <span class="number">5</span>   <span class="number">2</span></span><br><span class="line">street_4   <span class="number">5</span>   <span class="number">1</span></span><br><span class="line">street_5   <span class="number">6</span>   <span class="number">2</span></span><br><span class="line">street_6   <span class="number">2</span>  <span class="number">15</span></span><br><span class="line">street_7   <span class="number">1</span>   <span class="number">7</span></span><br></pre></td></tr></table></figure>

<ul>
<li>2.② 除了边际参数margins外，还引入了normalize参数，可选’all’,’index’,’columns’参数值</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>pd.crosstab(index=df[<span class="string">&#x27;Address&#x27;</span>],columns=df[<span class="string">&#x27;Gender&#x27;</span>],normalize=<span class="string">&#x27;all&#x27;</span>,margins=<span class="literal">True</span>)</span><br><span class="line">Gender           F         M       All</span><br><span class="line">Address</span><br><span class="line">street_1  <span class="number">0.028571</span>  <span class="number">0.057143</span>  <span class="number">0.085714</span></span><br><span class="line">street_2  <span class="number">0.114286</span>  <span class="number">0.057143</span>  <span class="number">0.171429</span></span><br><span class="line">street_4  <span class="number">0.085714</span>  <span class="number">0.142857</span>  <span class="number">0.228571</span></span><br><span class="line">street_5  <span class="number">0.085714</span>  <span class="number">0.085714</span>  <span class="number">0.171429</span></span><br><span class="line">street_6  <span class="number">0.142857</span>  <span class="number">0.028571</span>  <span class="number">0.171429</span></span><br><span class="line">street_7  <span class="number">0.085714</span>  <span class="number">0.085714</span>  <span class="number">0.171429</span></span><br><span class="line">All       <span class="number">0.542857</span>  <span class="number">0.457143</span>  <span class="number">1.000000</span></span><br></pre></td></tr></table></figure>

<h2 id="其他变形方法"><a href="#其他变形方法" class="headerlink" title="其他变形方法"></a>其他变形方法</h2><h3 id="1-melt"><a href="#1-melt" class="headerlink" title="1.melt"></a>1.melt</h3><p>melt函数可以认为是pivot函数的逆操作，将unstacked状态的数据，压缩成stacked，使“宽”的DataFrame变“窄”</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>df_m = df[[<span class="string">&#x27;ID&#x27;</span>,<span class="string">&#x27;Gender&#x27;</span>,<span class="string">&#x27;Math&#x27;</span>]]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df_m.head()</span><br><span class="line">     ID Gender  Math</span><br><span class="line"><span class="number">0</span>  <span class="number">1101</span>      M  <span class="number">34.0</span></span><br><span class="line"><span class="number">1</span>  <span class="number">1102</span>      F  <span class="number">32.5</span></span><br><span class="line"><span class="number">2</span>  <span class="number">1103</span>      M  <span class="number">87.2</span></span><br><span class="line"><span class="number">3</span>  <span class="number">1104</span>      F  <span class="number">80.4</span></span><br><span class="line"><span class="number">4</span>  <span class="number">1105</span>      F  <span class="number">84.8</span></span><br></pre></td></tr></table></figure>

<p>melt函数中的id_vars表示需要保留的列，value_vars表示需要stack的一组列</p>
<h3 id="2-压缩与展开"><a href="#2-压缩与展开" class="headerlink" title="2. 压缩与展开"></a>2. 压缩与展开</h3><ul>
<li><p>stack：这是最基础的变形函数，总共只有两个参数：level和dropna</p>
</li>
<li><p>unstack：stack的逆函数，功能上类似于pivot_table</p>
</li>
</ul>
<h2 id="哑变量与因子化"><a href="#哑变量与因子化" class="headerlink" title="哑变量与因子化"></a>哑变量与因子化</h2><h3 id="1-Dummy-Variable（哑变量）"><a href="#1-Dummy-Variable（哑变量）" class="headerlink" title="1.Dummy Variable（哑变量）"></a>1.Dummy Variable（哑变量）</h3><p>主要介绍get_dummies函数，其功能主要是进行one-hot编码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>df_d = df[[<span class="string">&#x27;Class&#x27;</span>,<span class="string">&#x27;Gender&#x27;</span>,<span class="string">&#x27;Weight&#x27;</span>]]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df_d.head()</span><br><span class="line">  Class Gender  Weight</span><br><span class="line"><span class="number">0</span>   C_1      M      <span class="number">63</span></span><br><span class="line"><span class="number">1</span>   C_1      F      <span class="number">73</span></span><br><span class="line"><span class="number">2</span>   C_1      M      <span class="number">82</span></span><br><span class="line"><span class="number">3</span>   C_1      F      <span class="number">81</span></span><br><span class="line"><span class="number">4</span>   C_1      F      <span class="number">64</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pd.get_dummies(df_d[[<span class="string">&#x27;Class&#x27;</span>,<span class="string">&#x27;Gender&#x27;</span>]]).join(df_d[<span class="string">&#x27;Weight&#x27;</span>]).head()</span><br><span class="line">   Class_C_1  Class_C_2  Class_C_3  Class_C_4  Gender_F  Gender_M  Weight</span><br><span class="line"><span class="number">0</span>          <span class="number">1</span>          <span class="number">0</span>          <span class="number">0</span>          <span class="number">0</span>         <span class="number">0</span>         <span class="number">1</span>      <span class="number">63</span>  </span><br><span class="line"><span class="number">1</span>          <span class="number">1</span>          <span class="number">0</span>          <span class="number">0</span>          <span class="number">0</span>         <span class="number">1</span>         <span class="number">0</span>      <span class="number">73</span>  </span><br><span class="line"><span class="number">2</span>          <span class="number">1</span>          <span class="number">0</span>          <span class="number">0</span>          <span class="number">0</span>         <span class="number">0</span>         <span class="number">1</span>      <span class="number">82</span>  </span><br><span class="line"><span class="number">3</span>          <span class="number">1</span>          <span class="number">0</span>          <span class="number">0</span>          <span class="number">0</span>         <span class="number">1</span>         <span class="number">0</span>      <span class="number">81</span>  </span><br><span class="line"><span class="number">4</span>          <span class="number">1</span>          <span class="number">0</span>          <span class="number">0</span>          <span class="number">0</span>         <span class="number">1</span>         <span class="number">0</span>      <span class="number">64</span>  </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment">#可选prefix参数添加前缀，prefix_sep添加分隔符</span></span><br></pre></td></tr></table></figure>

<h3 id="2-factorize方法"><a href="#2-factorize方法" class="headerlink" title="2.factorize方法"></a>2.factorize方法</h3><p>该方法主要用于自然数编码，并且缺失值会被记做-1，其中sort参数表示是否排序后赋值</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>codes, uniques = pd.factorize([<span class="string">&#x27;b&#x27;</span>, <span class="literal">None</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;b&#x27;</span>], sort=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>codes</span><br><span class="line">array([ <span class="number">1</span>, -<span class="number">1</span>,  <span class="number">0</span>,  <span class="number">2</span>,  <span class="number">1</span>], dtype=int64)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>uniques</span><br><span class="line">array([<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>], dtype=<span class="built_in">object</span>)</span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>计算机</tag>
      </tags>
  </entry>
  <entry>
    <title>Pandas学习（四）</title>
    <url>/2020/07/17/Learning-pandas-05/</url>
    <content><![CDATA[<h1 id="合并"><a href="#合并" class="headerlink" title="合并"></a>合并</h1><span id="more"></span>

<h2 id="append"><a href="#append" class="headerlink" title="append"></a>append</h2><h3 id="1-利用序列添加行（必须指定name）"><a href="#1-利用序列添加行（必须指定name）" class="headerlink" title="1.利用序列添加行（必须指定name）"></a>1.利用序列添加行（必须指定name）</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>df_append = df.loc[:<span class="number">3</span>,[<span class="string">&#x27;Gender&#x27;</span>,<span class="string">&#x27;Height&#x27;</span>]].copy()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df_append</span><br><span class="line">  Gender  Height</span><br><span class="line"><span class="number">0</span>      M     <span class="number">173</span></span><br><span class="line"><span class="number">1</span>      F     <span class="number">192</span></span><br><span class="line"><span class="number">2</span>      M     <span class="number">186</span></span><br><span class="line"><span class="number">3</span>      F     <span class="number">167</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s = pd.Series(&#123;<span class="string">&#x27;Gender&#x27;</span>:<span class="string">&#x27;F&#x27;</span>,<span class="string">&#x27;Height&#x27;</span>:<span class="number">188</span>&#125;,name=<span class="string">&#x27;new_row&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df_append.append(s)</span><br><span class="line">        Gender  Height</span><br><span class="line"><span class="number">0</span>            M     <span class="number">173</span></span><br><span class="line"><span class="number">1</span>            F     <span class="number">192</span></span><br><span class="line"><span class="number">2</span>            M     <span class="number">186</span></span><br><span class="line"><span class="number">3</span>            F     <span class="number">167</span></span><br><span class="line">new_row      F     <span class="number">188</span></span><br></pre></td></tr></table></figure>

<h3 id="2-用DataFrame添加表"><a href="#2-用DataFrame添加表" class="headerlink" title="2.用DataFrame添加表"></a>2.用DataFrame添加表</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>df_temp = pd.DataFrame(&#123;<span class="string">&#x27;Gender&#x27;</span>:[<span class="string">&#x27;F&#x27;</span>,<span class="string">&#x27;M&#x27;</span>],<span class="string">&#x27;Height&#x27;</span>:[<span class="number">188</span>,<span class="number">176</span>]&#125;,index=[<span class="string">&#x27;new_1&#x27;</span>,<span class="string">&#x27;new_2&#x27;</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df_append.append(df_temp)</span><br><span class="line">      Gender  Height</span><br><span class="line"><span class="number">0</span>          M     <span class="number">173</span></span><br><span class="line"><span class="number">1</span>          F     <span class="number">192</span></span><br><span class="line"><span class="number">2</span>          M     <span class="number">186</span></span><br><span class="line"><span class="number">3</span>          F     <span class="number">167</span></span><br><span class="line">new_1      F     <span class="number">188</span></span><br><span class="line">new_2      M     <span class="number">176</span></span><br></pre></td></tr></table></figure>

<h2 id="assign"><a href="#assign" class="headerlink" title="assign"></a>assign</h2><p>该方法主要用于添加列，列名直接由参数指定</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>s = pd.Series(<span class="built_in">list</span>(<span class="string">&#x27;abcd&#x27;</span>),index=<span class="built_in">range</span>(<span class="number">4</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df_append.assign(Letter=s)</span><br><span class="line">  Gender  Height Letter</span><br><span class="line"><span class="number">0</span>      M     <span class="number">173</span>      a</span><br><span class="line"><span class="number">1</span>      F     <span class="number">192</span>      b</span><br><span class="line"><span class="number">2</span>      M     <span class="number">186</span>      c</span><br><span class="line"><span class="number">3</span>      F     <span class="number">167</span>      d</span><br></pre></td></tr></table></figure>

<p>可以一次添加多个列</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>df_append.assign(col1=<span class="keyword">lambda</span> x:x[<span class="string">&#x27;Gender&#x27;</span>]*<span class="number">2</span>,</span><br><span class="line"><span class="meta">... </span>                 col2=s)</span><br><span class="line">  Gender  Height col1 col2</span><br><span class="line"><span class="number">0</span>      M     <span class="number">173</span>   MM    a</span><br><span class="line"><span class="number">1</span>      F     <span class="number">192</span>   FF    b</span><br><span class="line"><span class="number">2</span>      M     <span class="number">186</span>   MM    c</span><br><span class="line"><span class="number">3</span>      F     <span class="number">167</span>   FF    d</span><br></pre></td></tr></table></figure>

<h2 id="combine"><a href="#combine" class="headerlink" title="combine"></a>combine</h2><h3 id="1-填充对象"><a href="#1-填充对象" class="headerlink" title="1.填充对象"></a>1.填充对象</h3><p>combine方法是按照表的顺序轮流进行逐列循环的，而且自动索引对齐，缺失值为NaN</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>df_combine_1.combine(df_combine_2,<span class="keyword">lambda</span> x,y:<span class="built_in">print</span>(x,y))           </span><br><span class="line"><span class="number">0</span>       M</span><br><span class="line"><span class="number">1</span>       F</span><br><span class="line"><span class="number">10</span>    NaN</span><br><span class="line"><span class="number">11</span>    NaN</span><br><span class="line">Name: Gender, dtype: <span class="built_in">object</span> <span class="number">0</span>     NaN</span><br><span class="line"><span class="number">1</span>     NaN</span><br><span class="line"><span class="number">10</span>      M</span><br><span class="line"><span class="number">11</span>      F</span><br><span class="line">Name: Gender, dtype: <span class="built_in">object</span></span><br><span class="line"><span class="number">0</span>     <span class="number">173.0</span></span><br><span class="line"><span class="number">1</span>     <span class="number">192.0</span></span><br><span class="line"><span class="number">10</span>      NaN</span><br><span class="line"><span class="number">11</span>      NaN</span><br><span class="line">Name: Height, dtype: float64 <span class="number">0</span>       NaN</span><br><span class="line"><span class="number">1</span>       NaN</span><br><span class="line"><span class="number">10</span>    <span class="number">161.0</span></span><br><span class="line"><span class="number">11</span>    <span class="number">175.0</span></span><br><span class="line">Name: Height, dtype: float64</span><br><span class="line">   Gender Height</span><br><span class="line"><span class="number">0</span>     NaN    NaN</span><br><span class="line"><span class="number">1</span>     NaN    NaN</span><br><span class="line"><span class="number">10</span>    NaN    NaN</span><br><span class="line"><span class="number">11</span>    NaN    NaN</span><br></pre></td></tr></table></figure>

<h3 id="2-combine-first"><a href="#2-combine-first" class="headerlink" title="2.combine_first"></a>2.combine_first</h3><p>用df2填补df1的缺失值，功能比较简单，但很多时候会比combine更常用</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>df1 = pd.DataFrame(&#123;<span class="string">&#x27;A&#x27;</span>: [<span class="literal">None</span>, <span class="number">0</span>], <span class="string">&#x27;B&#x27;</span>: [<span class="literal">None</span>, <span class="number">4</span>]&#125;)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df2 = pd.DataFrame(&#123;<span class="string">&#x27;A&#x27;</span>: [<span class="number">1</span>, <span class="number">1</span>], <span class="string">&#x27;B&#x27;</span>: [<span class="number">3</span>, <span class="number">3</span>]&#125;)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df1.combine_first(df2)</span><br><span class="line">     A    B</span><br><span class="line"><span class="number">0</span>  <span class="number">1.0</span>  <span class="number">3.0</span></span><br><span class="line"><span class="number">1</span>  <span class="number">0.0</span>  <span class="number">4.0</span></span><br></pre></td></tr></table></figure>

<h2 id="update"><a href="#update" class="headerlink" title="update"></a>update</h2><p>三个特点:</p>
<ul>
<li>①返回的框索引只会与被调用框的一致（默认使用左连接，下一节会介绍）</li>
<li>②第二个框中的nan元素不会起作用</li>
<li>③没有返回值，直接在df上操作</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>df1 = pd.DataFrame(&#123;<span class="string">&#x27;A&#x27;</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line"><span class="meta">... </span>                    <span class="string">&#x27;B&#x27;</span>: [<span class="number">400</span>, <span class="number">500</span>, <span class="number">600</span>]&#125;)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df2 = pd.DataFrame(&#123;<span class="string">&#x27;B&#x27;</span>: [<span class="number">4</span>, np.nan, <span class="number">6</span>]&#125;)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df1.update(df2)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df1</span><br><span class="line">   A      B</span><br><span class="line"><span class="number">0</span>  <span class="number">1</span>    <span class="number">4.0</span></span><br><span class="line"><span class="number">1</span>  <span class="number">2</span>  <span class="number">500.0</span></span><br><span class="line"><span class="number">2</span>  <span class="number">3</span>    <span class="number">6.0</span></span><br></pre></td></tr></table></figure>

<h2 id="concat"><a href="#concat" class="headerlink" title="concat"></a>concat</h2><p>concat方法可以在两个维度上拼接，默认纵向凭借（axis&#x3D;0），拼接方式默认外连接</p>
<p><em>所谓外连接，就是取拼接方向的并集，而’inner’时取拼接方向（若使用默认的纵向拼接，则为列的交集）的交集</em></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>df1 = pd.DataFrame(&#123;<span class="string">&#x27;A&#x27;</span>: [<span class="string">&#x27;A0&#x27;</span>, <span class="string">&#x27;A1&#x27;</span>],</span><br><span class="line"><span class="meta">... </span>                    <span class="string">&#x27;B&#x27;</span>: [<span class="string">&#x27;B0&#x27;</span>, <span class="string">&#x27;B1&#x27;</span>]&#125;,</span><br><span class="line"><span class="meta">... </span>                    index = [<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df2 = pd.DataFrame(&#123;<span class="string">&#x27;A&#x27;</span>: [<span class="string">&#x27;A2&#x27;</span>, <span class="string">&#x27;A3&#x27;</span>],</span><br><span class="line"><span class="meta">... </span>                    <span class="string">&#x27;B&#x27;</span>: [<span class="string">&#x27;B2&#x27;</span>, <span class="string">&#x27;B3&#x27;</span>]&#125;,</span><br><span class="line"><span class="meta">... </span>                    index = [<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df3 = pd.DataFrame(&#123;<span class="string">&#x27;A&#x27;</span>: [<span class="string">&#x27;A1&#x27;</span>, <span class="string">&#x27;A3&#x27;</span>],</span><br><span class="line"><span class="meta">... </span>                    <span class="string">&#x27;D&#x27;</span>: [<span class="string">&#x27;D1&#x27;</span>, <span class="string">&#x27;D3&#x27;</span>],</span><br><span class="line"><span class="meta">... </span>                    <span class="string">&#x27;E&#x27;</span>: [<span class="string">&#x27;E1&#x27;</span>, <span class="string">&#x27;E3&#x27;</span>]&#125;,</span><br><span class="line"><span class="meta">... </span>                    index = [<span class="number">1</span>,<span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pd.concat([df1,df2])</span><br><span class="line">    A   B</span><br><span class="line"><span class="number">0</span>  A0  B0</span><br><span class="line"><span class="number">1</span>  A1  B1</span><br><span class="line"><span class="number">2</span>  A2  B2</span><br><span class="line"><span class="number">3</span>  A3  B3</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pd.concat([df1,df2],axis=<span class="number">1</span>)</span><br><span class="line">     A    B    A    B</span><br><span class="line"><span class="number">0</span>   A0   B0  NaN  NaN</span><br><span class="line"><span class="number">1</span>   A1   B1  NaN  NaN</span><br><span class="line"><span class="number">2</span>  NaN  NaN   A2   B2</span><br><span class="line"><span class="number">3</span>  NaN  NaN   A3   B3</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pd.concat([df3,df1],join=<span class="string">&#x27;inner&#x27;</span>)</span><br><span class="line">    A</span><br><span class="line"><span class="number">1</span>  A1</span><br><span class="line"><span class="number">3</span>  A3</span><br><span class="line"><span class="number">0</span>  A0</span><br><span class="line"><span class="number">1</span>  A1</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pd.concat([df3,df1],join=<span class="string">&#x27;outer&#x27;</span>,sort=<span class="literal">True</span>) <span class="comment">#sort设置列排序，默认为False</span></span><br><span class="line">    A    B    D    E</span><br><span class="line"><span class="number">1</span>  A1  NaN   D1   E1</span><br><span class="line"><span class="number">3</span>  A3  NaN   D3   E3</span><br><span class="line"><span class="number">0</span>  A0   B0  NaN  NaN</span><br><span class="line"><span class="number">1</span>  A1   B1  NaN  NaN</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pd.concat([df3,df1],join=<span class="string">&#x27;outer&#x27;</span>)                                      </span><br><span class="line">    A    D    E    B</span><br><span class="line"><span class="number">1</span>  A1   D1   E1  NaN</span><br><span class="line"><span class="number">3</span>  A3   D3   E3  NaN</span><br><span class="line"><span class="number">0</span>  A0  NaN  NaN   B0</span><br><span class="line"><span class="number">1</span>  A1  NaN  NaN   B1</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s = pd.Series([<span class="string">&#x27;X0&#x27;</span>, <span class="string">&#x27;X1&#x27;</span>], name=<span class="string">&#x27;X&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pd.concat([df1,s],axis=<span class="number">1</span>)</span><br><span class="line">    A   B   X</span><br><span class="line"><span class="number">0</span>  A0  B0  X0</span><br><span class="line"><span class="number">1</span>  A1  B1  X1</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pd.concat([df1,s],axis=<span class="number">0</span>) </span><br><span class="line">     A    B    <span class="number">0</span></span><br><span class="line"><span class="number">0</span>   A0   B0  NaN</span><br><span class="line"><span class="number">1</span>   A1   B1  NaN</span><br><span class="line"><span class="number">0</span>  NaN  NaN   X0</span><br><span class="line"><span class="number">1</span>  NaN  NaN   X1</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pd.concat([df1,s],axis=<span class="number">0</span>,join=<span class="string">&#x27;outer&#x27;</span>) </span><br><span class="line">     A    B    <span class="number">0</span></span><br><span class="line"><span class="number">0</span>   A0   B0  NaN</span><br><span class="line"><span class="number">1</span>   A1   B1  NaN</span><br><span class="line"><span class="number">0</span>  NaN  NaN   X0</span><br><span class="line"><span class="number">1</span>  NaN  NaN   X1</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pd.concat([df1,s],axis=<span class="number">0</span>,join=<span class="string">&#x27;inner&#x27;</span>) </span><br><span class="line">Empty DataFrame</span><br><span class="line">Columns: []</span><br><span class="line">Index: [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br></pre></td></tr></table></figure>

<h2 id="merge"><a href="#merge" class="headerlink" title="merge"></a>merge</h2><p>merge函数的作用是将两个pandas对象<strong>横向</strong>合并，遇到重复的索引项时会使用笛卡尔积，默认inner连接，可选left、outer、right连接</p>
<p><em>所谓左连接，就是指以第一个表索引为基准，右边的表中如果不再左边的则不加入，如果在左边的就以笛卡尔积的方式加入</em></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>left = pd.DataFrame(&#123;<span class="string">&#x27;key1&#x27;</span>: [<span class="string">&#x27;K0&#x27;</span>, <span class="string">&#x27;K0&#x27;</span>, <span class="string">&#x27;K1&#x27;</span>, <span class="string">&#x27;K2&#x27;</span>],</span><br><span class="line"><span class="meta">... </span>                     <span class="string">&#x27;key2&#x27;</span>: [<span class="string">&#x27;K0&#x27;</span>, <span class="string">&#x27;K1&#x27;</span>, <span class="string">&#x27;K0&#x27;</span>, <span class="string">&#x27;K1&#x27;</span>],</span><br><span class="line"><span class="meta">... </span>                      <span class="string">&#x27;A&#x27;</span>: [<span class="string">&#x27;A0&#x27;</span>, <span class="string">&#x27;A1&#x27;</span>, <span class="string">&#x27;A2&#x27;</span>, <span class="string">&#x27;A3&#x27;</span>],</span><br><span class="line"><span class="meta">... </span>                      <span class="string">&#x27;B&#x27;</span>: [<span class="string">&#x27;B0&#x27;</span>, <span class="string">&#x27;B1&#x27;</span>, <span class="string">&#x27;B2&#x27;</span>, <span class="string">&#x27;B3&#x27;</span>]&#125;)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>right = pd.DataFrame(&#123;<span class="string">&#x27;key1&#x27;</span>: [<span class="string">&#x27;K0&#x27;</span>, <span class="string">&#x27;K1&#x27;</span>, <span class="string">&#x27;K1&#x27;</span>, <span class="string">&#x27;K2&#x27;</span>],</span><br><span class="line"><span class="meta">... </span>                      <span class="string">&#x27;key2&#x27;</span>: [<span class="string">&#x27;K0&#x27;</span>, <span class="string">&#x27;K0&#x27;</span>, <span class="string">&#x27;K0&#x27;</span>, <span class="string">&#x27;K0&#x27;</span>],</span><br><span class="line"><span class="meta">... </span>                      <span class="string">&#x27;C&#x27;</span>: [<span class="string">&#x27;C0&#x27;</span>, <span class="string">&#x27;C1&#x27;</span>, <span class="string">&#x27;C2&#x27;</span>, <span class="string">&#x27;C3&#x27;</span>],</span><br><span class="line"><span class="meta">... </span>                      <span class="string">&#x27;D&#x27;</span>: [<span class="string">&#x27;D0&#x27;</span>, <span class="string">&#x27;D1&#x27;</span>, <span class="string">&#x27;D2&#x27;</span>, <span class="string">&#x27;D3&#x27;</span>]&#125;)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>right2 = pd.DataFrame(&#123;<span class="string">&#x27;key1&#x27;</span>: [<span class="string">&#x27;K0&#x27;</span>, <span class="string">&#x27;K1&#x27;</span>, <span class="string">&#x27;K1&#x27;</span>, <span class="string">&#x27;K2&#x27;</span>],</span><br><span class="line"><span class="meta">... </span>                      <span class="string">&#x27;key2&#x27;</span>: [<span class="string">&#x27;K0&#x27;</span>, <span class="string">&#x27;K0&#x27;</span>, <span class="string">&#x27;K0&#x27;</span>, <span class="string">&#x27;K0&#x27;</span>],</span><br><span class="line"><span class="meta">... </span>                      <span class="string">&#x27;C&#x27;</span>: [<span class="string">&#x27;C0&#x27;</span>, <span class="string">&#x27;C1&#x27;</span>, <span class="string">&#x27;C2&#x27;</span>, <span class="string">&#x27;C3&#x27;</span>]&#125;)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>left</span><br><span class="line">  key1 key2   A   B</span><br><span class="line"><span class="number">0</span>   K0   K0  A0  B0</span><br><span class="line"><span class="number">1</span>   K0   K1  A1  B1</span><br><span class="line"><span class="number">2</span>   K1   K0  A2  B2</span><br><span class="line"><span class="number">3</span>   K2   K1  A3  B3</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pd.merge(left, right, on=<span class="string">&#x27;key2&#x27;</span>) </span><br><span class="line">  key1_x key2   A   B key1_y   C   D</span><br><span class="line"><span class="number">0</span>     K0   K0  A0  B0     K0  C0  D0</span><br><span class="line"><span class="number">1</span>     K0   K0  A0  B0     K1  C1  D1</span><br><span class="line"><span class="number">2</span>     K0   K0  A0  B0     K1  C2  D2</span><br><span class="line"><span class="number">3</span>     K0   K0  A0  B0     K2  C3  D3</span><br><span class="line"><span class="number">4</span>     K1   K0  A2  B2     K0  C0  D0</span><br><span class="line"><span class="number">5</span>     K1   K0  A2  B2     K1  C1  D1</span><br><span class="line"><span class="number">6</span>     K1   K0  A2  B2     K1  C2  D2</span><br><span class="line"><span class="number">7</span>     K1   K0  A2  B2     K2  C3  D3</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>right</span><br><span class="line">  key1 key2   C   D</span><br><span class="line"><span class="number">0</span>   K0   K0  C0  D0</span><br><span class="line"><span class="number">1</span>   K1   K0  C1  D1</span><br><span class="line"><span class="number">2</span>   K1   K0  C2  D2</span><br><span class="line"><span class="number">3</span>   K2   K0  C3  D3</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pd.merge(left, right, on=[<span class="string">&#x27;key1&#x27;</span>,<span class="string">&#x27;key2&#x27;</span>])</span><br><span class="line">  key1 key2   A   B   C   D</span><br><span class="line"><span class="number">0</span>   K0   K0  A0  B0  C0  D0</span><br><span class="line"><span class="number">1</span>   K1   K0  A2  B2  C1  D1</span><br><span class="line"><span class="number">2</span>   K1   K0  A2  B2  C2  D2</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pd.merge(right,left, on=[<span class="string">&#x27;key1&#x27;</span>,<span class="string">&#x27;key2&#x27;</span>])  </span><br><span class="line">  key1 key2   C   D   A   B</span><br><span class="line"><span class="number">0</span>   K0   K0  C0  D0  A0  B0</span><br><span class="line"><span class="number">1</span>   K1   K0  C1  D1  A2  B2</span><br><span class="line"><span class="number">2</span>   K1   K0  C2  D2  A2  B2</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pd.merge(left, right, how=<span class="string">&#x27;outer&#x27;</span>, on=[<span class="string">&#x27;key1&#x27;</span>,<span class="string">&#x27;key2&#x27;</span>])</span><br><span class="line">  key1 key2    A    B    C    D</span><br><span class="line"><span class="number">0</span>   K0   K0   A0   B0   C0   D0</span><br><span class="line"><span class="number">1</span>   K0   K1   A1   B1  NaN  NaN</span><br><span class="line"><span class="number">2</span>   K1   K0   A2   B2   C1   D1</span><br><span class="line"><span class="number">3</span>   K1   K0   A2   B2   C2   D2</span><br><span class="line"><span class="number">4</span>   K2   K1   A3   B3  NaN  NaN</span><br><span class="line"><span class="number">5</span>   K2   K0  NaN  NaN   C3   D3</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pd.merge(left, right, how=<span class="string">&#x27;left&#x27;</span>, on=[<span class="string">&#x27;key1&#x27;</span>, <span class="string">&#x27;key2&#x27;</span>])</span><br><span class="line">  key1 key2   A   B    C    D</span><br><span class="line"><span class="number">0</span>   K0   K0  A0  B0   C0   D0</span><br><span class="line"><span class="number">1</span>   K0   K1  A1  B1  NaN  NaN</span><br><span class="line"><span class="number">2</span>   K1   K0  A2  B2   C1   D1</span><br><span class="line"><span class="number">3</span>   K1   K0  A2  B2   C2   D2</span><br><span class="line"><span class="number">4</span>   K2   K1  A3  B3  NaN  NaN</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pd.merge(left, right, how=<span class="string">&#x27;right&#x27;</span>, on=[<span class="string">&#x27;key1&#x27;</span>, <span class="string">&#x27;key2&#x27;</span>])</span><br><span class="line">  key1 key2    A    B   C   D</span><br><span class="line"><span class="number">0</span>   K0   K0   A0   B0  C0  D0</span><br><span class="line"><span class="number">1</span>   K1   K0   A2   B2  C1  D1</span><br><span class="line"><span class="number">2</span>   K1   K0   A2   B2  C2  D2</span><br><span class="line"><span class="number">3</span>   K2   K0  NaN  NaN  C3  D3</span><br></pre></td></tr></table></figure>

<h2 id="join"><a href="#join" class="headerlink" title="join"></a>join</h2><p>join函数作用是将多个pandas对象横向拼接，遇到重复的索引项时会使用笛卡尔积，默认左连接，可选inner、outer、right连接</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>left = pd.DataFrame(&#123;<span class="string">&#x27;A&#x27;</span>: [<span class="string">&#x27;A0&#x27;</span>, <span class="string">&#x27;A1&#x27;</span>, <span class="string">&#x27;A2&#x27;</span>],</span><br><span class="line"><span class="meta">... </span>                     <span class="string">&#x27;B&#x27;</span>: [<span class="string">&#x27;B0&#x27;</span>, <span class="string">&#x27;B1&#x27;</span>, <span class="string">&#x27;B2&#x27;</span>]&#125;,</span><br><span class="line"><span class="meta">... </span>                    index=[<span class="string">&#x27;K0&#x27;</span>, <span class="string">&#x27;K1&#x27;</span>, <span class="string">&#x27;K2&#x27;</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>right = pd.DataFrame(&#123;<span class="string">&#x27;C&#x27;</span>: [<span class="string">&#x27;C0&#x27;</span>, <span class="string">&#x27;C2&#x27;</span>, <span class="string">&#x27;C3&#x27;</span>],</span><br><span class="line"><span class="meta">... </span>                      <span class="string">&#x27;D&#x27;</span>: [<span class="string">&#x27;D0&#x27;</span>, <span class="string">&#x27;D2&#x27;</span>, <span class="string">&#x27;D3&#x27;</span>]&#125;,</span><br><span class="line"><span class="meta">... </span>                    index=[<span class="string">&#x27;K0&#x27;</span>, <span class="string">&#x27;K2&#x27;</span>, <span class="string">&#x27;K3&#x27;</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>left.join(right)</span><br><span class="line">     A   B    C    D</span><br><span class="line">K0  A0  B0   C0   D0</span><br><span class="line">K1  A1  B1  NaN  NaN</span><br><span class="line">K2  A2  B2   C2   D2</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>left.join(right,how=<span class="string">&#x27;outer&#x27;</span>) </span><br><span class="line">      A    B    C    D</span><br><span class="line">K0   A0   B0   C0   D0</span><br><span class="line">K1   A1   B1  NaN  NaN</span><br><span class="line">K2   A2   B2   C2   D2</span><br><span class="line">K3  NaN  NaN   C3   D3</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>left.join(right,how=<span class="string">&#x27;left&#x27;</span>)</span><br><span class="line">     A   B    C    D</span><br><span class="line">K0  A0  B0   C0   D0</span><br><span class="line">K1  A1  B1  NaN  NaN</span><br><span class="line">K2  A2  B2   C2   D2</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>left.join(right,how=<span class="string">&#x27;right&#x27;</span>)</span><br><span class="line">      A    B   C   D</span><br><span class="line">K0   A0   B0  C0  D0</span><br><span class="line">K2   A2   B2  C2  D2</span><br><span class="line">K3  NaN  NaN  C3  D3</span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>计算机</tag>
      </tags>
  </entry>
  <entry>
    <title>Servlet学习（一）</title>
    <url>/2020/05/09/Learning-servlet-01/</url>
    <content><![CDATA[<h1 id="Servlet"><a href="#Servlet" class="headerlink" title="Servlet"></a>Servlet</h1><blockquote>
<p><strong>Servlet</strong>（Server Applet）是Java Servlet的简称，称为小服务程序或服务连接器，用Java编写的服务器端程序，具有独立于平台和协议的特性，主要功能在于交互式地浏览和生成数据，生成动态Web内容。Servlet运行于支持Java的应用服务器中。从原理上讲，Servlet可以响应任何类型的请求，但绝大多数情况下Servlet只用来扩展基于HTTP协议的Web服务器。</p>
</blockquote>
<span id="more"></span>

<h2 id="Servlet生命周期"><a href="#Servlet生命周期" class="headerlink" title="Servlet生命周期"></a>Servlet生命周期</h2><ul>
<li>Servlet 通过调用 init () 方法进行初始化。</li>
<li>Servlet 调用 service() 方法来处理客户端的请求。</li>
<li>Servlet 通过调用 destroy() 方法终止（结束）。</li>
<li>最后，Servlet 是由 JVM 的垃圾回收器进行垃圾回收的。</li>
</ul>
<h3 id="init-方法"><a href="#init-方法" class="headerlink" title="init()方法"></a>init()方法</h3><p>它只在第一次创建Servlet时被调用。</p>
<p>当用户调用一个 Servlet 时，就会创建一个 Servlet 实例，每一个用户请求都会产生一个新的线程，适当的时候移交给 doGet 或 doPost 方法。</p>
<p>init() 方法简单地创建或加载一些数据，这些数据将被用于 Servlet 的整个生命周期。</p>
<p>init方法定义如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">init</span><span class="params">()</span> <span class="keyword">throws</span> ServletException &#123;</span><br><span class="line">  <span class="comment">// 初始化代码...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="service方法"><a href="#service方法" class="headerlink" title="service方法"></a>service方法</h3><p>service() 方法是执行实际任务的主要方法。Servlet 容器（即 Web 服务器）调用 service() 方法来处理来自客户端（浏览器）的请求，并把格式化的响应写回给客户端。</p>
<p>每次服务器接收到一个 Servlet 请求时，服务器会产生一个新的线程并调用服务。service() 方法检查 HTTP 请求类型（GET、POST、PUT、DELETE 等），并在适当的时候调用<br>doGet、doPost、doPut，doDelete 等方法。在实际操作中，我们只需根据客户端的请求类型重写doGet()或doPost()即可。</p>
<ul>
<li>doGet()的请求来自一个url的正常请求，或者来自于一个未指定 METHOD 的 HTML 表单，它由 doGet() 方法处理。</li>
<li>doPost()方法中POST 请求来自于一个特别指定了 METHOD 为 POST 的 HTML 表单，它由 doPost() 方法处理。如 <code>&lt;form action=&quot;HelloForm&quot; method=&quot;POST&quot;&gt;</code><br>就是在实例html中一个调用HelloForm中POST方法的表单，web在处理时就会产生新线程并调用HelloForm中的POST服务。</li>
</ul>
<p>service方法定义如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">service</span><span class="params">(ServletRequest request, </span></span><br><span class="line"><span class="params">                    ServletResponse response)</span> </span><br><span class="line">      <span class="keyword">throws</span> ServletException, IOException&#123;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="destroy-方法"><a href="#destroy-方法" class="headerlink" title="destroy() 方法"></a>destroy() 方法</h3><p>destroy() 方法只会被调用一次，在 Servlet 生命周期结束时被调用。</p>
<p>destroy() 方法可以让您的 Servlet 关闭数据库连接、停止后台线程、把 Cookie 列表或点击计数器写入到磁盘，并执行其他类似的清理活动。</p>
<p>在调用 destroy() 方法之后，servlet 对象被标记为垃圾回收。</p>
<p>destroy方法定义如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">destroy</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="comment">// 终止化代码...</span></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h2 id="ServletRequest，ServletResponse"><a href="#ServletRequest，ServletResponse" class="headerlink" title="ServletRequest，ServletResponse"></a>ServletRequest，ServletResponse</h2><p>Web服务器收到客户端的http请求，会针对每一次请求，分别创建一个用于代表请求的ServletRequest对象、和代表响应的ServletResponse对象。获取网页提交过来的数据，只需要找ServletRequest对象就行了。要向网页输出数据，只需要找ServletResponse对象。</p>
<p>ServletResponse文档：<a href="https://docs.oracle.com/javaee/6/api/javax/servlet/http/HttpServletResponse.html">https://docs.oracle.com/javaee/6/api/javax/servlet/http/HttpServletResponse.html</a></p>
<p>ServletRequest文档：<a href="https://docs.oracle.com/javaee/6/api/javax/servlet/ServletRequest.html">https://docs.oracle.com/javaee/6/api/javax/servlet/ServletRequest.html</a></p>
<h2 id="异常处理"><a href="#异常处理" class="headerlink" title="异常处理"></a>异常处理</h2><p>当一个 Servlet 抛出一个异常时，Web 容器在使用了 exception-type 元素的 web.xml 中搜索与抛出异常类型相匹配的配置。</p>
<p>您必须在 web.xml 中使用 error-page 元素来指定对特定异常 或 HTTP 状态码 作出相应的 Servlet 调用。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 对所有的异常有一个通用的错误处理程序 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">error-page</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">exception-type</span>&gt;</span>java.lang.Throwable&lt;/exception-type &gt;</span><br><span class="line">    <span class="tag">&lt;<span class="name">location</span>&gt;</span>/ErrorHandler<span class="tag">&lt;/<span class="name">location</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">error-page</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="请求属性-错误-x2F-异常"><a href="#请求属性-错误-x2F-异常" class="headerlink" title="请求属性 - 错误&#x2F;异常"></a>请求属性 - 错误&#x2F;异常</h3><p>以下是错误处理的 Servlet 可以访问的请求属性列表，用来分析错误&#x2F;异常的性质。</p>
<table>
<thead>
<tr>
<th>column0</th>
<th>column1</th>
</tr>
</thead>
<tbody><tr>
<td>序号</td>
<td>属性 &amp; 描述</td>
</tr>
<tr>
<td>1</td>
<td>javax.servlet.error.status_code. 该属性给出状态码，状态码可被存储，并在存储为 java.lang. Integer 数据类型后可被分析。</td>
</tr>
<tr>
<td>2</td>
<td>javax.servlet.error.exception_type. 该属性给出异常类型的信息，异常类型可被存储，并在存储为 java.lang. Class 数据类型后可被分析。</td>
</tr>
<tr>
<td>3</td>
<td>javax.servlet.error.message. 该属性给出确切错误消息的信息，信息可被存储，并在存储为 java.lang. String 数据类型后可被分析。</td>
</tr>
<tr>
<td>4</td>
<td>javax.servlet.error.request_uri. 该属性给出有关 URL 调用 Servlet 的信息，信息可被存储，并在存储为 java.lang. String 数据类型后可被分析。</td>
</tr>
<tr>
<td>5</td>
<td>javax.servlet.error.exception. 该属性给出异常产生的信息，信息可被存储，并在存储为 java.lang. Throwable 数据类型后可被分析。</td>
</tr>
<tr>
<td>6</td>
<td>javax.servlet.error.servlet_name. 该属性给出 Servlet 的名称，名称可被存储，并在存储为 java.lang. String 数据类型后可被分析。</td>
</tr>
</tbody></table>
<h2 id="POST，GET"><a href="#POST，GET" class="headerlink" title="POST，GET"></a>POST，GET</h2><p>最后我想介绍一下POST与GET方法之间的异同。</p>
<p>POST，GET属于两种常见的请求方法。</p>
<p>在浏览器中直接输入 URL 并回车，这便发起了一个 GET 请求，请求的参数会直接包含到 URL 里。例如，在百度中搜索 Servlet ，这就是一个 GET<br>请求，链接为 <a href="https://www.baidu.com/s?wd=Servlet">https://www.baidu.com/s?wd=Servlet</a> ，其中 URL 中包含了请求的参数信息，这里参数 wd 表示要搜寻的关键字。POST<br>请求大多在表单提交时发起。比如，对于一个登录表单，输入用户名和密码后，点击 “登录” 按钮，这通常会发起一个 POST 请求，其数据通常以表单的形式传输，而不会体现在 URL 中。</p>
<p>GET 和 POST 请求方法有如下区别。</p>
<ul>
<li>GET 请求中的参数包含在 URL 里面，数据可以在 URL 中看到，而 POST 请求的 URL 不会包含这些数据，数据都是通过表单形式传输的，会包含在请求体中。</li>
<li>GET 请求提交的数据最多只有 1024 字节，而 POST 方式没有限制。</li>
</ul>
<p><em>一般来说，登录时，需要提交用户名和密码，其中包含了敏感信息，使用 GET 方式请求的话，密码就会暴露在 URL 里面，造成密码泄露，所以这里最好以 POST 方式发送。上传文件时，由于文件内容比较大，也会选用 POST 方式。</em></p>
]]></content>
      <tags>
        <tag>计算机</tag>
      </tags>
  </entry>
  <entry>
    <title>Servlet学习（三）</title>
    <url>/2020/05/14/Learning-servlet-03/</url>
    <content><![CDATA[<h1 id="Servlet其他服务"><a href="#Servlet其他服务" class="headerlink" title="Servlet其他服务"></a>Servlet其他服务</h1><blockquote>
<p>本篇文章最后介绍一些Servlet上文件上传，处理日期，网页重定向，自动刷新等一些操作</p>
</blockquote>
<span id="more"></span>

<h2 id="文件上传"><a href="#文件上传" class="headerlink" title="文件上传"></a>文件上传</h2><p>Servlet 可以与 HTML form 标签一起使用，来允许用户上传文件到服务器。上传的文件可以是文本文件或图像文件或任何文档。</p>
<p><em>工程中需要引入jar文件：commons-fileupload-1.3.2、commons-io-2.5.jar。</em></p>
<ol>
<li>创建一个文件上传表单</li>
</ol>
<ul>
<li>表单 method 属性应该设置为 POST 方法，不能使用 GET 方法。</li>
<li>表单 enctype 属性应该设置为 multipart&#x2F;form-data.</li>
<li>表单 action 属性应该设置为在后端服务器上处理文件上传的 Servlet 文件。下面的实例使用了 UploadServlet Servlet 来上传文件。</li>
<li>上传单个文件，您应该使用单个带有属性 type&#x3D;”file” 的 &lt;input …&#x2F;&gt; 标签。为了允许多个文件上传，请包含多个 name 属性值不同的 input 标签。输入标签具有不同的名称属性的值。浏览器会为每个 input<br>标签关联一个浏览按钮。</li>
</ul>
<figure class="highlight jsp"><table><tr><td class="code"><pre><span class="line">&lt;form method=<span class="string">&quot;post&quot;</span> action=<span class="string">&quot;/TomcatTest/UploadServlet&quot;</span> enctype=<span class="string">&quot;multipart/form-data&quot;</span>&gt;</span><br><span class="line">    选择一个文件:</span><br><span class="line">    &lt;input type=<span class="string">&quot;file&quot;</span> name=<span class="string">&quot;uploadFile&quot;</span> /&gt;</span><br><span class="line">    &lt;br/&gt;&lt;br/&gt;</span><br><span class="line">    &lt;input type=<span class="string">&quot;submit&quot;</span> value=<span class="string">&quot;上传&quot;</span> /&gt;</span><br><span class="line">&lt;/form&gt;</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>编写后台 Servlet</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.File;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.io.PrintWriter;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> javax.servlet.ServletException;</span><br><span class="line"><span class="keyword">import</span> javax.servlet.annotation.WebServlet;</span><br><span class="line"><span class="keyword">import</span> javax.servlet.http.HttpServlet;</span><br><span class="line"><span class="keyword">import</span> javax.servlet.http.HttpServletRequest;</span><br><span class="line"><span class="keyword">import</span> javax.servlet.http.HttpServletResponse;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> org.apache.commons.fileupload.FileItem;</span><br><span class="line"><span class="keyword">import</span> org.apache.commons.fileupload.disk.DiskFileItemFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.commons.fileupload.servlet.ServletFileUpload;</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Servlet implementation class UploadServlet</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@WebServlet(&quot;/UploadServlet&quot;)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">UploadServlet</span> <span class="keyword">extends</span> <span class="title class_">HttpServlet</span> &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">     </span><br><span class="line">    <span class="comment">// 上传文件存储目录</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">String</span> <span class="variable">UPLOAD_DIRECTORY</span> <span class="operator">=</span> <span class="string">&quot;upload&quot;</span>;</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// 上传配置</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">MEMORY_THRESHOLD</span>   <span class="operator">=</span> <span class="number">1024</span> * <span class="number">1024</span> * <span class="number">3</span>;  <span class="comment">// 3MB</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">MAX_FILE_SIZE</span>      <span class="operator">=</span> <span class="number">1024</span> * <span class="number">1024</span> * <span class="number">40</span>; <span class="comment">// 40MB</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">MAX_REQUEST_SIZE</span>   <span class="operator">=</span> <span class="number">1024</span> * <span class="number">1024</span> * <span class="number">50</span>; <span class="comment">// 50MB</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 上传数据及保存文件</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">doPost</span><span class="params">(HttpServletRequest request,</span></span><br><span class="line"><span class="params">        HttpServletResponse response)</span> <span class="keyword">throws</span> ServletException, IOException &#123;</span><br><span class="line">        <span class="comment">// 检测是否为多媒体上传</span></span><br><span class="line">        <span class="keyword">if</span> (!ServletFileUpload.isMultipartContent(request)) &#123;</span><br><span class="line">            <span class="comment">// 如果不是则停止</span></span><br><span class="line">            <span class="type">PrintWriter</span> <span class="variable">writer</span> <span class="operator">=</span> response.getWriter();</span><br><span class="line">            writer.println(<span class="string">&quot;Error: 表单必须包含 enctype=multipart/form-data&quot;</span>);</span><br><span class="line">            writer.flush();</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"> </span><br><span class="line">        <span class="comment">// 配置上传参数</span></span><br><span class="line">        <span class="type">DiskFileItemFactory</span> <span class="variable">factory</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">DiskFileItemFactory</span>();</span><br><span class="line">        <span class="comment">// 设置内存临界值 - 超过后将产生临时文件并存储于临时目录中</span></span><br><span class="line">        factory.setSizeThreshold(MEMORY_THRESHOLD);</span><br><span class="line">        <span class="comment">// 设置临时存储目录</span></span><br><span class="line">        factory.setRepository(<span class="keyword">new</span> <span class="title class_">File</span>(System.getProperty(<span class="string">&quot;java.io.tmpdir&quot;</span>)));</span><br><span class="line"> </span><br><span class="line">        <span class="type">ServletFileUpload</span> <span class="variable">upload</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ServletFileUpload</span>(factory);</span><br><span class="line">         </span><br><span class="line">        <span class="comment">// 设置最大文件上传值</span></span><br><span class="line">        upload.setFileSizeMax(MAX_FILE_SIZE);</span><br><span class="line">         </span><br><span class="line">        <span class="comment">// 设置最大请求值 (包含文件和表单数据)</span></span><br><span class="line">        upload.setSizeMax(MAX_REQUEST_SIZE);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 中文处理</span></span><br><span class="line">        upload.setHeaderEncoding(<span class="string">&quot;UTF-8&quot;</span>); </span><br><span class="line"></span><br><span class="line">        <span class="comment">// 构造临时路径来存储上传的文件</span></span><br><span class="line">        <span class="comment">// 这个路径相对当前应用的目录</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">uploadPath</span> <span class="operator">=</span> request.getServletContext().getRealPath(<span class="string">&quot;./&quot;</span>) + File.separator + UPLOAD_DIRECTORY;</span><br><span class="line">       </span><br><span class="line">         </span><br><span class="line">        <span class="comment">// 如果目录不存在则创建</span></span><br><span class="line">        <span class="type">File</span> <span class="variable">uploadDir</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">File</span>(uploadPath);</span><br><span class="line">        <span class="keyword">if</span> (!uploadDir.exists()) &#123;</span><br><span class="line">            uploadDir.mkdir();</span><br><span class="line">        &#125;</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 解析请求的内容提取文件数据</span></span><br><span class="line">            <span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span></span><br><span class="line">            List&lt;FileItem&gt; formItems = upload.parseRequest(request);</span><br><span class="line"> </span><br><span class="line">            <span class="keyword">if</span> (formItems != <span class="literal">null</span> &amp;&amp; formItems.size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="comment">// 迭代表单数据</span></span><br><span class="line">                <span class="keyword">for</span> (FileItem item : formItems) &#123;</span><br><span class="line">                    <span class="comment">// 处理不在表单中的字段</span></span><br><span class="line">                    <span class="keyword">if</span> (!item.isFormField()) &#123;</span><br><span class="line">                        <span class="type">String</span> <span class="variable">fileName</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">File</span>(item.getName()).getName();</span><br><span class="line">                        <span class="type">String</span> <span class="variable">filePath</span> <span class="operator">=</span> uploadPath + File.separator + fileName;</span><br><span class="line">                        <span class="type">File</span> <span class="variable">storeFile</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">File</span>(filePath);</span><br><span class="line">                        <span class="comment">// 在控制台输出文件的上传路径</span></span><br><span class="line">                        System.out.println(filePath);</span><br><span class="line">                        <span class="comment">// 保存文件到硬盘</span></span><br><span class="line">                        item.write(storeFile);</span><br><span class="line">                        request.setAttribute(<span class="string">&quot;message&quot;</span>,</span><br><span class="line">                            <span class="string">&quot;文件上传成功!&quot;</span>);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception ex) &#123;</span><br><span class="line">            request.setAttribute(<span class="string">&quot;message&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;错误信息: &quot;</span> + ex.getMessage());</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 跳转到 message.jsp</span></span><br><span class="line">        request.getServletContext().getRequestDispatcher(<span class="string">&quot;/message.jsp&quot;</span>).forward(</span><br><span class="line">                request, response);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>编译和运行 Servlet</li>
</ol>
<ul>
<li>编译上面的 Servlet UploadServlet，并在 web.xml 文件中创建所需的条目，</li>
</ul>
<h2 id="Servlet-处理日期"><a href="#Servlet-处理日期" class="headerlink" title="Servlet 处理日期"></a>Servlet 处理日期</h2><p>Servlet可以直接使用Java 提供的 java.util 包中的 Date 类。</p>
<table>
<thead>
<tr>
<th>column0</th>
<th>column1</th>
</tr>
</thead>
<tbody><tr>
<td>序号</td>
<td>方法 &amp; 描述</td>
</tr>
<tr>
<td>1</td>
<td>boolean after(Date date)。如果调用的 Date 对象中包含的日期在 date 指定的日期之后，则返回 true，否则返回 false。</td>
</tr>
<tr>
<td>2</td>
<td>boolean before(Date date)。如果调用的 Date 对象中包含的日期在 date 指定的日期之前，则返回 true，否则返回 false。</td>
</tr>
<tr>
<td>3</td>
<td>Object clone( )。重复调用 Date 对象。</td>
</tr>
<tr>
<td>4</td>
<td>int compareTo(Date date)。把调用对象的值与 date 的值进行比较。如果两个值是相等的，则返回 0。如果调用对象在 date 之前，则返回一个负值。如果调用对象在 date 之后，则返回一个正值。</td>
</tr>
<tr>
<td>5</td>
<td>int compareTo(Object obj)。如果 obj 是 Date 类，则操作等同于 compareTo(Date)。否则，它会抛出一个 ClassCastException。</td>
</tr>
<tr>
<td>6</td>
<td>boolean equals(Object date)。如果调用的 Date 对象中包含的时间和日期与 date 指定的相同，则返回 true，否则返回 false。</td>
</tr>
<tr>
<td>7</td>
<td>long getTime( )。返回 1970 年 1 月 1 日以来经过的毫秒数。</td>
</tr>
<tr>
<td>8</td>
<td>int hashCode( )。为调用对象返回哈希代码。</td>
</tr>
<tr>
<td>9</td>
<td>void setTime(long time)。设置 time 指定的时间和日期，这表示从 1970 年 1 月 1 日午夜以来经过的时间（以毫秒为单位）。</td>
</tr>
<tr>
<td>10</td>
<td>String toString( )。转换调用的 Date 对象为一个字符串，并返回结果。</td>
</tr>
</tbody></table>
<h2 id="Servlet-网页重定向"><a href="#Servlet-网页重定向" class="headerlink" title="Servlet 网页重定向"></a>Servlet 网页重定向</h2><p>当文档移动到新的位置，我们需要向客户端发送这个新位置时，我们需要用到网页重定向。网页重定向主要是为了实现负载均衡。</p>
<p>实现方法如下：</p>
<ol>
<li>sendRedirect() 方法</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> HttpServletResponse.sendRedirect(String location)</span><br><span class="line"><span class="keyword">throws</span> IOException </span><br></pre></td></tr></table></figure>

<p>该方法把响应连同状态码和新的网页位置发送回浏览器。</p>
<ol start="2">
<li>setStatus() 和 setHeader() 方法</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">String</span> <span class="variable">site</span> <span class="operator">=</span> <span class="string">&quot;http://www.nanaiii.com&quot;</span> ;</span><br><span class="line">response.setStatus(response.SC_MOVED_TEMPORARILY);</span><br><span class="line">response.setHeader(<span class="string">&quot;Location&quot;</span>, site); </span><br></pre></td></tr></table></figure>

<h2 id="Servlet-点击计数器"><a href="#Servlet-点击计数器" class="headerlink" title="Servlet 点击计数器"></a>Servlet 点击计数器</h2><h3 id="网页点击计数器"><a href="#网页点击计数器" class="headerlink" title="网页点击计数器"></a>网页点击计数器</h3><p>很多时候，您可能有兴趣知道网站的某个特定页面上的总点击量。使用 Servlet 来计算这些点击量是非常简单的，因为一个 Servlet 的生命周期是由它运行所在的容器控制的。</p>
<p>以下是实现一个简单的基于 Servlet 生命周期的网页点击计数器需要采取的步骤：</p>
<ol>
<li>在 init() 方法中初始化一个全局变量。</li>
<li>每次调用 doGet() 或 doPost() 方法时，都增加全局变量。</li>
<li>如果需要，您可以使用一个数据库表来存储全局变量的值在 destroy() 中。在下次初始化 Servlet 时，该值可在 init() 方法内被读取。这一步是可选的。</li>
<li>如果您只想对一个 session 会话计数一次页面点击，那么请使用 isNew() 方法来检查该 session 会话是否已点击过相同页面。这一步是可选的。</li>
<li>您可以通过显示全局计数器的值，来在网站上展示页面的总点击量。这一步是可选的。</li>
</ol>
<h3 id="网站点击计数器"><a href="#网站点击计数器" class="headerlink" title="网站点击计数器"></a>网站点击计数器</h3><p>很多时候，您可能有兴趣知道整个网站的总点击量。在 Servlet 中，这也是非常简单的，我们可以使用过滤器做到这一点。</p>
<p>以下是实现一个简单的基于过滤器生命周期的网站点击计数器需要采取的步骤：</p>
<ol>
<li>在过滤器的 init() 方法中初始化一个全局变量。</li>
<li>每次调用 doFilter 方法时，都增加全局变量。</li>
<li>如果需要，您可以在过滤器的 destroy() 中使用一个数据库表来存储全局变量的值。在下次初始化过滤器时，该值可在 init() 方法内被读取, 这一步是可选的。</li>
</ol>
<p>代码实现：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span>  <span class="title function_">doFilter</span><span class="params">(ServletRequest request, </span></span><br><span class="line"><span class="params">            ServletResponse response,</span></span><br><span class="line"><span class="params">            FilterChain chain)</span> </span><br><span class="line">            <span class="keyword">throws</span> java.io.IOException, ServletException &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 把计数器的值增加 1</span></span><br><span class="line">    hitCount++;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 输出计数器</span></span><br><span class="line">    System.out.println(<span class="string">&quot;网站访问统计：&quot;</span>+ hitCount );</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 把请求传回到过滤器链</span></span><br><span class="line">    chain.doFilter(request,response);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>web.xml中的配置：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">filter</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">filter-name</span>&gt;</span>SiteHitCounter<span class="tag">&lt;/<span class="name">filter-name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">filter-class</span>&gt;</span>SiteHitCounter<span class="tag">&lt;/<span class="name">filter-class</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">filter-mapping</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">filter-name</span>&gt;</span>SiteHitCounter<span class="tag">&lt;/<span class="name">filter-name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">url-pattern</span>&gt;</span>/*<span class="tag">&lt;/<span class="name">url-pattern</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">filter-mapping</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h2 id="自动刷新页面"><a href="#自动刷新页面" class="headerlink" title="自动刷新页面"></a>自动刷新页面</h2><h3 id="setIntHeader-方法"><a href="#setIntHeader-方法" class="headerlink" title="setIntHeader()方法"></a>setIntHeader()方法</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setIntHeader</span><span class="params">(String header, <span class="type">int</span> headerValue)</span></span><br><span class="line"><span class="comment">//实例</span></span><br><span class="line"><span class="comment">// 设置刷新自动加载的事件间隔为 5 秒</span></span><br><span class="line">response.setIntHeader(<span class="string">&quot;Refresh&quot;</span>, <span class="number">5</span>);</span><br></pre></td></tr></table></figure>

<p>此方法把头信息 “Refresh” 连同一个表示时间间隔的整数值（以秒为单位）发送回浏览器，从而实现自动刷新页面。</p>
]]></content>
      <tags>
        <tag>计算机</tag>
      </tags>
  </entry>
  <entry>
    <title>Servlet学习（二）</title>
    <url>/2020/05/10/Learning-servlet-02/</url>
    <content><![CDATA[<h1 id="Cookie，Session"><a href="#Cookie，Session" class="headerlink" title="Cookie，Session"></a>Cookie，Session</h1><blockquote>
<p>本文主要介绍Servlet对Cookies与Session的处理。因此在此之前，我们先了解什么是Cookie和Session。</p>
</blockquote>
<span id="more"></span>

<h2 id="无状态HTTP"><a href="#无状态HTTP" class="headerlink" title="无状态HTTP"></a>无状态HTTP</h2><p>HTTP 的无状态是指 HTTP<br>协议对事务处理是没有记忆能力的，也就是说服务器不知道客户端是什么状态。当我们向服务器发送请求后，服务器解析此请求，然后返回对应的响应，服务器负责完成这个过程，而且这个过程是完全独立的，服务器不会记录前后状态的变化，也就是缺少状态记录。这意味着如果后续需要处理前面的信息，则必须重传，这导致需要额外传递一些前面的重复请求，才能获取后续响应，然而这种效果显然不是我们想要的。为了保持前后状态，我们肯定不能将前面的请求全部重传一次，这太浪费资源了，对于这种需要用户登录的页面来说，更是棘手。</p>
<p>而Cookies与Session就是用于保持HTTP连接状态的技术。Session在服务端，也就是网站的服务器，用来保存用户的会话信息；Cookies 在客户端，也可以理解为浏览器端，有了<br>Cookies，浏览器在下次访问网页时会自动附带上它发送给服务器，服务器通过识别 Cookies 并鉴定出是哪个用户，然后再判断用户是否是登录状态，然后返回对应的响应。</p>
<p>我们可以理解为 Cookies 里面保存了登录的凭证，有了它，只需要在下次请求携带 Cookies 发送请求而不必重新输入用户名、密码等信息重新登录了。</p>
<h3 id="Session"><a href="#Session" class="headerlink" title="Session"></a>Session</h3><p>在 Web 中，会话对象用来存储特定用户会话所需的属性及配置信息。这样，当用户在应用程序的 Web 页之间跳转时，存储在会话对象中的变量将不会丢失，而是在整个用户会话中一直存在下去。当用户请求来自应用程序的 Web<br>页时，如果该用户还没有会话，则 Web 服务器将自动创建一个会话对象。当会话过期或被放弃后，服务器将终止该会话。</p>
<h3 id="Cookies"><a href="#Cookies" class="headerlink" title="Cookies"></a>Cookies</h3><p>Cookies 指某些网站为了辨别用户身份、进行会话跟踪而存储在用户本地终端上的数据。</p>
<h4 id="Cookies属性结构"><a href="#Cookies属性结构" class="headerlink" title="Cookies属性结构"></a>Cookies属性结构</h4><ul>
<li>Name，即该 Cookie 的名称。Cookie 一旦创建，名称便不可更改</li>
<li>Value，即该 Cookie 的值。如果值为 Unicode 字符，需要为字符编码。如果值为二进制数据，则需要使用 BASE64 编码。</li>
<li>Max Age，即该 Cookie 失效的时间，单位秒，也常和 Expires 一起使用，通过它可以计算出其有效时间。Max Age 如果为正数，则该 Cookie 在 Max Age 秒之后失效。如果为负数，则关闭浏览器时<br>Cookie 即失效，浏览器也不会以任何形式保存该 Cookie。</li>
<li>Path，即该 Cookie 的使用路径。如果设置为 &#x2F;path&#x2F;，则只有路径为 &#x2F;path&#x2F; 的页面可以访问该 Cookie。如果设置为 &#x2F;，则本域名下的所有页面都可以访问该 Cookie。</li>
<li>Domain，即可以访问该 Cookie 的域名。例如如果设置为 .zhihu.com，则所有以 zhihu.com，结尾的域名都可以访问该 Cookie。</li>
<li>Size 字段，即此 Cookie 的大小。</li>
<li>Http 字段，即 Cookie 的 httponly 属性。若此属性为 true，则只有在 HTTP Headers 中会带有此 Cookie 的信息，而不能通过 document.cookie 来访问此 Cookie。</li>
<li>Secure，即该 Cookie 是否仅被使用安全协议传输。安全协议。安全协议有 HTTPS，SSL 等，在网络上传输数据之前先将数据加密。默认为 false。</li>
</ul>
<h3 id="会话维持"><a href="#会话维持" class="headerlink" title="会话维持"></a>会话维持</h3><p>当客户端第一次请求服务器时，服务器会返回一个响应头中带有 Set-Cookie 字段的响应给客户端，用来标记是哪一个用户，客户端浏览器会把 Cookies 保存起来。当浏览器下一次再请求该网站时，浏览器会把此 Cookies<br>放到请求头一起提交给服务器，Cookies 携带了会话 ID 信息，服务器检查该 Cookies 即可找到对应的会话是什么，然后再判断会话来以此来辨认用户状态。</p>
<p>在成功登录某个网站时，服务器会告诉客户端设置哪些 Cookies 信息，在后续访问页面时客户端会把 Cookies<br>发送给服务器，服务器再找到对应的会话加以判断。如果会话中的某些设置登录状态的变量是有效的，那就证明用户处于登录状态，此时返回登录之后才可以查看的网页内容，浏览器再进行解析便可以看到了。</p>
<p>反之，如果传给服务器的 Cookies 是无效的，或者会话已经过期了，我们将不能继续访问页面，此时可能会收到错误的响应或者跳转到登录页面重新登录。</p>
<p>所以，Cookies 和会话需要配合，一个处于客户端，一个处于服务端，二者共同协作，就实现了登录会话控制。</p>
<h3 id="会话-Cookie-和持久-Cookie"><a href="#会话-Cookie-和持久-Cookie" class="headerlink" title="会话 Cookie 和持久 Cookie"></a>会话 Cookie 和持久 Cookie</h3><p>从表面意思来说，会话 Cookie 就是把 Cookie 放在浏览器内存里，浏览器在关闭之后该 Cookie 即失效；持久 Cookie 则会保存到客户端的硬盘中，下次还可以继续使用，用于长久保持用户登录状态。</p>
<p>其实严格来说，没有会话 Cookie 和持久 Cookie 之分，只是由 Cookie 的 Max Age 或 Expires 字段决定了过期的时间。</p>
<p>因此，一些持久化登录的网站其实就是把 Cookie 的有效时间和会话有效期设置得比较长，下次我们再访问页面时仍然携带之前的 Cookie，就可以直接保持登录状态。</p>
<h2 id="Servlet-Cookie-处理"><a href="#Servlet-Cookie-处理" class="headerlink" title="Servlet Cookie 处理"></a>Servlet Cookie 处理</h2><h3 id="通过-Servlet-设置-Cookie"><a href="#通过-Servlet-设置-Cookie" class="headerlink" title="通过 Servlet 设置 Cookie"></a>通过 Servlet 设置 Cookie</h3><ol>
<li>创建一个 Cookie 对象：您可以调用带有 cookie 名称和 cookie 值的 Cookie 构造函数，cookie 名称和 cookie 值都是字符串。</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">Cookie</span> <span class="variable">cookie</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Cookie</span>(<span class="string">&quot;key&quot;</span>,<span class="string">&quot;value&quot;</span>);</span><br></pre></td></tr></table></figure>

<p><em>注意：’key’, ‘value’中不应该出现空格和[ ] ( ) &#x3D; , “ &#x2F; ? @ : ; 等字符。</em></p>
<ol start="2">
<li>设置最大生存周期：您可以使用 setMaxAge 方法来指定 cookie 能够保持有效的时间（以秒为单位）。</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">cookie.setMaxAge(<span class="number">60</span>*<span class="number">60</span>*<span class="number">24</span>); <span class="comment">//最长有效期24小时</span></span><br></pre></td></tr></table></figure>

<ol start="3">
<li>发送 Cookie 到 HTTP 响应头：您可以使用 response.addCookie 来添加 HTTP 响应头中的 Cookie。</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">response.addCookie(cookie);</span><br></pre></td></tr></table></figure>

<h3 id="通过-Servlet-读取-Cookie"><a href="#通过-Servlet-读取-Cookie" class="headerlink" title="通过 Servlet 读取 Cookie"></a>通过 Servlet 读取 Cookie</h3><p>要读取 Cookie，您需要通过调用 HttpServletRequest 的 getCookies( ) 方法创建一个 javax.servlet.http. Cookie 对象的数组。然后循环遍历数组，并使用 getName() 和<br>getValue() 方法来访问每个 cookie 和关联的值。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">Cookie</span> <span class="variable">cookie</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line">Cookie[] cookies = <span class="literal">null</span>;</span><br><span class="line"><span class="type">PrintWriter</span> <span class="variable">out</span> <span class="operator">=</span> response.getWriter();</span><br><span class="line"><span class="comment">// 获取与该域相关的 Cookie 的数组</span></span><br><span class="line">cookies = request.getCookies();</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; cookies.length; i++)&#123;</span><br><span class="line">    cookie = cookies[i];</span><br><span class="line">    out.print(<span class="string">&quot;名称：&quot;</span> + cookie.getName( ) + <span class="string">&quot;，&quot;</span>);</span><br><span class="line">    out.print(<span class="string">&quot;值：&quot;</span> +  URLDecoder.decode(cookie.getValue(), <span class="string">&quot;utf-8&quot;</span>) +<span class="string">&quot; &lt;br/&gt;&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="通过-Servlet-删除-Cookie"><a href="#通过-Servlet-删除-Cookie" class="headerlink" title="通过 Servlet 删除 Cookie"></a>通过 Servlet 删除 Cookie</h3><ol>
<li>读取一个现有的 cookie，并把它存储在 Cookie 对象中。</li>
<li>使用 setMaxAge() 方法设置 cookie 的年龄为零，来删除现有的 cookie。</li>
<li>把这个 cookie 添加到响应头。</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">Cookie</span> <span class="variable">cookie</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line">Cookie[] cookies = <span class="literal">null</span>;</span><br><span class="line"><span class="type">PrintWriter</span> <span class="variable">out</span> <span class="operator">=</span> response.getWriter();</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; cookies.length; i++)&#123;</span><br><span class="line">    cookie = cookies[i];</span><br><span class="line">    <span class="comment">//删除现有的名为 &quot;url&quot; 的 cookie，当下次运行 ReadCookies 的 Servlet 时，它会返回 url 为 null。</span></span><br><span class="line">    <span class="keyword">if</span>((cookie.getName( )).compareTo(<span class="string">&quot;url&quot;</span>) == <span class="number">0</span> )&#123;</span><br><span class="line">        cookie.setMaxAge(<span class="number">0</span>);</span><br><span class="line">        response.addCookie(cookie);</span><br><span class="line">        out.print(<span class="string">&quot;已删除的 cookie：&quot;</span> + </span><br><span class="line">                    cookie.getName( ) + <span class="string">&quot;&lt;br/&gt;&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Cookie-API"><a href="#Cookie-API" class="headerlink" title="Cookie API"></a>Cookie API</h3><table>
<thead>
<tr>
<th>column0</th>
<th>column1</th>
</tr>
</thead>
<tbody><tr>
<td>序号</td>
<td>方法 &amp; 描述</td>
</tr>
<tr>
<td>1</td>
<td>public void setDomain(String pattern)。该方法设置 cookie 适用的域，例如 nanaiii.com。</td>
</tr>
<tr>
<td>2</td>
<td>public String getDomain()。该方法获取 cookie 适用的域，例如 nanaiii.com。</td>
</tr>
<tr>
<td>3</td>
<td>public void setMaxAge(int expiry)。该方法设置 cookie 过期的时间（以秒为单位）。如果不这样设置，cookie 只会在当前 session 会话中持续有效。</td>
</tr>
<tr>
<td>4</td>
<td>public int getMaxAge()。该方法返回 cookie 的最大生存周期（以秒为单位），默认情况下，-1 表示 cookie 将持续下去，直到浏览器关闭。</td>
</tr>
<tr>
<td>5</td>
<td>public String getName()。该方法返回 cookie 的名称。名称在创建后不能改变。</td>
</tr>
<tr>
<td>6</td>
<td>public void setValue(String newValue)。该方法设置与 cookie 关联的值。</td>
</tr>
<tr>
<td>7</td>
<td>public String getValue()。该方法获取与 cookie 关联的值。</td>
</tr>
<tr>
<td>8</td>
<td>public void setPath(String uri)。该方法设置 cookie 适用的路径。如果您不指定路径，与当前页面相同目录下的（包括子目录下的）所有 URL 都会返回 cookie。</td>
</tr>
<tr>
<td>9</td>
<td>public String getPath()。该方法获取 cookie 适用的路径。</td>
</tr>
<tr>
<td>10</td>
<td>public void setSecure(boolean flag)。该方法设置布尔值，表示 cookie 是否应该只在加密的（即 SSL）连接上发送。</td>
</tr>
<tr>
<td>11</td>
<td>public void setComment(String purpose)。设置cookie的注释。该注释在浏览器向用户呈现 cookie 时非常有用。</td>
</tr>
<tr>
<td>12</td>
<td>public String getComment()。获取 cookie 的注释，如果 cookie 没有注释则返回 null。</td>
</tr>
</tbody></table>
<h2 id="Servlet-Session-跟踪"><a href="#Servlet-Session-跟踪" class="headerlink" title="Servlet Session 跟踪"></a>Servlet Session 跟踪</h2><p>Servlet 提供了 HttpSession 接口，该接口提供了一种跨多个页面请求或访问网站时识别用户以及存储有关用户信息的方式。</p>
<p>Servlet 容器使用这个接口来创建一个 HTTP 客户端和 HTTP 服务器之间的 session 会话。会话持续一个指定的时间段，跨多个连接或页面请求。</p>
<p>你需要在向客户端发送任何文档内容之前调用 request.getSession()来获取 HttpSession 对象。如下所示： <code>HttpSession session = request.getSession();</code></p>
<p>下面展示如何使用 HttpSession 对象获取 session 会话创建时间和最后访问时间。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 如果不存在 session 会话，则创建一个 session 对象</span></span><br><span class="line"><span class="type">HttpSession</span> <span class="variable">session</span> <span class="operator">=</span> request.getSession(<span class="literal">true</span>);</span><br><span class="line"><span class="comment">// 获取 session 创建时间</span></span><br><span class="line"><span class="type">Date</span> <span class="variable">createTime</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Date</span>(session.getCreationTime());</span><br><span class="line"><span class="comment">// 获取该网页的最后一次访问时间</span></span><br><span class="line"><span class="type">Date</span> <span class="variable">lastAccessTime</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Date</span>(session.getLastAccessedTime());</span><br></pre></td></tr></table></figure>

<h3 id="删除-Session-会话数据"><a href="#删除-Session-会话数据" class="headerlink" title="删除 Session 会话数据"></a>删除 Session 会话数据</h3><p>当您完成了一个用户的 session 会话数据，您有以下几种选择：</p>
<ul>
<li><p>移除一个特定的属性：您可以调用 public void removeAttribute(String name) 方法来删除与特定的键相关联的值。</p>
</li>
<li><p>删除整个 session 会话：您可以调用 public void invalidate() 方法来丢弃整个 session 会话。</p>
</li>
<li><p>设置 session 会话过期时间：您可以调用 public void setMaxInactiveInterval(int interval) 方法来单独设置 session 会话超时。</p>
</li>
<li><p>注销用户：如果使用的是支持 servlet 2.4 的服务器，您可以调用 logout 来注销 Web 服务器的客户端，并把属于所有用户的所有 session 会话设置为无效。</p>
</li>
<li><p>web.xml 配置：如果您使用的是 Tomcat，除了上述方法，您还可以在 web.xml 文件中配置 session 会话超时，如下所示：</p>
</li>
</ul>
<figure class="highlight jsp"><table><tr><td class="code"><pre><span class="line">&lt;session-config&gt;</span><br><span class="line">  &lt;session-timeout&gt;<span class="number">15</span>&lt;/session-timeout&gt;</span><br><span class="line">  &lt;!-- 超时时间是以分钟为单位，这将覆盖 Tomcat 中默认的 <span class="number">30</span> 分钟超时时间。 --&gt;</span><br><span class="line">&lt;/session-config&gt;</span><br></pre></td></tr></table></figure>

<h3 id="HttpSession-API"><a href="#HttpSession-API" class="headerlink" title="HttpSession API"></a>HttpSession API</h3><table>
<thead>
<tr>
<th>column0</th>
<th>column1</th>
</tr>
</thead>
<tbody><tr>
<td>序号</td>
<td>方法 &amp; 描述</td>
</tr>
<tr>
<td>1</td>
<td>public Object getAttribute(String name)。该方法返回在该 session 会话中具有指定名称的对象，如果没有指定名称的对象，则返回 null。</td>
</tr>
<tr>
<td>2</td>
<td>public Enumeration getAttributeNames()。该方法返回 String 对象的枚举，String 对象包含所有绑定到该 session 会话的对象的名称。</td>
</tr>
<tr>
<td>3</td>
<td>public long getCreationTime()。该方法返回该 session 会话被创建的时间，自格林尼治标准时间 1970 年 1 月 1 日午夜算起，以毫秒为单位。</td>
</tr>
<tr>
<td>4</td>
<td>public String getId()。该方法返回一个包含分配给该 session 会话的唯一标识符的字符串。</td>
</tr>
<tr>
<td>5</td>
<td>public long getLastAccessedTime()。该方法返回客户端最后一次发送与该 session 会话相关的请求的时间自格林尼治标准时间 1970 年 1 月 1 日午夜算起，以毫秒为单位。</td>
</tr>
<tr>
<td>6</td>
<td>public int getMaxInactiveInterval()。该方法返回 Servlet 容器在客户端访问时保持 session 会话打开的最大时间间隔，以秒为单位。</td>
</tr>
<tr>
<td>7</td>
<td>public void invalidate()。该方法指示该 session 会话无效，并解除绑定到它上面的任何对象。</td>
</tr>
<tr>
<td>8</td>
<td>public boolean isNew()。如果客户端还不知道该 session 会话，或者如果客户选择不参入该 session 会话，则该方法返回 true。</td>
</tr>
<tr>
<td>9</td>
<td>public void removeAttribute(String name)。该方法将从该 session 会话移除指定名称的对象。</td>
</tr>
<tr>
<td>10</td>
<td>public void setAttribute(String name, Object value)。该方法使用指定的名称绑定一个对象到该 session 会话。</td>
</tr>
<tr>
<td>11</td>
<td>public void setMaxInactiveInterval(int interval)。该方法在 Servlet 容器指示该 session 会话无效之前，指定客户端请求之间的时间，以秒为单位。</td>
</tr>
</tbody></table>
]]></content>
      <tags>
        <tag>计算机</tag>
      </tags>
  </entry>
  <entry>
    <title>TF学习（一）</title>
    <url>/2020/06/30/Learning-tensorflow-01/</url>
    <content><![CDATA[<blockquote>
<p><strong>TensorFlow™</strong>（以下简称TF）是一个采用数据流图（data flow graphs），用于数值计算的开源软件库。节点（Nodes）在图中表示数学操作，图中的线（edges）则表示在节点间相互联系的多维数据数组，即张量（tensor）。它灵活的架构让你可以在多种平台上展开计算，例如台式计算机中的一个或多个CPU（或GPU），服务器，移动设备等等。</p>
</blockquote>
<p><em>（本博客主要介绍TF1.5. 而TF2.x与TF1.x间并不兼容）</em></p>
<p>Tensorflow官网：<a href="https://tensorflow.google.cn/">https://tensorflow.google.cn/</a></p>
<p>Tensorflow中文社区：<a href="http://tensorfly.cn/">http://tensorfly.cn/</a></p>
<h1 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h1><span id="more"></span>

<h2 id="数据流图（Data-Flow-Graph）"><a href="#数据流图（Data-Flow-Graph）" class="headerlink" title="数据流图（Data Flow Graph）"></a>数据流图（Data Flow Graph）</h2><p>数据流图用“结点”（nodes）和“线”(edges)的有向图来描述数学计算。“节点” 一般用来表示施加的数学操作，但也可以表示数据输入（feed in）的起点&#x2F;输出（push out）的终点，或者是读取&#x2F;写入持久变量（persistent<br>variable）的终点。“线”表示“节点”之间的输入&#x2F;输出关系。这些数据“线”可以输运“size可动态调整”的多维数据数组，即“张量”（tensor）。张量从图中流过的直观图像是这个工具取名为“TF”的原因。一旦输入端的所有张量准备好，节点将被分配到各种计算设备完成异步并行地执行运算。</p>
<p><img src="/images/TF_01_01.png" alt="avatar"></p>
<h3 id="设计理念"><a href="#设计理念" class="headerlink" title="设计理念"></a>设计理念</h3><ol>
<li><p>将图的定义与图的运行完全分开，采用符号式编程。符号式计算一般是先定义各种变量，然后建立一个数据流图，在数据流图中规定各个变量之间的计算关系，最后需要对数据流图进行编译，但此时的数据流图还是一个空壳儿，里面没有任何实际数据，只有把需要运算的输入放进去后，才能在整个模型中形成数据流，从而形成输出值。</p>
</li>
<li><p>将涉及的运算都存放在图中，而图的运行只发生在会话中（session）中。当开启会话后，就可以用数据去填充节点，进行运算；关闭会话后，就不能进行计算了。因此，会话提供了操作运行和Tensor求值的环境。</p>
</li>
</ol>
<h3 id="边"><a href="#边" class="headerlink" title="边"></a>边</h3><p>TensorFlow的边有两种连接关系：数据依赖和控制依赖。</p>
<p>实线边表示数据依赖，代表数据，即张量。任意维度的数据统称为张量。在机器学习算法中，张量在数据流图中从前往后流动一遍就完成了一次前向传播 （forword propagation），而残差从后向前流动一遍就完成了一次反向传播<br>（backword propagation）。</p>
<p>还有一种特殊边，一般画为虚线边，称为控制依赖 （control dependency），可以用于控制操作的运行，这被用来确保happens-before关系，这类边上没有数据流过，但源节点必须在目的节点开始执行前完成执行。</p>
<h3 id="节点"><a href="#节点" class="headerlink" title="节点"></a>节点</h3><p>图中的节点又称为算子它代表一个操作（operation，OP），一般用来表示施加的数学运算，也可以表示数据输入 （feed in）的起点以及输出 （push out）的终点，或者是读取&#x2F;写入持久变量 （persistent<br>variable）的终点。算子支持张量的各种数据属性，并且需要在建立图的时候确定下来。</p>
<h3 id="图"><a href="#图" class="headerlink" title="图"></a>图</h3><p>构建图的第一步是创建各个节点。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment"># 创建一个常量运算操作，产生一个 1×2 矩阵</span></span><br><span class="line">matrix1 = tf.constant([[<span class="number">3.</span>, <span class="number">3.</span>]])</span><br><span class="line"><span class="comment"># 创建另外一个常量运算操作，产生一个 2×1 矩阵</span></span><br><span class="line">matrix2 = tf.constant([[<span class="number">2.</span>],[<span class="number">2.</span>]])</span><br><span class="line"><span class="comment"># 创建一个矩阵乘法运算 ，把matrix1和matrix2作为输入</span></span><br><span class="line"><span class="comment"># 返回值product代表矩阵乘法的结果</span></span><br><span class="line">product = tf.matmul(matrix1, matrix2)</span><br></pre></td></tr></table></figure>

<h3 id="会话"><a href="#会话" class="headerlink" title="会话"></a>会话</h3><p>启动图的第一步是创建一个Session对象。会话（session）提供在图中执行操作的一些方法。一般的模式是，建立会话，此时会生成一张空图，在会话中添加节点和边，形成一张图，然后执行。</p>
<p>要创建一张图并运行操作的类，在Python的API中使用tf. Session，在C++ 的API中使用tensorflow:: Session。示例如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">result = sess.run([product])</span><br><span class="line"><span class="built_in">print</span> (result)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>在调用Session对象的run()方法来执行图时，传入一些Tensor，这个过程叫填充 （feed）；返回的结果类型根据输入的类型而定，这个过程叫取回 （fetch）。</p>
<h3 id="设备"><a href="#设备" class="headerlink" title="设备"></a>设备</h3><p>设备 （device）是指一块可以用来运算并且拥有自己的地址空间的硬件，如GPU和CPU。TensorFlow为了实现分布式执行操作，充分利用计算资源，可以明确指定操作在哪个设备上执行。具体如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line"><span class="comment"># 指定在第二个gpu上运行</span></span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">&quot;/gpu:1&quot;</span>):</span><br><span class="line">matrix1 = tf.constant([[<span class="number">3.</span>, <span class="number">3.</span>]])</span><br><span class="line">matrix2 = tf.constant([[<span class="number">2.</span>],[<span class="number">2.</span>]])</span><br><span class="line">product = tf.matmul(matrix1, matrix2)</span><br></pre></td></tr></table></figure>

<h3 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h3><p>变量 （variable）是一种特殊的数据，它在图中有固定的位置，不像普通张量那样可以流动。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建一个变量，初始化为标量0</span></span><br><span class="line">state = tf.Variable(<span class="number">0</span>, name=<span class="string">&quot;counter&quot;</span>)</span><br><span class="line"><span class="comment">#创建一个常量张量</span></span><br><span class="line">input1 = tf.constant(<span class="number">3.0</span>)</span><br></pre></td></tr></table></figure>

<p>TensorFlow 还提供了填充机制，可以在构建图时使用tf.placeholder()临时替代任意操作的 张量，在调用Session对象的run()方法去执行图时，使用填充数据作为调用的参数，调用结束 后，填充数据就消失。代码示例如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">input1 = tf.placeholder(tf.float32)</span><br><span class="line">input2 = tf.placeholder(tf.float32)</span><br><span class="line">output = tf.multiply(input1, input2)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line"><span class="built_in">print</span> sess.run([output], feed_dict=&#123;input1:[<span class="number">7.</span>], input2:[<span class="number">2.</span>]&#125;)</span><br><span class="line"><span class="comment"># 输出 [array([ 14.], dtype=float32)]</span></span><br></pre></td></tr></table></figure>

<h3 id="内核"><a href="#内核" class="headerlink" title="内核"></a>内核</h3><p>我们知道操作 （operation）是对抽象操作（如matmul或者add）的一个统称，而内核<br>（kernel）则是能够运行在特定设备（如CPU、GPU）上的一种对操作的实现。因此，同一个操作可能会对应多个内核。当自定义一个操作时，需要把新操作和内核通过注册的方式添加到系统中。</p>
<h2 id="常用API"><a href="#常用API" class="headerlink" title="常用API"></a>常用API</h2><h3 id="图（tf-Graph）"><a href="#图（tf-Graph）" class="headerlink" title="图（tf. Graph）"></a>图（tf. Graph）</h3><p><em>tf. Graph类中包含一系列表示计算的操作对象（tf. Operation），以及在操作之间流动的数据——张量对象（tf. Tensor）</em></p>
<p><img src="/images/TF_01_05.png" alt="avatar"></p>
<h3 id="操作对象-x2F-节点（tf-Operation）"><a href="#操作对象-x2F-节点（tf-Operation）" class="headerlink" title="操作对象&#x2F;节点（tf. Operation）"></a>操作对象&#x2F;节点（tf. Operation）</h3><p><em>用于计算张量数据，由节点构造器（如tf.matmul()或者Graph.create_op()）产生.</em></p>
<p><img src="/images/TF_01_04.png" alt="avatar"></p>
<h3 id="张量对象（tf-Tensor）"><a href="#张量对象（tf-Tensor）" class="headerlink" title="张量对象（tf. Tensor）"></a>张量对象（tf. Tensor）</h3><p><em>tf. Tensor类是操作输出的符号句柄，它不包含操作输出的值，而是提供了一种在tf.<br>Session中计算这些值的方法。这样就可以在操作之间构建一个数据流连接，使TensorFlow能够执行一个表示大量多步计算的图形。与张量相关的API均位于tf. Tensor类中</em></p>
<p><img src="/images/TF_01_03.png" alt="avatar"></p>
<h3 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h3><p><em>可视化时，需要在程序中给必要的节点添加摘要 （summary），摘要会收集该节点的数据，并标记上第几步、时间戳等标识，写入事件文件 （event file）中。tf.summary.<br>FileWriter类用于在目录中创建事件文件，并且向文件中添加摘要和事件，用来在TensorBoard中展示。</em></p>
<p><img src="/images/TF_01_02.png" alt="avatar"></p>
<h3 id="变量作用域"><a href="#变量作用域" class="headerlink" title="变量作用域"></a>变量作用域</h3><p>在TensorFlow中有两个作用域 （scope），一个是name_scope，另一个是variable_scope。</p>
<p>variable_scope主要是给variable_name加前缀，也可以给op_name加前缀；name_scope是给op_name加前缀。</p>
<h4 id="variable-scope"><a href="#variable-scope" class="headerlink" title="variable_scope"></a>variable_scope</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">v = tf.get_variable(name, shape, dtype, initializer) <span class="comment"># 通过所给的名字创建或是返回一个变量</span></span><br><span class="line">tf.variable_scope(&lt;scope_name&gt;) <span class="comment"># 为变量指定命名空间</span></span><br></pre></td></tr></table></figure>

<p>当tf.get_variable_scope().reuse &#x3D;&#x3D; False时，variable_scope作用域只能用来创建新变量</p>
<p>当tf.get_variable_scope().reuse &#x3D;&#x3D; True时，作用域可以共享变量</p>
<h5 id="1-获取变量作用域"><a href="#1-获取变量作用域" class="headerlink" title="1. 获取变量作用域"></a>1. 获取变量作用域</h5><p>可以直接通过tf.variable_scope()来获取变量作用域</p>
<p>如果在开启的一个变量作用域里使用之前预先定义的一个作用域，则会跳过当前变量的作用域，保持预先存在的作用域不变。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;foo&quot;</span>) <span class="keyword">as</span> foo_scope:</span><br><span class="line">    <span class="keyword">assert</span> foo_scope.name == <span class="string">&quot;foo&quot;</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;bar&quot;</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;baz&quot;</span>) <span class="keyword">as</span> other_scope:</span><br><span class="line">        <span class="keyword">assert</span> other_scope.name == <span class="string">&quot;bar/baz&quot;</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(foo_scope) <span class="keyword">as</span> foo_scope2:</span><br><span class="line">            <span class="keyword">assert</span> foo_scope2.name == <span class="string">&quot;foo&quot;</span> <span class="comment"># 保持不变</span></span><br></pre></td></tr></table></figure>

<h5 id="2-变量作用域的初始化"><a href="#2-变量作用域的初始化" class="headerlink" title="2. 变量作用域的初始化"></a>2. 变量作用域的初始化</h5><p>变量作用域可以默认携带一个初始化器，在这个作用域中的子作用域或变量都可以继承或者重写父作用域初始化器中的值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;foo&quot;</span>, initializer=tf.constant_initializer(<span class="number">0.4</span>)):</span><br><span class="line">    v = tf.get_variable(<span class="string">&quot;v&quot;</span>, [<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">assert</span> v.<span class="built_in">eval</span>() == <span class="number">0.4</span> <span class="comment"># 被作用域初始化</span></span><br><span class="line">    w = tf.get_variable(<span class="string">&quot;w&quot;</span>, [<span class="number">1</span>], initializer=tf.constant_initializer(<span class="number">0.3</span>)):</span><br><span class="line">    <span class="keyword">assert</span> w.<span class="built_in">eval</span>() == <span class="number">0.3</span> <span class="comment"># 重写初始化器的值</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;bar&quot;</span>):</span><br><span class="line">        v = tf.get_variable(<span class="string">&quot;v&quot;</span>, [<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">assert</span> v.<span class="built_in">eval</span>() == <span class="number">0.4</span> <span class="comment"># 继承默认的初始化器</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;baz&quot;</span>, initializer=tf.constant_initializer(<span class="number">0.2</span>)):</span><br><span class="line">        v = tf.get_variable(<span class="string">&quot;v&quot;</span>, [<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">assert</span> v.<span class="built_in">eval</span>() == <span class="number">0.2</span> <span class="comment"># 重写父作用域的初始化器的值</span></span><br></pre></td></tr></table></figure>

<h5 id="op-name"><a href="#op-name" class="headerlink" title="op_name"></a>op_name</h5><p>那对于op_name, 在variable_scope作用域下的操作，也会被加上前缀：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;foo&quot;</span>):</span><br><span class="line">    x = <span class="number">1.0</span> + tf.get_variable(<span class="string">&quot;v&quot;</span>, [<span class="number">1</span>])</span><br><span class="line"><span class="keyword">assert</span> x.op.name == <span class="string">&quot;foo/add&quot;</span></span><br></pre></td></tr></table></figure>

<h4 id="name-scope"><a href="#name-scope" class="headerlink" title="name_scope"></a>name_scope</h4><p>ensorFlow中常常会有数以千计的节点，在可视化的过程中很难一下子展示出来，因此用name_scope为变量划分范围，在可视化中，这表示在计算图中的一个层级。name_scope会影响op_name，不会影响用get_variable()<br>创建的变量，而会影响通过Variable()创建的变量。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;foo&quot;</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&quot;bar&quot;</span>):</span><br><span class="line">        v = tf.get_variable(<span class="string">&quot;v&quot;</span>, [<span class="number">1</span>])</span><br><span class="line">        b = tf.Variable(tf.zeros([<span class="number">1</span>]), name=<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line">        x = <span class="number">1.0</span> + v</span><br><span class="line"><span class="keyword">assert</span> v.name == <span class="string">&quot;foo/v:0&quot;</span></span><br><span class="line"><span class="keyword">assert</span> b.name == <span class="string">&quot;foo/bar/b:0&quot;</span></span><br><span class="line"><span class="keyword">assert</span> x.op.name == <span class="string">&quot;foo/bar/add&quot;</span></span><br></pre></td></tr></table></figure>

<h2 id="批标准化（BN）"><a href="#批标准化（BN）" class="headerlink" title="批标准化（BN）"></a>批标准化（BN）</h2><blockquote>
<p><strong>ICS（Internal Covariate Shift）理论</strong>源域（source domain）和目标域 （target domain）的数据分布 是一致的。</p>
</blockquote>
<p>Covariate Shift是指训练集的样本数据和目标样本集分布不一致时，训练得到的模型无法很好地泛化<br>（generalization）。它是分布不一致假设之下的一个分支问题，也就是指源域和目标域的条件概率是一致的，但是其边缘概率不同。的确，对于神经网络的各层输出，在经过了层内操作后，各层输出分布就会与对应的输入信号分布不同，而且差异会随着网络深度增大而加大，但是每一层所指向的样本标记<br>（label）仍然是不变的。（常常会导致梯度弥散问题 （vanishing gradient problem）。使训练起来会越来越困难，收敛速度会很慢）</p>
<p>解决思路一般是根据训练样本和目标样本的比例对训练样本做一个矫正。因此，通过引入批标准化来规范化某些层或者所有层的输入，从而固定每层输入信号的均值与方差：<em>一般用在非线性映射（激活函数）之前，对x &#x3D;Wu +b<br>做规范化，使结果（输出信号各个维度）的均值为0，方差为1。让每一层的输入有一个稳定的分布会有利于网络的训练。</em></p>
<h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对每层的Wx_plus_b进行批标准化，这个步骤放在激活函数之前</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算Wx_plus_b的均值和方差，其中axes=[0]表示想要标准化的维度</span></span><br><span class="line">fc_mean, fc_var = tf.nn.moments(Wx_plus_b, axes=[<span class="number">0</span>], )</span><br><span class="line">scale = tf.Variable(tf.ones([out_size]))</span><br><span class="line">shift = tf.Variable(tf.zeros([out_size]))</span><br><span class="line">epsilon = <span class="number">0.001</span></span><br><span class="line">Wx_plus_b = tf.nn.batch_normalization(Wx_plus_b, fc_mean, fc_var, shift,</span><br><span class="line">scale, epsilon)</span><br><span class="line"><span class="comment"># 也就是在做：</span></span><br><span class="line"><span class="comment"># Wx_plus_b = (Wx_plus_b - fc_mean) / tf.sqrt(fc_var + 0.001)</span></span><br><span class="line"><span class="comment"># Wx_plus_b = Wx_plus_b * scale + shift</span></span><br></pre></td></tr></table></figure>

<h2 id="神经元函数及优化方法"><a href="#神经元函数及优化方法" class="headerlink" title="神经元函数及优化方法"></a>神经元函数及优化方法</h2><h3 id="激活函数（activation-function）"><a href="#激活函数（activation-function）" class="headerlink" title="激活函数（activation function）"></a>激活函数（activation function）</h3><p>激活函数运行时激活神经网络中某一部分神经元，将激活信息向后传入下一层的神经网络。</p>
<p>神经网络的数学基础是处处可微的，所以选取的激活函数要能保证数据输入与输出也是可微的。同时，激活函数不会更改输入数据的维度，也就是输入和输出的维度是相同的。TF中的激活函数如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.nn.relu()</span><br><span class="line">tf.nn.sigmoid()</span><br><span class="line">tf.nn.tanh()</span><br><span class="line">tf.nn.elu()</span><br><span class="line">tf.nn.bias_add()</span><br><span class="line">tf.nn.crelu()</span><br><span class="line">tf.nn.relu6()</span><br><span class="line">tf.nn.softplus()</span><br><span class="line">tf.nn.softsign()</span><br><span class="line">tf.nn.dropout() <span class="comment"># 防止过拟合，用来舍弃某些神经元</span></span><br></pre></td></tr></table></figure>

<h4 id="sigmoid函数"><a href="#sigmoid函数" class="headerlink" title="sigmoid函数"></a>sigmoid函数</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">1.0</span>, <span class="number">2.0</span>]])</span><br><span class="line">sess = tf.Session()</span><br><span class="line"><span class="built_in">print</span> sess.run(tf.sigmoid(a))</span><br></pre></td></tr></table></figure>

<p><img src="/images/TF_01_06.png" alt="avatar"></p>
<p><em>sigmoid函数的优点在于，它的输出映射在(0, 1)内，单调连续，非常适合用作输出层，并且求导比较容易。但是，它也有缺点，因为软饱和性，一旦输入落入饱和区，f ‘ (x )就会变得接近于0，很容易产生梯度消失 。</em></p>
<h4 id="tanh函数"><a href="#tanh函数" class="headerlink" title="tanh函数"></a>tanh函数</h4><p><img src="/images/TF_01_07.png" alt="avatar"></p>
<p><em>tanh函数也具有软饱和性。因为它的输出以0为中心，收敛速度比sigmoid要快。但是仍无法解决梯度消失的问题。</em></p>
<h4 id="relu函数"><a href="#relu函数" class="headerlink" title="relu函数"></a>relu函数</h4><p>relu：f (x )&#x3D;max(x , 0)</p>
<p>softplus：f (x )&#x3D;log(1+exp(x ))</p>
<p><img src="/images/TF_01_08.png" alt="avatar"></p>
<p><em>relu在x &lt; 0时硬饱和。由于x&gt;0时导数为1，所以，relu能够在x &gt;<br>0时保持梯度不衰减，从而缓解梯度消失问题，还能够更快地收敛，并提供了神经网络的稀疏表达能力。但是，随着训练的进行，部分输入会落到硬饱和区，导致对应的权重无法更新，称为“神经元死亡”。</em></p>
<h4 id="dropout函数"><a href="#dropout函数" class="headerlink" title="dropout函数"></a>dropout函数</h4><p><em>一个神经元将以概率keep_prob决定是否被抑制。如果被抑制，该神经元的输出就为0；如果不被抑制，那么该神经元的输出值将被放大到原来的1&#x2F;keep_prob倍。</em></p>
<p>在默认情况下，每个神经元是否被抑制是相互独立的。但是否被抑制也可以通过noise_shape来调节。</p>
<p>当noise_shape[i] &#x3D;&#x3D; shape(x)[i]时，x中的元素是相互独立的。如果shape(x)&#x3D; [k, l, m, n]，x中的维度的顺序分别为批、行、列和通道，如果noise_shape &#x3D; [k, 1, 1, n]<br>，那么每个批和通道都是相互独立的，但是每行和每列的数据都是关联的，也就是说，要不都为0，要不都还是原来的值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = tf.constant([[-<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>]])</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    b = tf.nn.dropout(a, <span class="number">0.5</span>, noise_shape = [<span class="number">1</span>,<span class="number">4</span>])</span><br><span class="line">    <span class="built_in">print</span> (sess.run(b))</span><br><span class="line">    b = tf.nn.dropout(a, <span class="number">0.5</span>, noise_shape = [<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line">    <span class="built_in">print</span> (sess.run(b))</span><br></pre></td></tr></table></figure>

<h4 id="激活函数的选择"><a href="#激活函数的选择" class="headerlink" title="激活函数的选择"></a>激活函数的选择</h4><p>当输入数据特征相差明显时，用tanh的效果会很好，且在循环过程中会不断扩大特征效果并显示出来。</p>
<p>当特征相差不明显时，sigmoid效果比较好。</p>
<p><em>同时，用sigmoid和tanh作为激活函数时，需要对输入进行规范化，否则激活后的值全部都进入平坦区，隐层的输出会全部趋同，丧失原有的特征表达。</em></p>
<p>而relu会好很多，有时可以不需要输入规范化来避免上述情况。</p>
<p>因此，现在大部分的卷积神经网络都采用relu作为激活函数。大概有85%～90%的神经网络会采用ReLU，10%～15%的神经网络会采用tanh，尤其用在自然语言处理上。</p>
<p><img src="/images/TF_01_09.png" alt="avarat"></p>
<h3 id="卷积函数"><a href="#卷积函数" class="headerlink" title="卷积函数"></a>卷积函数</h3><p>卷积函数是构建神经网络的重要支架，是在一批图像上扫描的二维过滤器。</p>
<h4 id="tf-nn-convolution-input-filter-padding-strides-x3D-None-dilation-rate-x3D-None-name-x3D-None-data-format-x3D-None"><a href="#tf-nn-convolution-input-filter-padding-strides-x3D-None-dilation-rate-x3D-None-name-x3D-None-data-format-x3D-None" class="headerlink" title="tf.nn.convolution(input, filter, padding, strides&#x3D;None, dilation_rate&#x3D;None, name&#x3D;None, data_format &#x3D;None)"></a>tf.nn.convolution(input, filter, padding, strides&#x3D;None, dilation_rate&#x3D;None, name&#x3D;None, data_format &#x3D;None)</h4><p><em>计算N维卷积的和</em></p>
<p>输入：</p>
<ul>
<li>input：一个Tensor。数据类型必须是float32或者float64</li>
<li>filter：一个Tensor。数据类型必须是input相同</li>
<li>strides： strides: Optional. Sequence of N ints &gt;&#x3D; 1. Specifies the output stride. Defaults to [1]*N. If any value of<br>strides is &gt; 1, then all values of dilation_rate must be 1.</li>
<li>padding：一个字符串，取值为SAME或者VALID；padding&#x3D;’SAME’：仅适用于全尺寸操作，即输入数据维度和输出数据维度相同；padding&#x3D;’VALID：适用于部分窗口，即输入数据维度和输出数据维度不同</li>
<li>name：（可选）为这个操作取一个名字</li>
</ul>
<p>输出：一个Tensor，数据类型是input相同</p>
<h4 id="tf-nn-conv2d-input-filter-strides-padding-use-cudnn-on-gpu-x3D-None-data-format-x3D-None-name-x3D-None"><a href="#tf-nn-conv2d-input-filter-strides-padding-use-cudnn-on-gpu-x3D-None-data-format-x3D-None-name-x3D-None" class="headerlink" title="tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu&#x3D;None, data_format&#x3D;None, name&#x3D;None)"></a>tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu&#x3D;None, data_format&#x3D;None, name&#x3D;None)</h4><p><em>对一个四维的输入数据input和四维的卷积核filter进行操作，然后对输入数据进行一个二维的卷积操作，最后得到卷积之后的结果</em></p>
<p>输入：</p>
<ul>
<li>strides：一个长度是4的一维整数类型数组，每一维度对应的是input中每一维的对应移动步数，比如，strides[1]对应input[1]的移动步数</li>
<li>use_cudnn_on_gpu：一个可选布尔值，默认情况下是True</li>
</ul>
<p>输出：一个Tensor，数据类型是input相同</p>
<h4 id="tf-nn-depthwise-conv2d-input-filter-strides-padding-rate-x3D-None-name-x3D-None-data-format-x3D-None"><a href="#tf-nn-depthwise-conv2d-input-filter-strides-padding-rate-x3D-None-name-x3D-None-data-format-x3D-None" class="headerlink" title="tf.nn.depthwise_conv2d (input, filter, strides, padding, rate&#x3D;None, name&#x3D;None, data_format&#x3D;None)"></a>tf.nn.depthwise_conv2d (input, filter, strides, padding, rate&#x3D;None, name&#x3D;None, data_format&#x3D;None)</h4><p>输入张量的数据维度是[batch, in_height, in_width, in_channels]</p>
<p>卷积核的维度是[filter_height, filter_width, in_channels, channel_multiplier]</p>
<p>在通道in_channels上面的卷积深度是1</p>
<p><em>depthwise_conv2d函数将不同的卷积核独立地应用在in_channels的每个通道上（从通道1到通道channel_multiplier），然后把所以的结果进行汇总。最后输出通道的总数是in_channels *<br>channel_multiplier。</em></p>
<h4 id="tf-nn-separable-conv2d-input-depthwise-filter-pointwise-filter-strides-padding-rate-x3D-None-name-x3D-None-data-format-x3D-None"><a href="#tf-nn-separable-conv2d-input-depthwise-filter-pointwise-filter-strides-padding-rate-x3D-None-name-x3D-None-data-format-x3D-None" class="headerlink" title="tf.nn.separable_conv2d (input, depthwise_filter, pointwise_filter, strides, padding, rate&#x3D;None, name&#x3D;None, data_format&#x3D;None)"></a>tf.nn.separable_conv2d (input, depthwise_filter, pointwise_filter, strides, padding, rate&#x3D;None, name&#x3D;None, data_format&#x3D;None)</h4><p><em>应用一个二维的卷积核，在每个通道上，以深度channel_multiplier进行卷积。</em></p>
<p>输入：</p>
<ul>
<li>depthwise_filter：一个张量。数据维度是四维[filter_height, filter_width, in_channels, channel_multiplier]。其中，in_channels的卷积深度是1</li>
<li>pointwise_filter：一个张量。数据维度是四维[1, 1, channel_multiplier * in_channels, out_channels]<br>。其中，pointwise_filter是在depthwise_filter卷积之后的混合卷积</li>
</ul>
<h4 id="tf-nn-atrous-conv2d-value-filters-rate-padding-name-x3D-None"><a href="#tf-nn-atrous-conv2d-value-filters-rate-padding-name-x3D-None" class="headerlink" title="tf.nn.atrous_conv2d(value, filters, rate, padding, name&#x3D;None)"></a>tf.nn.atrous_conv2d(value, filters, rate, padding, name&#x3D;None)</h4><p><em>计算Atrous卷积，又称孔卷积或者扩张卷积</em></p>
<p>输入：</p>
<ul>
<li>rate：正整数int32。我们跨height和跨width维度采样输入值的跨度。等效地，我们通过在height和 width维度上插入零来对滤波器值进行升采样的速率。</li>
</ul>
<h4 id="tf-nn-conv2d-transpose-value-filter-output-shape-strides-padding-x3D-’SAME’-data-format-x3D-’NHWC’-name-x3D-None"><a href="#tf-nn-conv2d-transpose-value-filter-output-shape-strides-padding-x3D-’SAME’-data-format-x3D-’NHWC’-name-x3D-None" class="headerlink" title="tf.nn.conv2d_transpose(value, filter, output_shape, strides, padding&#x3D;’SAME’, data_format&#x3D;’NHWC’, name&#x3D;None)"></a>tf.nn.conv2d_transpose(value, filter, output_shape, strides, padding&#x3D;’SAME’, data_format&#x3D;’NHWC’, name&#x3D;None)</h4><p><em>在解卷积网络（deconvolutional network）中有时称为“反卷积”，但实际上是conv2d的转置，而不是实际的反卷积。</em></p>
<p>输入：</p>
<ul>
<li>output_shape：一维的张量，表示反卷积运算后输出的形状</li>
</ul>
<p>输出：和value一样维度的Tensor</p>
<h4 id="）tf-nn-conv1d-value-filters-stride-padding-use-cudnn-on-gpu-x3D-None-data-format-x3D-None-name-x3D-None"><a href="#）tf-nn-conv1d-value-filters-stride-padding-use-cudnn-on-gpu-x3D-None-data-format-x3D-None-name-x3D-None" class="headerlink" title="）tf.nn.conv1d(value, filters, stride, padding, use_cudnn_on_gpu&#x3D;None, data_format&#x3D;None, name&#x3D;None)"></a>）tf.nn.conv1d(value, filters, stride, padding, use_cudnn_on_gpu&#x3D;None, data_format&#x3D;None, name&#x3D;None)</h4><p><em>计算给定三维的输入和过滤器的情况下的一维卷积。</em></p>
<p>输入：</p>
<ul>
<li>value：[batch, in_width, in_channels]。</li>
<li>filter: 卷积核的维度也是三维，少了一维filter_height，如 [filter_width, in_channels, out_channels]。</li>
<li>stride: 正整数，代表卷积核向右移动每一步的长度。</li>
</ul>
<h4 id="tf-nn-conv3d-input-filter-strides-padding-name-x3D-None"><a href="#tf-nn-conv3d-input-filter-strides-padding-name-x3D-None" class="headerlink" title="tf.nn.conv3d(input, filter, strides, padding, name&#x3D;None)"></a>tf.nn.conv3d(input, filter, strides, padding, name&#x3D;None)</h4><p><em>计算给定五维的输入和过滤器的情况下的三维卷积</em></p>
<p>输入：（与二维卷积相对比）</p>
<ul>
<li>input的shape中多了一维in_depth，形状为Shape[batch, in_depth, in_height, in_width, in_channels]；</li>
<li>filter的shape中多了一维filter_depth，由filter_depth, filter_height, filter_width构成了卷积核的大小；</li>
<li>strides中多了一维，变为[strides_batch, strides_depth, strides_height, strides_width, strides_channel]，必须保证strides[0] &#x3D;<br>strides[4] &#x3D; 1</li>
</ul>
<h4 id="tf-nn-conv3d-transpose-value-filter-output-shape-strides-padding-x3D-’SAME’-name-x3D-None"><a href="#tf-nn-conv3d-transpose-value-filter-output-shape-strides-padding-x3D-’SAME’-name-x3D-None" class="headerlink" title="tf.nn.conv3d_transpose(value, filter, output_shape, strides, padding&#x3D;’SAME’, name&#x3D;None)"></a>tf.nn.conv3d_transpose(value, filter, output_shape, strides, padding&#x3D;’SAME’, name&#x3D;None)</h4><p><em>与二维反卷积类似</em></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>input_data = tf.Variable( np.random.rand(<span class="number">10</span>,<span class="number">9</span>,<span class="number">9</span>,<span class="number">3</span>), dtype = np.float32 )</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>filter_data = tf.Variable( np.random.rand(<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>), dtype = np.float32)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = tf.nn.convolution(input_data,filter_data,padding=<span class="string">&#x27;SAME&#x27;</span>,strides=[<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(y)</span><br><span class="line">Tensor(<span class="string">&quot;convolution_4:0&quot;</span>, shape=(<span class="number">10</span>, <span class="number">9</span>, <span class="number">9</span>, <span class="number">2</span>), dtype=float32)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = tf.nn.conv2d(input_data,filter_data,padding=<span class="string">&#x27;SAME&#x27;</span>,strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(y)</span><br><span class="line">Tensor(<span class="string">&quot;Conv2D_1:0&quot;</span>, shape=(<span class="number">10</span>, <span class="number">9</span>, <span class="number">9</span>, <span class="number">2</span>), dtype=float32)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = tf.nn.depthwise_conv2d(input_data,filter_data,padding=<span class="string">&#x27;SAME&#x27;</span>,strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(y)</span><br><span class="line">Tensor(<span class="string">&quot;depthwise_1:0&quot;</span>, shape=(<span class="number">10</span>, <span class="number">9</span>, <span class="number">9</span>, <span class="number">6</span>), dtype=float32)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>input_data = tf.Variable( np.random.rand(<span class="number">10</span>, <span class="number">9</span>, <span class="number">9</span>, <span class="number">3</span>), dtype = np.float32 )</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>depthwise_filter = tf.Variable( np.random.rand(<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>), dtype = np.float32)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pointwise_filter = tf.Variable( np.random.rand(<span class="number">1</span>, <span class="number">1</span>, <span class="number">15</span>, <span class="number">20</span>), dtype = np.float32)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># out_channels &gt;= channel_multiplier * in_channels</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = tf.nn.separable_conv2d(input_data, depthwise_filter, pointwise_filter,strides = [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding = <span class="string">&#x27;SAME&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(y)</span><br><span class="line">Tensor(<span class="string">&quot;separable_conv2d_1:0&quot;</span>, shape=(<span class="number">10</span>, <span class="number">9</span>, <span class="number">9</span>, <span class="number">20</span>), dtype=float32)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>input_data = tf.Variable( np.random.rand(<span class="number">1</span>,<span class="number">5</span>,<span class="number">5</span>,<span class="number">1</span>), dtype = np.float32 )</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>filters = tf.Variable( np.random.rand(<span class="number">3</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>), dtype = np.float32)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = tf.nn.atrous_conv2d(input_data, filters, <span class="number">2</span>, padding=<span class="string">&#x27;SAME&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(y)</span><br><span class="line">Tensor(<span class="string">&quot;convolution_6/BatchToSpaceND:0&quot;</span>, shape=(<span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>), dtype=float32)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.random_normal(shape=[<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">1</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>kernel = tf.random_normal(shape=[<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = tf.nn.conv2d_transpose(x,kernel,output_shape=[<span class="number">1</span>,<span class="number">5</span>,<span class="number">5</span>,<span class="number">3</span>],strides=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],padding=<span class="string">&quot;SAME&quot;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(y)</span><br><span class="line">Tensor(<span class="string">&quot;conv2d_transpose:0&quot;</span>, shape=(<span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">3</span>), dtype=float32)</span><br></pre></td></tr></table></figure>

<h3 id="池化函数"><a href="#池化函数" class="headerlink" title="池化函数"></a>池化函数</h3><p>池化操作是利用一个矩阵窗口在张量上进行扫描，将每个矩阵窗口中的值通过取最大值或平均值来减少元素个数。每个池化操作的矩阵窗口大小是由ksize指定的，并且根据步长strides决定移动步长。</p>
<h4 id="tf-nn-avg-pool-value-ksize-strides-padding-data-format-x3D-’NHWC’-name-x3D-None"><a href="#tf-nn-avg-pool-value-ksize-strides-padding-data-format-x3D-’NHWC’-name-x3D-None" class="headerlink" title="tf.nn.avg_pool(value, ksize, strides, padding, data_format&#x3D;’NHWC’, name&#x3D;None)"></a>tf.nn.avg_pool(value, ksize, strides, padding, data_format&#x3D;’NHWC’, name&#x3D;None)</h4><p><em>计算池化区域中元素的平均值</em></p>
<p>输入：</p>
<ul>
<li>value：一个四维的张量。数据维度是[batch, height, width, channels]</li>
<li>ksize：一个长度不小于4的整型数组。每一位上的值对应于输入数据张量中每一维的窗口对应值</li>
<li>strides：一个长度不小于4的整型数组。该参数指定滑动窗口在输入数据张量每一维上的步长</li>
<li>padding：一个字符串，取值为SAME或者VALID</li>
<li>data_format: ‘NHWC’代表输入张量维度的顺序，N为个数，H为高度，W为宽度，C为通道数（RGB三通道或者灰度单通道）</li>
<li>name（可选）：为这个操作取一个名字</li>
</ul>
<p>输出：一个张量，数据类型和value相同</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>input_data = tf.Variable( np.random.rand(<span class="number">10</span>,<span class="number">6</span>,<span class="number">6</span>,<span class="number">3</span>), dtype = np.float32 )</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>filter_data = tf.Variable( np.random.rand(<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">10</span>), dtype = np.float32)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = tf.nn.conv2d(input_data, filter_data, strides = [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding = <span class="string">&#x27;SAME&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output = tf.nn.avg_pool(value = y, ksize = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides = [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],padding =<span class="string">&#x27;SAME&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(output)</span><br><span class="line">Tensor(<span class="string">&quot;AvgPool:0&quot;</span>, shape=(<span class="number">10</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">10</span>), dtype=float32)</span><br><span class="line"><span class="comment">#计算输出维度的方法是：shape(output)= (shape(value) - ksize + 1) / strides。</span></span><br></pre></td></tr></table></figure>

<h4 id="tf-nn-max-pool-value-ksize-strides-padding-data-format-x3D-’NHWC’-name-x3D-None"><a href="#tf-nn-max-pool-value-ksize-strides-padding-data-format-x3D-’NHWC’-name-x3D-None" class="headerlink" title="tf.nn.max_pool(value, ksize, strides, padding, data_format&#x3D;’NHWC’, name&#x3D;None)"></a>tf.nn.max_pool(value, ksize, strides, padding, data_format&#x3D;’NHWC’, name&#x3D;None)</h4><p><em>计算池化区域中元素的最大值</em></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>input_data = tf.Variable( np.random.rand(<span class="number">10</span>,<span class="number">6</span>,<span class="number">6</span>,<span class="number">3</span>), dtype = np.float32 )</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>filter_data = tf.Variable( np.random.rand(<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">10</span>), dtype = np.float32)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = tf.nn.conv2d(input_data, filter_data, strides = [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding = <span class="string">&#x27;SAME&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output = tf.nn.max_pool(value = y, ksize = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides = [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line"><span class="meta">... </span>padding =<span class="string">&#x27;SAME&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(output)</span><br><span class="line">Tensor(<span class="string">&quot;MaxPool:0&quot;</span>, shape=(<span class="number">10</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">10</span>), dtype=float32)</span><br></pre></td></tr></table></figure>

<h4 id="tf-nn-max-pool-with-argmax-input-ksize-strides-padding-Targmax-x3D-None-name-x3D-None"><a href="#tf-nn-max-pool-with-argmax-input-ksize-strides-padding-Targmax-x3D-None-name-x3D-None" class="headerlink" title="tf.nn.max_pool_with_argmax(input, ksize, strides, padding, Targmax &#x3D; None, name&#x3D;None)"></a>tf.nn.max_pool_with_argmax(input, ksize, strides, padding, Targmax &#x3D; None, name&#x3D;None)</h4><p><em>计算池化区域中元素的最大值和该最大值所在的位置</em></p>
<blockquote>
<p>该函数只能在GPU下运行，在CPU下没有对应的函数实现</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">input_data = tf.Variable( np.random.rand(<span class="number">10</span>,<span class="number">6</span>,<span class="number">6</span>,<span class="number">3</span>), dtype = tf.float32 )</span><br><span class="line">filter_data = tf.Variable( np.random.rand(<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">10</span>), dtype = np.float32)</span><br><span class="line">y = tf.nn.conv2d(input_data, filter_data, strides = [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding = <span class="string">&#x27;SAME&#x27;</span>)</span><br><span class="line">output, argmax = tf.nn.max_pool_with_argmax(<span class="built_in">input</span> = y, ksize = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],strides = [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding = <span class="string">&#x27;SAME&#x27;</span>)</span><br><span class="line"><span class="comment">#返回结果是一个张量组成的元组（output, argmax），output表示池化区域的最大值；argmax的数据类型是Targmax，维度是四维</span></span><br></pre></td></tr></table></figure>

<h4 id="tf-nn-avg-pool3d-和tf-nn-max-pool3d"><a href="#tf-nn-avg-pool3d-和tf-nn-max-pool3d" class="headerlink" title="tf.nn.avg_pool3d()和tf.nn.max_pool3d()"></a>tf.nn.avg_pool3d()和tf.nn.max_pool3d()</h4><p><em>三维下的平均池化和最大池化</em></p>
<h4 id="tf-nn-fractional-avg-pool-和tf-nn-fractional-max-pool"><a href="#tf-nn-fractional-avg-pool-和tf-nn-fractional-max-pool" class="headerlink" title="tf.nn.fractional_avg_pool()和tf.nn.fractional_max_pool()"></a>tf.nn.fractional_avg_pool()和tf.nn.fractional_max_pool()</h4><p><em>三维下的平均池化和最大池化。</em></p>
<h4 id="tf-nn-pool-input-window-shape-pooling-type-padding-dilation-rate-x3D-None-strides-x3D-None-name-x3D-None-data-format-x3D-None"><a href="#tf-nn-pool-input-window-shape-pooling-type-padding-dilation-rate-x3D-None-strides-x3D-None-name-x3D-None-data-format-x3D-None" class="headerlink" title="tf.nn.pool(input, window_shape, pooling_type, padding, dilation_rate&#x3D;None, strides&#x3D;None, name&#x3D;None, data_format&#x3D;None)"></a>tf.nn.pool(input, window_shape, pooling_type, padding, dilation_rate&#x3D;None, strides&#x3D;None, name&#x3D;None, data_format&#x3D;None)</h4><p><em>执行一个N维的池化操作</em></p>
<h3 id="分类函数"><a href="#分类函数" class="headerlink" title="分类函数"></a>分类函数</h3><h4 id="tf-nn-sigmoid-cross-entropy-with-logits-logits-targets-name-x3D-None"><a href="#tf-nn-sigmoid-cross-entropy-with-logits-logits-targets-name-x3D-None" class="headerlink" title="tf.nn.sigmoid_cross_entropy_with_logits(logits, targets, name&#x3D;None)"></a>tf.nn.sigmoid_cross_entropy_with_logits(logits, targets, name&#x3D;None)</h4><p>输入：</p>
<ul>
<li>logits:[batch_size, num_classes], targets:[batch_size, size].logits用最后一层的输入即可</li>
</ul>
<blockquote>
<p>最后一层不需要进行sigmoid运算，此函数内部进行了sigmoid操作</p>
</blockquote>
<p>输出：loss [batch_size, num_classes]</p>
<h4 id="tf-nn-softmax-logits-dim-x3D-1-name-x3D-None"><a href="#tf-nn-softmax-logits-dim-x3D-1-name-x3D-None" class="headerlink" title="tf.nn.softmax(logits, dim&#x3D;-1, name&#x3D;None)"></a>tf.nn.softmax(logits, dim&#x3D;-1, name&#x3D;None)</h4><p><em>计算Softmax激活，也就是softmax&#x3D;exp(logits)&#x2F;reduce_sum(exp(logits), dim)</em></p>
<h4 id="tf-nn-log-softmax-logits-dim-x3D-1-name-x3D-None"><a href="#tf-nn-log-softmax-logits-dim-x3D-1-name-x3D-None" class="headerlink" title="tf.nn.log_softmax(logits, dim&#x3D;-1, name&#x3D;None)"></a>tf.nn.log_softmax(logits, dim&#x3D;-1, name&#x3D;None)</h4><p><em>计算log softmax激活，也就是logsoftmax &#x3D;logits - log(reduce_sum(exp(logits), dim))</em></p>
<h4 id="tf-nn-softmax-cross-entropy-with-logits-sentinel-x3D-None-labels-x3D-None-logits-x3D-None-dim-x3D-1-name-x3D-None"><a href="#tf-nn-softmax-cross-entropy-with-logits-sentinel-x3D-None-labels-x3D-None-logits-x3D-None-dim-x3D-1-name-x3D-None" class="headerlink" title="tf.nn.softmax_cross_entropy_with_logits(_sentinel&#x3D;None, labels&#x3D;None, logits&#x3D;None, dim&#x3D;-1, name &#x3D;None)"></a>tf.nn.softmax_cross_entropy_with_logits(_sentinel&#x3D;None, labels&#x3D;None, logits&#x3D;None, dim&#x3D;-1, name &#x3D;None)</h4><p>输入：</p>
<ul>
<li>logits and labels 均为[batch_size, num_classes]</li>
</ul>
<p>输出：loss [batch_size]，里面保存是batch中每个样本的交叉熵</p>
<h4 id="tf-nn-sparse-softmax-cross-entropy-with-logits-logits-labels-name-x3D-None"><a href="#tf-nn-sparse-softmax-cross-entropy-with-logits-logits-labels-name-x3D-None" class="headerlink" title="tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels, name&#x3D;None)"></a>tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels, name&#x3D;None)</h4><p>输入：</p>
<ul>
<li>logits: [batch_size, num_classes] labels: [batch_size]，必须在[0, num_classes]</li>
</ul>
<p><em>logits是神经网络最后一层的结果</em></p>
<p>输出：loss [batch_size]，里面保存是batch中每个样本的交叉熵</p>
<h3 id="优化方法"><a href="#优化方法" class="headerlink" title="优化方法"></a>优化方法</h3><p>重点介绍以下8个优化器：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 梯度下降法（BGD和SGD）</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">tf</span>.train.GradientDescentOptimizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># Adadelta法</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">tf</span>.train.AdadeltaOptimizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># Adagrad法（Adagrad和AdagradDAO）</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">tf</span>.train.AdagradOptimizer</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">tf</span>.train.AdagradDAOptimizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># Momentum法（Momentum和Nesterov Momentum）</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">tf</span>.train.MomentumOptimizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># Adam法</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">tf</span>.train.AdamOptimizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># Ftrl法</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">tf</span>.train.FtrlOptimizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># RMSProp法</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">tf</span>.train.RMSPropOptimizer</span><br></pre></td></tr></table></figure>

<p><em>BGD、SGD、Momentum和Nesterov Momentum是手动指定学习率的，其余算法能够自动调节学习率。</em></p>
<h5 id="BGD法"><a href="#BGD法" class="headerlink" title="BGD法"></a>BGD法</h5><p>BGD的全称是batch gradient<br>descent，即批梯度下降。这种方法是利用现有参数对训练集中的每一个输入生成一个估计输出yi，然后跟实际输出yi比较，统计所有误差，求平均以后得到平均误差，以此作为更新参数的依据。它的迭代过程为：</p>
<ul>
<li><p>（1）提取训练集中的所有内容{x 1 , …, x n }，以及相关的输出yi；</p>
</li>
<li><p>（2）计算梯度和误差并更新参数。</p>
</li>
</ul>
<p>这种方法的优点是，使用所有训练数据计算，能够保证收敛，并且不需要逐渐减少学习率；缺点是，每一步都需要使用所有的训练数据，随着训练的进行，速度会越来越慢。</p>
<h4 id="SGD法"><a href="#SGD法" class="headerlink" title="SGD法"></a>SGD法</h4><p>SGD的全称是stochastic gradient descent，即随机梯度下降。因为这种方法的主要思想是将数据集拆分成一个个批次（batch），随机抽取一个批次来计算并更新参数，所以也称为MBGD（minibatch gradient<br>descent）。SGD在每一次迭代计算mini-batch的梯度，然后对参数进行更新。</p>
<p>与BGD相比，SGD在训练数据集很大时，仍能以较快的速度收敛。</p>
<p>但是，它仍然会有下面两个缺点：</p>
<ul>
<li><p>（1）由于抽取不可避免地梯度会有误差，需要手动调整学习率 （learning<br>rate），但是选择合适的学习率又比较困难。尤其在训练时，我们常常想对常出现的特征更新速度快一些，而对不常出现的特征更新速度慢一些，而SGD在更新参数时对所有参数采用一样的学习率，因此无法满足要求。</p>
</li>
<li><p>（2）SGD容易收敛到局部最优，并且在某些情况下可能被困在鞍点。</p>
</li>
</ul>
<h4 id="Momentum法"><a href="#Momentum法" class="headerlink" title="Momentum法"></a>Momentum法</h4><p>Momentum是模拟物理学中动量的概念，更新时在一定程度上保留之前的更新方向，利用当前的批次再微调本次的更新参数，因此引入了一个新的变量v（速度），作为前几次梯度的累加。因此，Momentum能够更新学习率，在下降初期，前后梯度方向一致时，能够加速学习；在下降的中后期，在局部最小值的附近来回震荡时，能够抑制震荡，加快收敛。</p>
<h4 id="Nesterov-Momentum法"><a href="#Nesterov-Momentum法" class="headerlink" title="Nesterov Momentum法"></a>Nesterov Momentum法</h4><p>标准Momentum法首先计算一个梯度（短的1号线），然后在加速更新梯度的方向进行一个大的跳跃（长的1号线）；Nesterov项首先在原来加速的梯度方向进行一个大的跳跃（2号线），然后在该位置计算梯度值（3号线），然后用这个梯度值修正最终的更新方向（4号线）。</p>
<p><img src="/images/TF_01_11.png" alt="avarat"></p>
<h4 id="Adagrad法"><a href="#Adagrad法" class="headerlink" title="Adagrad法"></a>Adagrad法</h4><p>Adagrad法能够自适应地为各个参数分配不同的学习率，能够控制每个维度的梯度方向。这种方法的优点是能够实现学习率的自动更改：如果本次更新时梯度大，学习率就衰减得快一些；如果这次更新时梯度小，学习率衰减得就慢一些。</p>
<h4 id="Adadelta法"><a href="#Adadelta法" class="headerlink" title="Adadelta法"></a>Adadelta法</h4><p>Adagrad法仍然存在一些问题：其学习率单调递减，在训练的后期学习率非常小，并且需要手动设置一个全局的初始学习率。Adadelta法用一阶的方法，近似模拟二阶牛顿法，解决了这些问题。</p>
<h4 id="RMSprop法"><a href="#RMSprop法" class="headerlink" title="RMSprop法"></a>RMSprop法</h4><p>RMSProp法与Momentum法类似，通过引入一个衰减系数，使每一回合都衰减一定比例。在实践中，对循环神经网络（RNN）效果很好。</p>
<h4 id="Adam法"><a href="#Adam法" class="headerlink" title="Adam法"></a>Adam法</h4><p>Adam的名称来源于自适应矩估计（adaptive moment estimation）。Adam法根据损失函数针对每个参数的梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。</p>
<h4 id="各个方法的比较"><a href="#各个方法的比较" class="headerlink" title="各个方法的比较"></a>各个方法的比较</h4><blockquote>
<p>在不怎么调整参数的情况下，Adagrad法比SGD法和Momentum法更稳定，性能更优；精调参数的情况下，精调的SGD法和Momentum法在收敛速度和准确性上要优于Adagrad法</p>
</blockquote>
<h5 id="各个优化器的损失值比较结果"><a href="#各个优化器的损失值比较结果" class="headerlink" title="各个优化器的损失值比较结果"></a>各个优化器的损失值比较结果</h5><p><img src="/images/TF_01_12.png" alt="avarat"></p>
<h5 id="各个优化器的测试准确率比较"><a href="#各个优化器的测试准确率比较" class="headerlink" title="各个优化器的测试准确率比较"></a>各个优化器的测试准确率比较</h5><p><img src="/images/TF_01_13.png" alt="avarat"></p>
<h5 id="各个优化器的训练准确率比较"><a href="#各个优化器的训练准确率比较" class="headerlink" title="各个优化器的训练准确率比较"></a>各个优化器的训练准确率比较</h5><p><img src="/images/TF_01_14.png" alt="avarat"></p>
<h2 id="模型的存储与加载"><a href="#模型的存储与加载" class="headerlink" title="模型的存储与加载"></a>模型的存储与加载</h2><p>TensorFlow的API提供了以下两种方式来存储和加载模型。</p>
<ul>
<li><p>（1）生成检查点文件 （checkpoint file），扩展名一般为.ckpt，通过在tf.train. Saver对象上调用Saver.save()<br>生成。它包含权重和其他在程序中定义的变量，不包含图结构。如果需要在另一个程序中使用，需要重新创建图形结构，并告诉TensorFlow如何处理这些权重。</p>
</li>
<li><p>（2）生成图协议文件（graph proto file），这是一个二进制文件，扩展名一般为.pb，用tf.trainwrite_graph()保存，只包含图形结构，不包含权重，然后使用tf.import_graph_def()<br>来加载图形。</p>
</li>
</ul>
<h3 id="训练模型及存储模型过程"><a href="#训练模型及存储模型过程" class="headerlink" title="训练模型及存储模型过程"></a>训练模型及存储模型过程</h3><h4 id="1-我们定义一个存储路径，这里就用当前路径下的ckpt-dir目录"><a href="#1-我们定义一个存储路径，这里就用当前路径下的ckpt-dir目录" class="headerlink" title="1. 我们定义一个存储路径，这里就用当前路径下的ckpt_dir目录"></a>1. 我们定义一个存储路径，这里就用当前路径下的ckpt_dir目录</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ckpt_dir = <span class="string">&quot;./ckpt_dir&quot;</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(ckpt_dir):</span><br><span class="line">    os.makedirs(ckpt_dir)</span><br></pre></td></tr></table></figure>

<h4 id="2-定义一个计数器，为训练轮数计数"><a href="#2-定义一个计数器，为训练轮数计数" class="headerlink" title="2. 定义一个计数器，为训练轮数计数"></a>2. 定义一个计数器，为训练轮数计数</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 计数器变量，设置它的trainable=False，不需要被训练</span></span><br><span class="line">global_step = tf.Variable(<span class="number">0</span>, name=<span class="string">&#x27;global_step&#x27;</span>, trainable=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<h4 id="3-当定义完所有变量后，调用tf-train-Saver-来保存和提取变量，其后面定义的变量将不会被存储"><a href="#3-当定义完所有变量后，调用tf-train-Saver-来保存和提取变量，其后面定义的变量将不会被存储" class="headerlink" title="3. 当定义完所有变量后，调用tf.train. Saver()来保存和提取变量，其后面定义的变量将不会被存储"></a>3. 当定义完所有变量后，调用tf.train. Saver()来保存和提取变量，其后面定义的变量将不会被存储</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在声明完所有变量后，调用tf.train.Saver</span></span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line"><span class="comment"># 位于tf.train.Saver之后的变量将不会被存储</span></span><br><span class="line">non_storable_variable = tf.Variable(<span class="number">777</span>)</span><br></pre></td></tr></table></figure>

<h4 id="4-训练模型并存储"><a href="#4-训练模型并存储" class="headerlink" title="4. 训练模型并存储"></a>4. 训练模型并存储</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    tf.initialize_all_variables().run()</span><br><span class="line"></span><br><span class="line">    start = global_step.<span class="built_in">eval</span>() <span class="comment"># 得到global_step的初始值</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Start from:&quot;</span>, start)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(start, <span class="number">100</span>):</span><br><span class="line">        <span class="comment"># 以128作为batch_size</span></span><br><span class="line">        <span class="keyword">for</span> start, end <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(trX), <span class="number">128</span>), <span class="built_in">range</span>(<span class="number">128</span>, <span class="built_in">len</span>(trX)+<span class="number">1</span>, <span class="number">128</span>)):</span><br><span class="line">            sess.run(train_op, feed_dict=&#123;X: trX[start:end], Y: trY[start:end],</span><br><span class="line">                p_keep_input: <span class="number">0.8</span>, p_keep_hidden: <span class="number">0.5</span>&#125;)</span><br><span class="line"></span><br><span class="line">    global_step.assign(i).<span class="built_in">eval</span>() <span class="comment"># 更新计数器</span></span><br><span class="line">    saver.save(sess, ckpt_dir + <span class="string">&quot;/model.ckpt&quot;</span>, global_step=global_step) <span class="comment"># 存储模型</span></span><br></pre></td></tr></table></figure>

<p>在训练的过程中，ckpt_dir下会出现16个文件，其中有5个model.ckpt-{n}.data-00000-of-00001文件，是训练过程中保存的模型，5个model.ckpt-{n}.meta文件，是训练过程中保存的元数据（TensorFlow默认只保存最近5个模型和元数据，删除前面没用的模型和元数据），5个model.ckpt-{n}.index文件，{n}代表迭代次数，以及1个检查点文本文件，里面保存着当前模型和最近的5个模型，内容如下：</p>
<pre><code>model_checkpoint_path: &quot;model.ckpt-60&quot;
all_model_checkpoint_paths: &quot;model.ckpt-56&quot;
all_model_checkpoint_paths: &quot;model.ckpt-57&quot;
all_model_checkpoint_paths: &quot;model.ckpt-58&quot;
all_model_checkpoint_paths: &quot;model.ckpt-59&quot;
all_model_checkpoint_paths: &quot;model.ckpt-60&quot;
</code></pre>
<h3 id="加载模型"><a href="#加载模型" class="headerlink" title="加载模型"></a>加载模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    tf.initialize_all_variables().run()</span><br><span class="line">    ckpt = tf.train.get_checkpoint_state(ckpt_dir)</span><br><span class="line">    <span class="keyword">if</span> ckpt <span class="keyword">and</span> ckpt.model_checkpoint_path:</span><br><span class="line">        <span class="built_in">print</span>(ckpt.model_checkpoint_path)</span><br><span class="line">        saver.restore(sess, ckpt.model_checkpoint_path) <span class="comment"># 加载所有的参数</span></span><br><span class="line">        <span class="comment"># 从这里开始就可以直接使用模型进行预测，或者接着继续训练了</span></span><br></pre></td></tr></table></figure>

<h3 id="图的存储与加载"><a href="#图的存储与加载" class="headerlink" title="图的存储与加载"></a>图的存储与加载</h3><h4 id="当仅保存图模型时，才将图写入二进制协议文件中"><a href="#当仅保存图模型时，才将图写入二进制协议文件中" class="headerlink" title="当仅保存图模型时，才将图写入二进制协议文件中"></a>当仅保存图模型时，才将图写入二进制协议文件中</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">v = tf.Variable(<span class="number">0</span>, name=<span class="string">&#x27;my_variable&#x27;</span>)</span><br><span class="line">sess = tf.Session()</span><br><span class="line">tf.train.write_graph(sess.graph_def, <span class="string">&#x27;/tmp/tfmodel&#x27;</span>, <span class="string">&#x27;train.pbtxt&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="当读取时，又从协议文件中读取出来："><a href="#当读取时，又从协议文件中读取出来：" class="headerlink" title="当读取时，又从协议文件中读取出来："></a>当读取时，又从协议文件中读取出来：</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> _sess:</span><br><span class="line">    <span class="keyword">with</span> gfile.FastGFile(<span class="string">&quot;/tmp/tfmodel/train.pbtxt&quot;</span>,<span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        graph_def = tf.GraphDef()</span><br><span class="line">        graph_def.ParseFromString(f.read())</span><br><span class="line">        _sess.graph.as_default()</span><br><span class="line">        tf.import_graph_def(graph_def, name=<span class="string">&#x27;tfgraph&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="队列和线程"><a href="#队列和线程" class="headerlink" title="队列和线程"></a>队列和线程</h2><h3 id="队列"><a href="#队列" class="headerlink" title="队列"></a>队列</h3><h4 id="1．FIFOQueue"><a href="#1．FIFOQueue" class="headerlink" title="1．FIFOQueue"></a>1．FIFOQueue</h4><p>FIFOQueue创建一个先入先出队列。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment"># 创建一个先入先出队列,初始化队列插入0.1、0.2、0.3三个数字</span></span><br><span class="line">q = tf.FIFOQueue(<span class="number">3</span>, <span class="string">&quot;float&quot;</span>)</span><br><span class="line">init = q.enqueue_many(([<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>],))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义出队、+1、入队操作</span></span><br><span class="line">x = q.dequeue()</span><br><span class="line">y = x + <span class="number">1</span></span><br><span class="line">q_inc = q.enqueue([y])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后开启一个会话，执行2次q_inc操作，随后查看队列的内容：</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    quelen = sess.run(q.size())</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">        sess.run(q_inc) <span class="comment"># 执行2次操作，队列中的值变为0.3,1.1,1.2</span></span><br><span class="line">        quelen = sess.run(q.size())</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(quelen):</span><br><span class="line">        <span class="built_in">print</span> (sess.run(q.dequeue())) <span class="comment"># 输出队列的值</span></span><br></pre></td></tr></table></figure>

<p>最终结果如下：0.3, 1.1, 1.2</p>
<h4 id="2．RandomShuffleQueue"><a href="#2．RandomShuffleQueue" class="headerlink" title="2．RandomShuffleQueue"></a>2．RandomShuffleQueue</h4><p>RandomShuffleQueue创建一个随机队列，在出队列时，是以随机的顺序产生元素的。</p>
<p>RandomShuffleQueue在TensorFlow使用异步计算时非常重要。因为TensorFlow的会话是支 持多线程的，我们可以在主线程里执行训练操作，使用RandomShuffleQueue作为训练输入，开<br>多个线程来准备训练样本，将样本压入队列后，主线程会从队列中每次取出mini-batch的样本 进行训练。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建一个随机队列，队列最大长度为10，出队后最小长度为2：</span></span><br><span class="line">q = tf.RandomShuffleQueue(capacity=<span class="number">10</span>, min_after_dequeue=<span class="number">2</span>, dtypes=<span class="string">&quot;float&quot;</span>)</span><br><span class="line"><span class="comment"># 开启一个会话，执行10次入队操作，8次出队操作：</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">10</span>): <span class="comment">#10次入队</span></span><br><span class="line">    sess.run(q.enqueue(i))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">3</span>): <span class="comment"># 8次出队</span></span><br><span class="line">    <span class="built_in">print</span>(sess.run(q.dequeue()))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果：8.0，4.0，9.0（乱序输出）</span></span><br></pre></td></tr></table></figure>

<p>阻断一般发生在：</p>
<ul>
<li>队列长度等于最小值，执行出队操作；</li>
<li>队列长度等于最大值，执行入队操作。</li>
</ul>
<h3 id="队列管理器"><a href="#队列管理器" class="headerlink" title="队列管理器"></a>队列管理器</h3><p>会话中可以运行多个线程，我们使用线程管理器QueueRunner创建一系列的新线程进行入队操作，让主线程继续使用数据，即训练网络和读取数据是异步的，主线程在训练网络，另一个线程在将数据从硬盘读入内存。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建一个含有队列的图：</span></span><br><span class="line">q = tf.FIFOQueue(<span class="number">1000</span>, <span class="string">&quot;float&quot;</span>)</span><br><span class="line">counter = tf.Variable(<span class="number">0.0</span>) <span class="comment"># 计数器</span></span><br><span class="line">increment_op = tf.assign_add(counter, tf.constant(<span class="number">1.0</span>)) <span class="comment"># 操作：给计数器加1</span></span><br><span class="line">enqueue_op = q.enqueue(counter) <span class="comment"># 操作：计数器值加入队列</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个队列管理器QueueRunner，用这两个操作向队列q中添加元素。目前我们只使用一个线程：</span></span><br><span class="line">qr = tf.train.QueueRunner(q, enqueue_ops=[increment_op, enqueue_op] * <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动一个会话，从队列管理器qr中创建线程：</span></span><br><span class="line"><span class="comment">#主线程</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    enqueue_threads = qr.create_threads(sess, start=<span class="literal">True</span>) <span class="comment"># 启动入队线程</span></span><br><span class="line">    <span class="comment">#主线程</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        <span class="built_in">print</span> (sess.run(q.dequeue()))</span><br></pre></td></tr></table></figure>

<h3 id="线程和协调器"><a href="#线程和协调器" class="headerlink" title="线程和协调器"></a>线程和协调器</h3><p>使用协调器（coordinator）来管理线程：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 主线程</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Coordinator：协调器，协调线程间的关系可以视为一种信号量，用来做同步</span></span><br><span class="line">coord = tf.train.Coordinator()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动入队线程，协调器是线程的参数</span></span><br><span class="line">enqueue_threads = qr.create_threads(sess, coord = coord,start=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">coord.request_stop()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 主线程</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">10</span>):</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="built_in">print</span>(sess.run(q.dequeue()))</span><br><span class="line">    <span class="comment"># 使用tf.errors.OutOfRangeError来捕捉错误，终止循环</span></span><br><span class="line">    <span class="keyword">except</span> tf.errors.OutOfRangeError:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">coord.join(enqueue_threads</span><br></pre></td></tr></table></figure>

<h2 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h2><p>TensorFlow作为符号编程框架，需要先构建数据流图，再读取数据，随后进行模型训练。</p>
<h3 id="1-预加载数据"><a href="#1-预加载数据" class="headerlink" title="1. 预加载数据"></a>1. 预加载数据</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x1 = tf.constant([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">x2 = tf.constant([<span class="number">4</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">y = tf.add(x1, x2)</span><br></pre></td></tr></table></figure>

<p><em>这种方式的缺点在于，将数据直接嵌在数据流图中，当训练数据较大时，很消耗内存。</em></p>
<h3 id="2-填充数据"><a href="#2-填充数据" class="headerlink" title="2. 填充数据"></a>2. 填充数据</h3><p>使用sess.run()中的feed_dict参数，将Python产生的数据填充给后端。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment"># 设计图</span></span><br><span class="line">a1 = tf.placeholder(tf.int16)</span><br><span class="line">a2 = tf.placeholder(tf.int16)</span><br><span class="line">b = tf.add(x1, x2)</span><br><span class="line"><span class="comment"># 用Python产生数据</span></span><br><span class="line">li1 = [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line">li2 = [<span class="number">4</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line"><span class="comment"># 打开一个会话，将数据填充给后端</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="built_in">print</span> sess.run(b, feed_dict=&#123;a1: li1, a2: li2&#125;)</span><br></pre></td></tr></table></figure>

<h3 id="3-从文件读取数据"><a href="#3-从文件读取数据" class="headerlink" title="3. 从文件读取数据"></a>3. 从文件读取数据</h3><h4 id="1-生成TFRecords文件"><a href="#1-生成TFRecords文件" class="headerlink" title="1. 生成TFRecords文件"></a>1. 生成TFRecords文件</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 主函数main：给训练、验证、测试数据集做转换</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">unused_argv</span>):</span><br><span class="line">    <span class="comment"># 获取数据</span></span><br><span class="line">    data_sets = mnist.read_data_sets(FLAGS.directory,dtype=tf.uint8,reshape=<span class="literal">False</span>,validation_size=FLAGS.validation_size) <span class="comment"># 注意，这里的编码是uint8</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将数据转换为tf.train.Example类型，并写入TFRecords文件</span></span><br><span class="line">    convert_to(data_sets.train, <span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">    convert_to(data_sets.validation, <span class="string">&#x27;validation&#x27;</span>)</span><br><span class="line">    convert_to(data_sets.test, <span class="string">&#x27;test&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转换函数convert_to： 将数据填入到tf.train.Example的协议缓冲区 （protocolbuffer）中，将协议缓冲区序列化为一个字符串，通过tf.python_io.TFRecordWriter 写入TFRecords文件。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">convert_to</span>(<span class="params">data_set, name</span>):</span><br><span class="line">    images = data_set.images</span><br><span class="line">    labels = data_set.labels</span><br><span class="line">    num_examples = data_set.num_examples <span class="comment"># 55000个训练数据，5000个验证数据，10000个测试数据</span></span><br><span class="line">    <span class="keyword">if</span> images.shape[<span class="number">0</span>] != num_examples:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&#x27;Images size %d does not match label size %d.&#x27;</span> %(images.shape[<span class="number">0</span>], num_examples))</span><br><span class="line">    rows = images.shape[<span class="number">1</span>] <span class="comment"># 28</span></span><br><span class="line">    cols = images.shape[<span class="number">2</span>] <span class="comment"># 28</span></span><br><span class="line">    depth = images.shape[<span class="number">3</span>] <span class="comment"># 1，是黑白图像，所以是单通道</span></span><br><span class="line"></span><br><span class="line">    filename = os.path.join(FLAGS.directory, name + <span class="string">&#x27;.tfrecords&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Writing&#x27;</span>, filename)</span><br><span class="line">    writer = tf.python_io.TFRecordWriter(filename)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> <span class="built_in">range</span>(num_examples):</span><br><span class="line">        image_raw = images[index].tostring()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 写入协议缓冲区中，height、width、depth、label编码成int64类型，image_raw编码成二进制</span></span><br><span class="line">    example = tf.train.Example(features=tf.train.Features(feature=&#123;<span class="string">&#x27;height&#x27;</span>: _int64_feature(rows),<span class="string">&#x27;width&#x27;</span>: _int64_feature(cols),<span class="string">&#x27;depth&#x27;</span>: _int64_feature(depth),<span class="string">&#x27;label&#x27;</span>: _int64_feature(<span class="built_in">int</span>(labels[index])),<span class="string">&#x27;image_raw&#x27;</span>: _bytes_feature(image_raw)&#125;))</span><br><span class="line"></span><br><span class="line">    writer.write(example.SerializeToString()) <span class="comment"># 序列化为字符串</span></span><br><span class="line">    writer.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 编码函数：</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_int64_feature</span>(<span class="params">value</span>):</span><br><span class="line">    <span class="keyword">return</span> tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_bytes_feature</span>(<span class="params">value</span>):</span><br><span class="line">    <span class="keyword">return</span> tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))</span><br></pre></td></tr></table></figure>

<p><em>运行结束后，在&#x2F;tmp&#x2F;data下生成3个文件，即train.tfrecords、validation.tfrecords和test.tfrecords。</em></p>
<h4 id="2-从队列中读取"><a href="#2-从队列中读取" class="headerlink" title="2. 从队列中读取"></a>2. 从队列中读取</h4><p>一旦生成了TFRecords文件，接下来就可以使用队列读取数据了。主要分为3步：</p>
<ul>
<li><p>（1）创建张量，从二进制文件读取一个样本；</p>
</li>
<li><p>（2）创建张量，从二进制文件随机读取一个mini-batch；</p>
</li>
<li><p>（3）把每一批张量传入网络作为输入节点。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 首先我们定义从文件中读取并解析一个样本：</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_and_decode</span>(<span class="params">filename_queue</span>): <span class="comment"># 输入文件名队列</span></span><br><span class="line">    reader = tf.TFRecordReader()</span><br><span class="line">    _, serialized_example = reader.read(filename_queue)</span><br><span class="line">    features = tf.parse_single_example( <span class="comment"># 解析example</span></span><br><span class="line">    serialized_example,</span><br><span class="line">    <span class="comment"># 必须写明features里面的key的名称</span></span><br><span class="line">    features=&#123;</span><br><span class="line">        <span class="string">&#x27;image_raw&#x27;</span>: tf.FixedLenFeature([], tf.string), <span class="comment"># 图片是string类型</span></span><br><span class="line">        <span class="string">&#x27;label&#x27;</span>: tf.FixedLenFeature([], tf.int64), <span class="comment"># 标记是int64类型</span></span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="comment"># 对于BytesList，要重新进行解码，把string类型的0维Tensor变成uint8类型的一维Tensor</span></span><br><span class="line">    image = tf.decode_raw(features[<span class="string">&#x27;image_raw&#x27;</span>], tf.uint8)</span><br><span class="line">    image.set_shape([mnist.IMAGE_PIXELS])</span><br><span class="line">    <span class="comment"># Tensor(&quot;input/DecodeRaw:0&quot;, shape=(784,), dtype=uint8)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># image张量的形状为：Tensor(&quot;input/sub:0&quot;, shape=(784,), dtype=float32)</span></span><br><span class="line">    image = tf.cast(image, tf.float32) * (<span class="number">1</span>．/ <span class="number">255</span>) - <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 把标记从uint8类型转换为int32类型</span></span><br><span class="line">    <span class="comment"># label张量的形状为Tensor(&quot;input/Cast_1:0&quot;, shape=(), dtype=int32)</span></span><br><span class="line">    label = tf.cast(features[<span class="string">&#x27;label&#x27;</span>], tf.int32)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> image, label</span><br><span class="line"></span><br><span class="line"><span class="comment"># 接下来使用tf.train.shuffle_batch将前面生成的样本随机化，获得一个最小批次的张量：</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">inputs</span>(<span class="params">train, batch_size, num_epochs</span>):</span><br><span class="line">    <span class="comment"># 输入参数:</span></span><br><span class="line">    <span class="comment"># train: 选择输入训练数据/验证数据</span></span><br><span class="line">    <span class="comment"># batch_size: 训练的每一批有多少个样本</span></span><br><span class="line">    <span class="comment"># num_epochs: 过几遍数据，设置为0/None表示永远训练下去</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    返回结果：A tuple (images, labels)</span></span><br><span class="line"><span class="string">    * images: 类型float, 形状[batch_size, mnist.IMAGE_PIXELS]，范围[-0.5, 0.5].</span></span><br><span class="line"><span class="string">    * labels： 类型int32，形状[batch_size]，范围 [0, mnist.NUM_CLASSES]</span></span><br><span class="line"><span class="string">    注意tf.train.QueueRunner 必须用tf.train.start_queue_runners()来启动线程</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> num_epochs: num_epochs = <span class="literal">None</span></span><br><span class="line">    <span class="comment"># 获取文件路径，即/tmp/data/train.tfrecords, /tmp/data/validation.records</span></span><br><span class="line">    filename = os.path.join(FLAGS.train_dir,</span><br><span class="line">    TRAIN_FILE <span class="keyword">if</span> train <span class="keyword">else</span> VALIDATION_FILE)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;input&#x27;</span>):</span><br><span class="line">        <span class="comment"># tf.train.string_input_producer返回一个QueueRunner，里面有一个FIFOQueue</span></span><br><span class="line">        filename_queue = tf.train.string_input_producer([filename], num_epochs=num_epochs) <span class="comment"># 如果样本量很大，可以分成若干文件，把文件名列表传入</span></span><br><span class="line"></span><br><span class="line">        image, label = read_and_decode(filename_queue)</span><br><span class="line">        <span class="comment"># 随机化example，并把它们规整成batch_size大小</span></span><br><span class="line">        <span class="comment"># tf.train.shuffle_batch生成了RandomShuffleQueue，并开启两个线程</span></span><br><span class="line">        images, sparse_labels = tf.train.shuffle_batch(</span><br><span class="line">            [image, label], batch_size=batch_size, num_threads=<span class="number">2</span>,</span><br><span class="line">            capacity=<span class="number">1000</span> + <span class="number">3</span> * batch_size,</span><br><span class="line">            min_after_dequeue=<span class="number">1000</span>) <span class="comment"># 留下一部分队列，来保证每次有足够的数据做随机打乱</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> images, sparse_labels</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最后，我们把生成的batch张量作为网络的输入，进行训练：</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_training</span>():</span><br><span class="line">    <span class="keyword">with</span> tf.Graph().as_default():</span><br><span class="line">        <span class="comment"># 输入images和labels</span></span><br><span class="line">        images, labels = inputs(train=<span class="literal">True</span>, batch_size=FLAGS.batch_size,num_epochs=FLAGS.num_epochs)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 构建一个从推理模型来预测数据的图</span></span><br><span class="line">        logits = mnist.inference(images,</span><br><span class="line">                                FLAGS.hidden1,</span><br><span class="line">                                FLAGS.hidden2)</span><br><span class="line"></span><br><span class="line">        loss = mnist.loss(logits, labels) <span class="comment"># 定义损失函数</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Add to the Graph operations that train the model.</span></span><br><span class="line">        train_op = mnist.training(loss, FLAGS.learning_rate)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化参数，特别注意：string_input_producer内部创建了一个epoch计数变量，</span></span><br><span class="line">        <span class="comment"># 归入tf.GraphKeys.LOCAL_VARIABLES集合中，必须单独用initialize_local_variables()初始化</span></span><br><span class="line">        init_op = tf.group(tf.global_variables_initializer(),</span><br><span class="line">                            tf.local_variables_initializer())</span><br><span class="line"></span><br><span class="line">        sess = tf.Session()</span><br><span class="line"></span><br><span class="line">        sess.run(init_op)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Start input enqueue threads.</span></span><br><span class="line">        coord = tf.train.Coordinator()</span><br><span class="line">        threads = tf.train.start_queue_runners(sess=sess, coord=coord)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            step = <span class="number">0</span></span><br><span class="line">            <span class="keyword">while</span> <span class="keyword">not</span> coord.should_stop(): <span class="comment"># 进入永久循环</span></span><br><span class="line">                start_time = time.time()</span><br><span class="line">                _, loss_value = sess.run([train_op, loss])</span><br><span class="line">            duration = time.time() - start_time</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 每100次训练输出一次结果</span></span><br><span class="line">            <span class="keyword">if</span> step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;Step %d: loss = %.2f (%.3f sec)&#x27;</span> % (step, loss_value, duration))</span><br><span class="line">            step += <span class="number">1</span></span><br><span class="line">        <span class="keyword">except</span> tf.errors.OutOfRangeError:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Done training for %d epochs, %d steps.&#x27;</span> % (FLAGS.num_epochs, step))</span><br><span class="line">        <span class="keyword">finally</span>:</span><br><span class="line">        coord.request_stop() <span class="comment"># 通知其他线程关闭</span></span><br><span class="line"></span><br><span class="line">        coord.join(threads)</span><br><span class="line">        sess.close()</span><br></pre></td></tr></table></figure>

<p>如上所述，我们总结出TensorFlow使用TFRecords文件训练样本的步骤：</p>
<ul>
<li><p>（1）在生成文件名队列中，设定epoch数量；</p>
</li>
<li><p>（2）训练时，设定为无穷循环；</p>
</li>
<li><p>（3）在读取数据时，如果捕捉到错误，终止。</p>
</li>
</ul>
<h2 id="实现自定义操作"><a href="#实现自定义操作" class="headerlink" title="实现自定义操作"></a>实现自定义操作</h2><p><strong>略</strong></p>
]]></content>
      <tags>
        <tag>计算机</tag>
      </tags>
  </entry>
  <entry>
    <title>TF学习（二）</title>
    <url>/2020/07/03/Learning-tensorflow-02/</url>
    <content><![CDATA[<blockquote>
<p><strong>本篇文章主要介绍TF的源码结构、神经网络概述以及Keras，TFLearn高级框架</strong></p>
</blockquote>
<span id="more"></span>

<h1 id="Tensorflow源码解析"><a href="#Tensorflow源码解析" class="headerlink" title="Tensorflow源码解析"></a>Tensorflow源码解析</h1><h2 id="Tensorflow目录结构"><a href="#Tensorflow目录结构" class="headerlink" title="Tensorflow目录结构"></a>Tensorflow目录结构</h2><p>tensorflow-1.1.0目录:</p>
<p><img src="/images/TF_02_01.png" alt="avarat"></p>
<p>tensorflow目录:</p>
<p><img src="/images/TF_02_02.png" alt="avarat"></p>
<h2 id="Tensorflow源代码学习方法"><a href="#Tensorflow源代码学习方法" class="headerlink" title="Tensorflow源代码学习方法"></a>Tensorflow源代码学习方法</h2><ul>
<li><p>（1）了解自己要研究的基本领域，如图像分类、物体检测、语音识别等，了解对应这个领域所用的技术，如卷积神经网络 （convolutional neural network，CNN）和循环神经网络 （recurrentneural<br>network，RNN），知道实现的基本原理。</p>
</li>
<li><p>（2）尝试运行GitHub上对应的基本模型</p>
</li>
</ul>
<p><img src="/images/TF_02_03.png" alt="avarat"></p>
<p><strong>如果研究领域是计算机视觉，可以看代码中的如下几个目录：compresssion（图像压缩）、im2txt（图像描述）、inception（对ImageNet数据集用Inception<br>V3架构去训练和评估）、resnet（残差网络）、slim（图像分类）和street（路标识别或验证码识别）。</strong></p>
<p>如果研究领域是自然语言处理，可以看lm_1b（语言模型）、namignizer（起名字）、swivel（使用Swivel算法转换词向量）、syntaxnet（分词和语法分析）、textsum（文本摘要）以及tutorials目录里的word2vec（词转换为向量）。</p>
<h1 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h1><h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><p>卷积神经网络（CNN），属于人工神经网络的一种，它的权值共享 （weight sharing）的网络结构显著降低了模型的复杂度，减少了权值的数量，是目前语音分析和图像识别领域研究热点。</p>
<p>卷积是泛函分析中的一种积分变换的数学方法，通过两个函数f 和g 生成第三个函数的一种数学算子，表征函数f 与g 经过翻转和平移的重叠部分的面积。 ① 设f (x )和g (x )是R 1 上的两个可积函数，做积分后的新函数就称为函数f 与g<br>的卷积：</p>
<p><img src="/images/TF_02_04.png" alt="avarat"></p>
<p>神经网络 （neural networks，NN）的基本组成包括输入层、隐藏层、输出层。卷积神经网络的特点在于隐藏层分为卷积层 和池化层 （pooling layer，又叫下采样层 ）。卷积层通过一块块卷积核 （conventional<br>kernel）在原始图像上平移来提取特征，每一个特征就是一个特征映射；而池化层通过汇聚特征后稀疏参数来减少要学习的参数，降低网络的复杂度，池化层最常见的包括最大值池化 （max pooling）和平均值池化 （average<br>pooling）</p>
<p><img src="/images/TF_02_05.png" alt="avarat"></p>
<p>卷积核在提取特征映射时的动作称为padding，其有两种方式，即SAME和VALID。由于移动步长（Stride）不一定能整除整张图的像素宽度，我们把不越过边缘取样称为Valid<br>Padding，取样的面积小于输入图像的像素宽度；越过边缘取样称为Same Padding，取样的面积和输入图像的像素宽度一致。</p>
<h2 id="卷积神经网络发展"><a href="#卷积神经网络发展" class="headerlink" title="卷积神经网络发展"></a>卷积神经网络发展</h2><p><img src="/images/TF_02_06.png" alt="avarat"></p>
<h3 id="LeNet"><a href="#LeNet" class="headerlink" title="LeNet"></a>LeNet</h3><p>LeNet的论文详见：<a href="http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf">http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf</a></p>
<p>LeNet包含的组件如下。</p>
<ul>
<li>输入层：32×32。</li>
<li>卷积层：3个。</li>
<li>降采样层：2个。</li>
<li>全连接层：1个。</li>
<li>输出层（高斯连接）：10个类别（数字0～9的概率）。</li>
</ul>
<p><img src="/images/TF_02_07.jpg" alt="avarat"></p>
<p>各个层的用途及意义:</p>
<p>（1）输入层。输入图像尺寸为32×32。这要比MNIST数据集中的字母（28×28）还大，即对图 像做了预处理reshape操作。这样做的目的是希望潜在的明显特征，如笔画断续、角点，能够出 现在最高层特征监测卷积核的中心。</p>
<p>（2）卷积层（C1, C3, C5）。卷积运算的主要目的是使原信号特征增强，并且降低噪音。在一 个可视化的在线演示示例 [5] 中，我们可以看出不同的卷积核输出特征映射的不同，如图 6-5 所示。</p>
<p>（3）下采样层（S2, S4）。下采样层主要是想降低网络训练参数及模型的过拟合程度。通常 有以下两种方式。</p>
<ul>
<li>最大池化 （max pooling）：在选中区域中找最大的值作为采样后的值。</li>
<li>平均值池化 （mean pooling）：把选中的区域中的平均值作为采样后的值。</li>
</ul>
<p>（4）全连接层（F6）。F6是全连接层，计算输入向量和权重向量的点积，再加上一个偏置。 随后将其传递给sigmoid函数，产生单元i 的一个状态。</p>
<p>（5）输出层。输出层由欧式径向基函数 （Euclidean radial basis function）单元组成，每个类 别（数字的0～9）对应一个径向基函数单元，每个单元有84个输入。也就是说，每个输出RBF单<br>元计算输入向量和该类别标记向量之间的欧式距离。距离越远，RBF输出越大。</p>
<h3 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h3><p>AlexNet的论文详见Alex Krizhevsky、Ilya Sutskever和Geoffrey E．Hinton的《ImageNet Classification with Deep Convolutional Neural<br>Networks》</p>
<p>AlexNet由5个卷积层、5个池化层、3个全连接层，大约5000万个可调参数组成。最后一个全连接层的输出被送到一个1000维的softmax层，产生一个覆盖1000类标记的分布。AlexNet由两个GPU协作：一个GPU运行图中顶部的层次部分，另一个GPU运行图中底部的层次部分。GPU之间仅在某些层互相通信。</p>
<p><img src="/images/TF_02_08.jpg" alt="avarat"></p>
<p>特点：</p>
<ul>
<li>防止过拟合：Dropout、数据增强 （data augmentation）。</li>
<li>非线性激活函数：ReLU。</li>
<li>大数据训练：120万（百万级）ImageNet图像数据。</li>
<li>GPU实现、LRN（local responce normalization）规范化层的使用。</li>
</ul>
<blockquote>
<p>Dropout。AlexNet做的是以0.5的概率将每个隐层神经元的输出设置为0。以这种方式被抑制的神经元既不参与前向传播，也不参与反向传播。因此，每次输入一个样本，就相当于该神经网络尝试了一个新结构，但是所有这些结构之间共享权重。因为神经元不能依赖于其他神经元而存在，所以这种技术降低了神经元复杂的互适应关系。因此，网络需要被迫学习更为健壮的特征，这些特征在结合其他神经元的一些不同随机子集时很有用。如果没有Dropout，我们的网络会表现出大量的过拟合。Dropout使收敛所需的迭代次数大致增加了一倍。</p>
</blockquote>
<h3 id="VGGNet"><a href="#VGGNet" class="headerlink" title="VGGNet"></a>VGGNet</h3><p>VGGNet可以看成是加深版本的AlexNet，参见Karen Simonyan和Andrew Zisserman 的论文《Very Deep Convolutional Networks for Large-Scale Visual<br>Recognition》</p>
<p>VGGNet也是5个卷积组、2层全连接图像特征、1层全连接分类特征，可以看作和AlexNet一样总共8个部分。根据前5个卷积组，VGGNet论文中给出了A～E这5种配置。卷积层数从8（A配置）到16（E配置）递增。VGGNet不同于AlexNet的地方是：VGGNet使用的层更多，通常有16～19层，而AlexNet只有8层。</p>
<p><img src="/images/TF_02_09.jpg" alt="avarat"></p>
<h3 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h3><p>GoogLeNet的更多内容详见Christian Szegedy和Wei Liu等人的论文《Going Deeper with Convolutions》</p>
<blockquote>
<p>NIN（Network in Network）（详见Min Lin和Qiang Chen和Shuicheng Yan的论文《Network In Network》），它对传统的卷积方法做了两点改进：将原来的线性卷积层（linear convolution layer）变为多层感知卷积层（multilayer perceptron）；将全连接层的改进为全局平均池化。</p>
</blockquote>
<p><img src="/images/TF_02_10.jpg" alt="avarat"></p>
<p>GoogLeNet的主要思想是围绕“深度”和“宽度”去实现的。</p>
<ul>
<li><p>（1）深度。层数更深，论文中采用了22层。为了避免梯度消失问题，GoogLeNet巧妙地在不同深度处增加了两个损失函数来避免反向传播时梯度消失的现象。</p>
</li>
<li><p>（2）宽度。增加了多种大小的卷积核，如1×1、3×3、5×5，但并没有将这些全都用在特征映射上，都结合起来的特征映射厚度将会很大。但是采用了图6-11右侧所示的降维的Inception模型，在3×3、5×5卷积前，和最大池化后都分别加上了1×1的卷积核，起到了降低特征映射厚度的作用。</p>
</li>
</ul>
<h3 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h3><p>残差网络的更多内容详见Kaiming He、Xiangyu Zhang、Shaoqing Ren和Jian Sun的论文《Deep Residual Learningfor Image Recognition》</p>
<h2 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h2><p>循环神经网络主要是自然语言处理（natural language<br>processing，NLP）应用的一种网络模型。循环神经网络的特点在于它是按时间顺序展开的，下一步会受本步处理的影响。循环神经网络的解决方式是，隐藏层的输入不仅包括上一层的输出，还包括上一时刻该隐藏层的输出。理论上，循环神经网络能够包含前面的任意多个时刻的状态，但实践中，为了降低训练的复杂性，一般只处理前面几个状态的输出。</p>
<p><img src="/images/TF_02_11.jpg" alt="avarat"></p>
<p>循环神经网络的训练也是使用误差反向传播 （backpropagation，BP）算法，并且参数w1、w2和w3是共享的。但是，其在反向传播中，不仅依赖当前层的网络，还依赖前面若干层的网络，这种算法称为随时间反向传播<br>（backpropagation through time，BPTT）算法。BPTT算法是BP算法的扩展，可以将加载在网络上的时序信号按层展开，这样就使得前馈神经网络的静态网络转化为动态网络。</p>
<h2 id="循环神经网络发展"><a href="#循环神经网络发展" class="headerlink" title="循环神经网络发展"></a>循环神经网络发展</h2><p><img src="/images/TF_02_12.jpg" alt="avarat"></p>
<h1 id="Tensorflow高级框架"><a href="#Tensorflow高级框架" class="headerlink" title="Tensorflow高级框架"></a>Tensorflow高级框架</h1><h2 id="TFLearn"><a href="#TFLearn" class="headerlink" title="TFLearn"></a>TFLearn</h2><h3 id="1-加载数据"><a href="#1-加载数据" class="headerlink" title="1.加载数据"></a>1.加载数据</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tflearn</span><br><span class="line"><span class="keyword">from</span> tflearn.layers.core <span class="keyword">import</span> input_data, dropout, fully_connected</span><br><span class="line"><span class="keyword">from</span> tflearn.layers.conv <span class="keyword">import</span> conv_2d, max_pool_2d</span><br><span class="line"><span class="keyword">from</span> tflearn.layers.normalization <span class="keyword">import</span> local_response_normalization</span><br><span class="line"><span class="keyword">from</span> tflearn.layers.estimator <span class="keyword">import</span> regression</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用的是牛津大学的鲜花数据集 [2] （Flower Dataset）。这个数据集提供了17个类别的鲜花数据，每个类别80张图片，并且图片有大量的姿态和光的变化。</span></span><br><span class="line"><span class="keyword">import</span> tflearn.datasets.oxflower17 <span class="keyword">as</span> oxflower17</span><br><span class="line">X, Y = oxflower17.load_data(one_hot=<span class="literal">True</span>, resize_pics=(<span class="number">227</span>, <span class="number">227</span>))</span><br></pre></td></tr></table></figure>

<h3 id="2-构建网络模型"><a href="#2-构建网络模型" class="headerlink" title="2.构建网络模型"></a>2.构建网络模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 构建AlexNet网络</span></span><br><span class="line">network = input_data(shape=[<span class="literal">None</span>, <span class="number">227</span>, <span class="number">227</span>, <span class="number">3</span>])</span><br><span class="line">network = conv_2d(network, <span class="number">96</span>, <span class="number">11</span>, strides=<span class="number">4</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">network = max_pool_2d(network, <span class="number">3</span>, strides=<span class="number">2</span>)</span><br><span class="line">network = local_response_normalization(network)</span><br><span class="line">network = conv_2d(network, <span class="number">256</span>, <span class="number">5</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">network = max_pool_2d(network, <span class="number">3</span>, strides=<span class="number">2</span>)</span><br><span class="line">network = local_response_normalization(network)</span><br><span class="line">network = conv_2d(network, <span class="number">384</span>, <span class="number">3</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">network = conv_2d(network, <span class="number">384</span>, <span class="number">3</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">network = conv_2d(network, <span class="number">256</span>, <span class="number">3</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">network = max_pool_2d(network, <span class="number">3</span>, strides=<span class="number">2</span>)</span><br><span class="line">network = local_response_normalization(network)</span><br><span class="line">network = fully_connected(network, <span class="number">4096</span>, activation=<span class="string">&#x27;tanh&#x27;</span>)</span><br><span class="line">network = dropout(network, <span class="number">0.5</span>)</span><br><span class="line">network = fully_connected(network, <span class="number">4096</span>, activation=<span class="string">&#x27;tanh&#x27;</span>)</span><br><span class="line">network = dropout(network, <span class="number">0.5</span>)</span><br><span class="line">network = fully_connected(network, <span class="number">17</span>, activation=<span class="string">&#x27;softmax&#x27;</span>)</span><br><span class="line">network = regression(network, optimizer=<span class="string">&#x27;momentum&#x27;</span>,</span><br><span class="line">                    loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>,</span><br><span class="line">                    learning_rate=<span class="number">0.001</span>) <span class="comment"># 回归操作，同时规定网络所使用的学习率、损失函数和优化器</span></span><br></pre></td></tr></table></figure>

<h3 id="3-训练模型"><a href="#3-训练模型" class="headerlink" title="3.训练模型"></a>3.训练模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = tflearn.DNN(network, checkpoint_path=<span class="string">&#x27;model_alexnet&#x27;</span>,</span><br><span class="line">                    max_checkpoints=<span class="number">1</span>, tensorboard_verbose=<span class="number">2</span>)</span><br><span class="line">model.fit(X, Y, n_epoch=<span class="number">1000</span>, validation_set=<span class="number">0.1</span>, shuffle=<span class="literal">True</span>,</span><br><span class="line">        show_metric=<span class="literal">True</span>, batch_size=<span class="number">64</span>, snapshot_step=<span class="number">200</span>,</span><br><span class="line">        snapshot_epoch=<span class="literal">False</span>, run_id=<span class="string">&#x27;alexnet_oxflowers17&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="Keras"><a href="#Keras" class="headerlink" title="Keras"></a>Keras</h2><p>特点：</p>
<ul>
<li><p>模块化 ：模型的各个部分，如神经层、成本函数、优化器、初始化、激活函数、规范化都是独立的模块，可以组合在一起来创建模型。</p>
</li>
<li><p>极简主义 ：每个模块都保持简短和简单。</p>
</li>
<li><p>易扩展性 ：很容易添加新模块，因此Keras适于做进一步的高级研究。</p>
</li>
<li><p>使用Python语言：模型用Python实现，非常易于调试和扩展。</p>
</li>
</ul>
<p>Keras的核心数据结构是模型。模型是用来组织网络层的方式。模型有两种，一种叫Sequential模型，另一种叫Model模型。</p>
<ul>
<li><p>Sequential模型是一系列网络层按顺序构成的栈，是单输入和单输出的，层与层之间只有相邻关系，是最简单的一种模型。</p>
</li>
<li><p>Model模型是用来建立更复杂的模型的。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Sequential模型的使用</span></span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Activation</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据</span></span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Dense(output_dim=<span class="number">64</span>, input_dim=<span class="number">100</span>))</span><br><span class="line">model.add(Activation(<span class="string">&quot;relu&quot;</span>))</span><br><span class="line">model.add(Dense(output_dim=<span class="number">10</span>))</span><br><span class="line">model.add(Activation(<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 编译模型，同时指明损失函数和优化器：</span></span><br><span class="line">model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>, optimizer=<span class="string">&#x27;sgd&#x27;</span>, metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型和评估模型：</span></span><br><span class="line">model.fit(X_train, Y_train, nb_epoch=<span class="number">5</span>, batch_size=<span class="number">32</span>)</span><br><span class="line">loss_and_metrics = model.evaluate(X_test, Y_test, batch_size=<span class="number">32</span>)</span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>计算机</tag>
      </tags>
  </entry>
  <entry>
    <title>开宗明义</title>
    <url>/2020/04/14/My-First-Post/</url>
    <content><![CDATA[<h1 id="南薆"><a href="#南薆" class="headerlink" title="南薆"></a>南薆</h1><p><strong>南园薆兮果载荣。——三国魏· 曹植《临观赋》</strong></p>
<p><strong>《诗经•尔雅》中有“薆，隐也”</strong></p>
<p>薆者，隐于草木茂盛间也。这也让我想起毛主席《卜算子•咏梅》中的那句“待到山花烂漫时，她在丛中笑！”十分有幸通过博客的方式与大家见面，希望自己可以通过它记录自己点点滴滴的成长，也希望它对于正在浏览该网站的各位有所裨益。共同收获山花烂漫时！</p>
<blockquote>
<p>南薆 于2020年4月14日下午</p>
</blockquote>
]]></content>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>PointNet</title>
    <url>/2022/03/04/PointNet/</url>
    <content><![CDATA[<p>论文：<a href="https://openaccess.thecvf.com/content_cvpr_2017/html/Qi_PointNet_Deep_Learning_CVPR_2017_paper.html">CVPR 2017 Open Access Repository (thecvf.com)</a></p>
<span id="more"></span>
<h1 id="前人的贡献"><a href="#前人的贡献" class="headerlink" title="前人的贡献"></a>前人的贡献</h1><p>传统的目标检测算法中对于数据格式有着严格的要求，将点云数据转换为符合要求的数据格式会使得数据规模扩大，影响计算效率。</p>
<p>点云数据由<strong>无序</strong>的数据点构成一个<strong>集合</strong>来表示。因此，在使用图像识别任务的深度学习模型处理点云数据之前，需要对点云数据进行一些处理。目前采用的方式主要有两种：</p>
<blockquote>
<p>1、将点云数据投影到二维平面。此种方式不直接处理三维的点云数据，而是先将点云投影到某些特定视角再处理，如<strong>前视视角和鸟瞰视角</strong>。同时，也可以融合使用来自相机的图像信息。通过将这些不同视角的数据相结合，来实现点云数据的认知任务。比较典型的算法有MV3D和AVOD。</p>
<p>2、将点云数据划分到有空间依赖关系的voxel。此种方式通过分割三维空间，引入空间依赖关系到点云数据中，再使用3D卷积等方式来进行处理。这种方法的精度依赖于三维空间的分割细腻度，而且<strong>3D卷积</strong>的运算复杂度也较高。</p>
</blockquote>
<h2 id="点集的性质"><a href="#点集的性质" class="headerlink" title="点集的性质"></a>点集的性质</h2><ol>
<li>无序：三维N个点的数据需要N！个排列组合</li>
<li>点之间相互作用：点云中的点不是独立存在的，模型需要提取局部信息</li>
<li>转换不变性：作为一个几何物体，对物体进行变换不应该改变物体的某些特征</li>
</ol>
<h1 id="论文概述"><a href="#论文概述" class="headerlink" title="论文概述"></a>论文概述</h1><img src="\images\image-20220116120541325.png" alt="image-20220105165859553" style="zoom:100%;" />


<p>PointNet主要用于点云数据分类问题，即在点云数据中找到属于一个物体的所有点云。</p>
<p>该分类网络以n个点作为输入，进行输入和特征转换，然后通过最大池法对点特征进行聚合。输出是k个类别的分类分数。分割网络是分类网络的延伸。它连接全局和局部特征，并输出每个分数。” mlp  “表示多层感知器，括号中的数字表示层大小。Batchnorm用于所有带有ReLU的层。在分类网的最后一个mlp中使用了Dropout层。</p>
<p>PointNet更多的是为CV领域提供了一种新的研究方向，拓展了对于原始数据处理的思路，后续的Frustum PoinNet等是对于PointNet在目标检测方面的特定研究，以及其他网络会将其作为网络设计的一部分充分的发挥其网络结构简单，计算简单的特点。</p>
<h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p> 作用于无序输入的对称函数</p>
<p>不改变模型的输入排序有三种方法：将输入按正则序排序（在高维空间中很难实现）；将输入作为序列来训练RNN（会增加训练数据）；使用一个<strong>对称函数</strong>来聚合每个点的信息（如+、*）</p>
<p>作者的主要思想是通过对一个点集中的变换<br>元素应用一个对称函数来逼近一个定义在点集上的一般函数：<br>$$<br>f({x_1,x_2,…,x_n})\approx g(h(x_1),…,h(x_n))\<br>f:2^{\mathbb{R}^N}\rightarrow \mathbb{R} ,h:\mathbb{R} ^N\rightarrow \mathbb{R} ^K,g:\mathbb{R} ^K\times …\times\mathbb{R} ^K\rightarrow \mathbb{R} (g为对称函数)<br>$$<br>根据经验，使用一个多层感知器网络近似h（mlp），使用一个单变量函数和一个最大池化函数组合近似g。通过h，可以学习到f中的不同性质,最后输出一个k维向量，表示数据的全局特征。</p>
<p> 局部全局的数据增强</p>
<p>根据上面网络得到的k维向量，可以训练SVM或是多层感知分类器对具有全局特征进行分类。</p>
<p>在计算全局点云特征向量后，通过连接全局特征和每个点的特征，将其反馈给每个点。接着基于集合的点特征提取每个点的新特征（此时每个点特征同时拥有局部特征与全局特征）。</p>
<p>之后，网络可以基于局部几何特性和全局语义预测每个点的数量。</p>
<p> 联合定位网络</p>
<p>作者通过一个微型网络T-Net预测一个仿射变换矩阵并且直接将这种变换应用到输入点的坐标上。</p>
<p>这种思想同时适用于特征空间的对齐，通过在点特征上插入另一个对齐网络，并预测一个特征转换矩阵来对齐来自不同输入点云的特征。但是特征空间中的变换矩阵比空间变换矩阵的维数高很多，这会增加优化难度。因此在softmax训练损失中增加一个<strong>正则化项</strong>，将特征变换矩阵约束为接近<strong>正交矩阵</strong>，以获得了较好的效果。<br>$$<br>L_{reg}&#x3D;    \Vert I-AA^T    \Vert^2_F(A是由T-Net预测的特征对齐矩阵)<br>$$</p>
<h2 id="理论研究"><a href="#理论研究" class="headerlink" title="理论研究"></a>理论研究</h2><ol>
<li><p>PointNet神经网络对于连续集函数具有很好的逼近能力。即输入点集的小扰动不会对函数数值造成很大的改变。所以即使在最坏的情况下，网络也可以通过将空间划分为等大小的voxel来探索空间。</p>
<img src="\images\image-20220116233004972.png" alt="image-20220105165859553" style="zoom:100%;" />
</li>
<li><p>即使输入有数据被损坏或是带有噪声，模型都具有鲁棒性；关键集的数据多少由maxpooling操作输出数据的维度K给出上界</p>
</li>
</ol>
<p>因此，该网络通过稀疏的关键点集合来总结一个形状。</p>
<h2 id="关键流程"><a href="#关键流程" class="headerlink" title="关键流程"></a>关键流程</h2><ol>
<li>输入为一帧的全部点云数据的集合，表示为一个n×3的二维 tensor，其中n代表点云数量，3对应xyz坐标。</li>
<li>输入数据先通过和一个T-Net学习到的转换矩阵相乘来对齐，保证了模型的对特定空间转换的不变性。</li>
<li>通过多次mlp对各点云数据进行特征提取后，再用一个T-Net对特征进行对齐。</li>
<li>在特征的各个维度上执行maxpooling操作来得到最终的全局特征。</li>
<li>对分类任务，将全局特征通过mlp来预测最后的分类分数；对分割任务，将全局特征和之前学习到的各点云的局部特征进行串联，再通过mlp得到每个数据点的分类结果。</li>
</ol>
<h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><img src="\images\image-20220116234754228.png" alt="image-20220105165859553" style="zoom:100%;" />


<h1 id="做出的贡献"><a href="#做出的贡献" class="headerlink" title="做出的贡献"></a>做出的贡献</h1><ol>
<li>设计一种新的深度网络结构，适用于三维空间中的无序点集</li>
<li>展示这样的网络如何执行三维形状分类、形状部分分割和场景语义解析任务</li>
<li>对这种方法的稳定性和有效性进行深入的实证和理论分析</li>
<li>举例说明网络中选定的神经元三维特征，研究其性能的直观解释</li>
</ol>
]]></content>
      <tags>
        <tag>论文阅读</tag>
        <tag>deep learning</tag>
        <tag>Object Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>PointPillars</title>
    <url>/2022/03/08/PointPillars/</url>
    <content><![CDATA[<p>论文：<a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Lang_PointPillars_Fast_Encoders_for_Object_Detection_From_Point_Clouds_CVPR_2019_paper.html">CVPR 2019 Open Access Repository (thecvf.com)</a></p>
<span id="more"></span>
<img src="\images\image-20220114222458551.png" alt="image-20220105165859553" style="zoom:100%;" />


<h1 id="论文概述"><a href="#论文概述" class="headerlink" title="论文概述"></a>论文概述</h1><p>本篇文献主要解决将点云编码为适合更适合目标检测的格式的问题。作者没有预训练网络，所有权值随机使用均匀分布。</p>
<h2 id="特征解码网络"><a href="#特征解码网络" class="headerlink" title="特征解码网络"></a>特征解码网络</h2><p>将点云转换为稀疏的伪图像。</p>
<p>不同于voxelNet将整个点云分割为许多的voxel，PointPillar只在xOy面进行划分，整个点云空间被分割为若干个高度为点云图高度（在z方向上近似是无限的，因此不需要超参数控制z维度）的柱体。</p>
<p>与voxel类似，大部分的柱体都是空的。而对于包含点数过多N的柱体采用随机采样，对于包含点数过小的柱体应用零填充。并通过限制非空柱体的数量P，将整个点云用一个尺寸为(D,P,N)的密集张量。</p>
<p>接着使用简化的PointNet，对于每个点应用一个线性层（张量的1×1卷积）、一个BN层、一个ReLU层，产生一个尺寸为(C,P,N)的张量。（C&#x3D;64）然后对其进行max操作，创建一个尺寸为(C,P)的输出张量（将三维点云转换为二维数据）。因为作者是使用的柱体而非voxel，所以可以在卷积中间层避免三维卷积，极大的提升计算效率。</p>
<p>编码后特征被分散到原始位置，以创建一个大小为(C,H,W)的伪图像（H，W为画布的高度和宽度）    </p>
<h2 id="二维卷积骨干网络"><a href="#二维卷积骨干网络" class="headerlink" title="二维卷积骨干网络"></a>二维卷积骨干网络</h2><p>将伪图像转换为高层的表示。该网络又可以分解为两个小的网络</p>
<ol>
<li>自上而下网络：在越来越小的空间分辨率上产生特征</li>
<li>对自上而下的特征进行上采样和连接</li>
</ol>
<p>作者使用Block(S,L,F)来表示自顶向下的主干，每个block的操作步长为S（与初始的伪图像大小有关），每块都有一个二维卷积层L和输出通道F，以及一个BN层、一个ReLU层。</p>
<p>第一层卷积步长为S&#x2F;Sin，<em>以确保block在执行步长为Sin卷积操作后还可以执行步长为S的操作。</em>（对于汽车S&#x3D;2，对于行人、自行车S&#x3D;1）（The first convolution inside the layer has stride S&#x2F;Sin to ensure the block operates on tride S after receiving an input blob of stride Sin）。后续block上的卷积步长为1.</p>
<p>最终从每个自顶向下block得到的特征通过一定的上采样和拼接进行组合：</p>
<ol>
<li>对特征进行上采样Up(Sin,Sout,F)，初始步长Sin，最终步长Sout，利用转置二维卷积和F得到特征</li>
<li>使用 BatchNorm和ReLU 应用上采样特征，最终的输出特征是从不同步长级联得到的特征</li>
</ol>
<h2 id="Detection-Head"><a href="#Detection-Head" class="headerlink" title="Detection Head"></a>Detection Head</h2><p>检测三维box，并进行回归。这里的 Detection Head是模块化的，即对于不同的任务可以使用不同的 Detection Head。就像使用不同的镜头来拍摄不同的照片。</p>
<p>作者这里使用 Single Shot Detector (SSD)以来处理三维目标检测。同时，使用二维的IoU将ground truth与先验box相匹配。而将box高度与高度elevation作为额外的回归目标</p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>回归残差：<br>$$</p>

\Delta x=\frac{x^{gt}-x^a}{d^a},
\Delta y=\frac{y^{gt}-y^a}{d^a},
\Delta z=\frac{z^{gt}-z^a}{d^a}\quad(d=\sqrt{(w^a)^2+(l^a)^2});\\
\Delta l=\log{\frac{l^{gt} }{l^a} },
\Delta w=\log{\frac{w^{gt} }{w^a} },
\Delta h=\log{\frac{h^{gt} }{h^a} },
\Delta\theta=\theta^{gt}-\theta^a;

<p>$$<br>定位损失：由于角度定位无法区分翻转的box，在离散化方向上使用softmax分类损失Ldir学习车辆前进方向<br>$$<br>L_{loc}&#x3D;\sum_{b\in(x,y,z,w,l,\theta)}SmoothL1(\Delta b)<br>$$<br>目标分类损失：p^a是锚点是类的概率，α&#x3D;0.25，γ&#x3D;2<br>$$<br>L_{cls}&#x3D;-\alpha_a(1-p^a)^\gamma\log{p^a},<br>$$</p>
<p>总损失函数：Npos为正锚点的数量、βloc&#x3D;2、βcls&#x3D;1、βdir&#x3D;0.2<br>$$</p>

L=\frac{1}{N_{pos} }(\beta_{loc}L_{loc}+\beta_{cls}L_{cls}+\beta_{dir}L_{dir}),

<p>$$<br>损失函数是以哦那个初始学习率为0.0002的Adam进行优化，每15个epoch减少0.8倍。用于验证与测试的epoch个数分别为160、320，batch大小分别为2、4.</p>
<h2 id="超参数设置"><a href="#超参数设置" class="headerlink" title="超参数设置"></a>超参数设置</h2><ul>
<li>xy分辨率：0.16m</li>
<li>点柱最大数量P：12000</li>
<li>点柱内最多点数N：100</li>
<li>轴对齐非极大抑制NMS的IoU阈值：0.5</li>
</ul>
<h3 id="汽车检测任务："><a href="#汽车检测任务：" class="headerlink" title="汽车检测任务："></a>汽车检测任务：</h3><ul>
<li><p>x,y,z检测范围：[(0, 70.4), (-40, 40), (-3, 1)]</p>
</li>
<li><p>锚点宽、长、高：(1.6, 3.9, 1.5)m，z中心：-1m</p>
</li>
<li><p>匹配正负阈值：0.6、0.45</p>
</li>
</ul>
<h3 id="行人、自行车检测任务："><a href="#行人、自行车检测任务：" class="headerlink" title="行人、自行车检测任务："></a>行人、自行车检测任务：</h3><ul>
<li><p>x,y,z检测范围：[(0, 48), (-20, 20), (-2.5, 0.5)]</p>
</li>
<li><p>行人锚点宽、长、高：(0.6, 0.8, 1.73)m，z中心：-0.6m</p>
</li>
<li><p>自行车锚点宽、长、高：(0.6, 1.76, 1.73)m，z中心：-0.6m</p>
</li>
<li><p>匹配正负阈值：0.5，0.35</p>
</li>
</ul>
<h2 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h2><ol>
<li>类似SECOND，创建一个查找表，其中包含所有类别的ground truth 3D boxes以及box中相关联的点云；对于每个样本随即选择若干个汽车、行人、自行车的真实样本将其放入当前环境中以提升对于不同环境下目标检测的能力</li>
<li>对所有 <strong>ground truth boxes独立的旋转、转换</strong>，进一步丰富训练集。</li>
<li>执行两个全局增强集合：随机沿x轴翻转并执行<strong>全局旋转和放缩</strong>、使用N(0,0,2)<strong>模拟噪声</strong>对x、y、z坐标转换</li>
</ol>
<h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><img src="\images\image-20220115183322884.png" alt="image-20220105165859553" style="zoom:100%;" />


<p>实验发现该网络对于行人的检测仍有一些不足，行人与自行车会被误认为彼此。行人也容易被混淆为狭窄的带有垂直特性的物体，如树干、电线杆。</p>
<p>推理速度快也是该网络的一大优势，总的运行时间：16.2ms。主要推理过程如下：（ Intel i7 CPU and a 1080ti GPU）</p>
<ol>
<li>根据图像的可见性、范围加载、过滤点云1.4ms</li>
<li>将点分配到点柱并进行处理2.7ms</li>
<li>将点柱张量加载进GPU2.9ms、<strong>编码1.3ms</strong>、分散为伪图像0.1ms</li>
<li>由卷积骨干网、检测头处理7.7ms</li>
<li>NMS处理0.1ms（使用CPU）</li>
</ol>
<p><strong>编码阶段</strong>是该网络运行时间少的关键，VoxelNet的解码时间190ms、SECOND的编码时间50ms。同时该网络只使用一个PointNet网络进行编码，将pytorch的运行时间减少了2.5ms。将第一个block的尺寸缩减到64以匹配解码输出的尺寸，并将上采样特征层输出尺寸减半到128，这些都大幅减少了运行时间。</p>
<p>实验证明，当推理速度达到105Hz时，准确率只减少了一点。相比之下，激光雷达的工作频率为20Hz。但是需要注意的是，现在的实验是使用桌面级GPU，如果应用于实际，使用嵌入式GPU，计算效率会下降；一个可操作的AV需要查看完整的环境并处理全部点云，而实验使用的KITTI数据集中，只会使用10%的点云数据。实际中需要计算的数据量有很大区别。</p>
<h3 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h3><p>作者通过一系列消融实验得到以下结论：</p>
<ol>
<li>好的编码器明显优于固定的编码器，特别是对于更大的分辨率。</li>
<li>box增强并不会带来更大提升，反而在检测行人方面导致性能下降</li>
<li>更小的点柱使得定位更准确以及学习的特征更多，更大的点柱计算速度更快（更少的非空点柱）</li>
</ol>
<h1 id="做出的贡献"><a href="#做出的贡献" class="headerlink" title="做出的贡献"></a>做出的贡献</h1><ol>
<li>提出一个新的基于点云解码器和网络PointPliiar，适用于端到端的基于点云的三维目标检测网络的训练</li>
<li>将对点云的柱上的计算变为密集的二维卷积，使得推理速率到达62Hz</li>
<li>在KITTI数据集上的实验，该网络表现出对于汽车、自行车、行人检测最先进的结果</li>
<li>通过消融实验 ablation studies，发现对检测性能起到关键影响的因素</li>
<li>作者提出的点柱偏移Xp、Yp以及簇偏移Xc、Yc、Zc带来更好的检测效果</li>
</ol>
]]></content>
      <tags>
        <tag>论文阅读</tag>
        <tag>deep learning</tag>
        <tag>Object Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>PointRCNN</title>
    <url>/2022/03/08/PointRCNN/</url>
    <content><![CDATA[<p>论文：<a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Shi_PointRCNN_3D_Object_Proposal_Generation_and_Detection_From_Point_Cloud_CVPR_2019_paper.html">CVPR 2019 Open Access Repository (thecvf.com)</a></p>
<span id="more"></span>
<h1 id="论文概述"><a href="#论文概述" class="headerlink" title="论文概述"></a>论文概述</h1><img src="\images\image-20220218103739679.png" alt="image-20220105165859553" style="zoom:100%;" />


<p>本篇论文使用两阶段的三维目标检测框架，并且直接应用于三维点云，实现了强大和准确的三位检测性能。</p>
<h2 id="第一阶段"><a href="#第一阶段" class="headerlink" title="第一阶段"></a>第一阶段</h2><p>第一阶段是生成自底向上的三维包围盒方案，分割前景，同时从分割点生成数量较少的box proposal，节省大量的计算量。具体来说，通过学习逐点特征来分割原始点云，并同时从分割的前景点生成3D提案。</p>
<p>对于训练集中的每个3D点云场景，我们从每个场景中抽取16,384个点作为输入。对于点数小于16,384的场景，我们随机重复这些点数，得到16,384点。对于stage-1子网络，我们遵循[28]的网络结构，其中使用四个具有多尺度分组的集抽象层，将点分组为大小为4096、1024、256、64的组。然后使用四个特征传播层来获取点特征向量，用于分割和生成建议。</p>
<p>鉴于骨干点云网络编码的逐点特征，通过附加一个分割头用于估计前景掩模和一个框回归头用于生成3D提案。对于点分割，ground-truth分割蒙版自然是由3D  ground-truth box提供的。对于大型户外场景，前景点的数量一般要比背景点的数量少得多。因此，我们使用焦点损失[19]来处理类不平衡问题<br>$$<br>L_{focal}(p_t)&#x3D;-\alpha_t(1-p_t)^\gamma\log(p_t),\<br>p_t&#x3D;\begin{cases} p \qquad for forground point \1-p \qquad otherwise\end{cases}<br>$$</p>
<h2 id="第二阶段"><a href="#第二阶段" class="headerlink" title="第二阶段"></a>第二阶段</h2><p>第二阶段进行规范的三维box框细化，生成proposal后，采用点云池化，将第一阶段学习到的点进行池化。</p>
<p>在LiDAR坐标系中，三维包围框表示为(x, y, z, h, w, l， θ)，其中(x, y, z)为目标中心位置，(h, w,  l)为目标大小，θ为从鸟瞰目标方向。为了约束生成的3D框建议，提出了基于bin的回归损失来估计对象的3D边界框。<br><img src="\images\image-20220218183400653.png" alt="image-20220105165859553" style="zoom:100%;" /></p>
<p>Fcls为交叉分类损失；Freg为平滑L1损失</p>
<p>同时为了消除冗余提案，通过基于鸟瞰图进行非最大抑制(non - maximum suppression,  NMS)，生成少量高质量提案。在训练方面，使用0.85作为IoU阈值，在NMS之后，保留stage-2子网培训建议的top  300的proposals。推理过程中，采用NMS，IoU阈值为0.8，只保留前100个建议对阶段2子网进行细化</p>
<p>对于框提案细化子网络，网络从每个提案的集合区域随机抽取512个点作为细化子网络的输入。使用三个单尺度分组集合抽象层(分组大小分别为128、32、1)生成单个特征向量，用于对象置信度分类和建议位置优化。</p>
<h2 id="点云区域池化"><a href="#点云区域池化" class="headerlink" title="点云区域池化"></a>点云区域池化</h2><p>在获得3D包围盒提案后，目标是在之前生成的box proposal的基础上细化box的位置和方向。为了了解每个方案更具体的局部特征，建议根据每个3D方案的位置，从stage-1集合3D点及其对应的点特征。</p>
<p>对于每个三维box，作者都会稍微放大尺寸得到一个新的三维box，从上下文编码额外的信息</p>
<p>对于每个点，通过内外测试确定点是否再扩大的box proposal中。</p>
<h2 id="规范的3D-box细化"><a href="#规范的3D-box细化" class="headerlink" title="规范的3D box细化"></a>规范的3D box细化</h2><h3 id="正则变换"><a href="#正则变换" class="headerlink" title="正则变换"></a>正则变换</h3><p>正则变换遵守如下规则：</p>
<ol>
<li>原点位于方框的中心</li>
<li>局部的X‘和Z’轴近似平行于地平面，X‘指向提案的头部方向，另一个Z‘轴垂直于X’</li>
<li>Y ’轴与激光雷达坐标系保持一致。<img src="\images\image-20220220120505467.png" alt="image-20220105165859553" style="zoom:100%;" /></li>
</ol>
<h3 id="改进box-proposal的特征学习"><a href="#改进box-proposal的特征学习" class="headerlink" title="改进box proposal的特征学习"></a>改进box proposal的特征学习</h3><p>细化子网络结合了变换后的局部空间点(特征)以及从阶段1进行进一步的盒和置信度细化得到的全局语义特征。</p>
<p>虽然正则变换能够实现鲁棒的局部空间特征学习，但它不可避免地会丢失每个对象的深度信息。为了补偿丢失的深度信息，将点到传感器的距离特征加入特征点p中。</p>
<p>对于每个提议，其关联点的局部空间特征和额外的特征首先连接并馈送给几个全连接层，将其局部特征编码为相同维的全局特征。然后将局部特征和全局特征串联并馈送到一个pointNet++结构的网络中，得到一个判别特征向量，用于后续的置信度分类和盒体细化。</p>
<h3 id="改进box-proposal的损失函数"><a href="#改进box-proposal的损失函数" class="headerlink" title="改进box proposal的损失函数"></a>改进box proposal的损失函数</h3><p>使用基于bin的回归损失来改进proposal，如果IoU大于0.55，则将ground-truth box 分配给三维box proposal，用于学习box的改进。（三维box proposal及其对应的ground-truth box 都被转换为标准坐标系）<br>$$</p>

L_{refine}=\frac{1}{\vert\vert B\vert\vert}\sum_{i\in B}F_{cls}(prob_i,label_i)+\frac{1}{\vert\vert B_{pos}\vert\vert}\sum_{i\in B_{pos} }(\hat{L}^{(i)}_{bin}+\hat{L}^{(i)}_{res})

<p>$$</p>
<p>B是阶段1的3D提案集合，Bpos存储回归的正提案，probi是bi的估计置信度，labeli是相应的标签，</p>
<p>对于box偏转方向，则将ground-truth box 与三维box proposal的IoU阈值为0.55。</p>
<h1 id="做出的贡献"><a href="#做出的贡献" class="headerlink" title="做出的贡献"></a>做出的贡献</h1><ol>
<li>提出了一种基于点云的自底向上的三维包围盒提案生成算法，该算法通过将点云分割成前景对象和背景，生成少量高质量的三维提案。从分割中学习到的点表示不仅擅长于提议的生成，而且对后续的框细化也有帮助。</li>
<li>所提出的规范3D包围盒细化利用了从阶段1生成的高召回量盒建议，并学会了在规范坐标中预测基于稳健盒基损耗的盒坐标细化。</li>
<li>提出的三维检测框架PointRCNN在仅使用点云作为输入的情况下，显著优于目前最先进的方法</li>
</ol>
]]></content>
      <tags>
        <tag>论文阅读</tag>
        <tag>deep learning</tag>
        <tag>Object Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>RCNN</title>
    <url>/2022/02/25/RCNN/</url>
    <content><![CDATA[<p><strong>论文：<a href="https://openaccess.thecvf.com/content_cvpr_2014/html/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.html">CVPR 2014 Open Access Repository (thecvf.com)</a></strong></p>
<span id="more"></span>

<h2 id="论文概述"><a href="#论文概述" class="headerlink" title="论文概述"></a>论文概述</h2><h3 id="RCNN算法4个步骤"><a href="#RCNN算法4个步骤" class="headerlink" title="RCNN算法4个步骤"></a>RCNN算法4个步骤</h3><ol>
<li>候选区域生成： 一张图像生成1K~2K个候选区域 （采用Selective Search 方法）</li>
<li>特征提取： 对每个候选区域，使用深度卷积网络提取特征 （CNN）</li>
<li>类别判断： 特征送入每一类的SVM 分类器，判别是否属于该类</li>
<li>位置精修： 使用回归器精细修正候选框位置</li>
</ol>
<h3 id="RCNN网络结构（三个模块）"><a href="#RCNN网络结构（三个模块）" class="headerlink" title="RCNN网络结构（三个模块）"></a>RCNN网络结构（三个模块）</h3><ol>
<li>第一模块提出独立类别的区域建议，定义可供检测器使用的候选测试集</li>
<li>第二模块大型卷积神经网络，从每个区域提取固定长度的特征向量</li>
<li>第三模块一组特定类别的线性支持向量机</li>
</ol>
<h3 id="区域建议-region-proposals"><a href="#区域建议-region-proposals" class="headerlink" title="区域建议(region proposals)"></a>区域建议(region proposals)</h3><p>region proposals就是从图像中选取2k个候选区域的过程.</p>
<p>现有的生成策略独立的region proposals： objectness, selective search ,category-independent object proposals , constrained parametric min-cuts (CPMC), multiscale combinatorial grouping </p>
<p>在本篇论文中，作者使用<strong>selective search方法</strong></p>
<p><strong>主要思想</strong>：</p>
<ol>
<li>使用一种过分割手段，将图像分割成小区域 (1k~2k 个)</li>
<li>查看现有小区域，按照合并规则合并可能性最高的相邻两个区域。重复直到整张图像合并成一个区域位置</li>
<li>输出所有曾经存在过的区域，所谓候选区域</li>
</ol>
<p><strong>合并策略</strong>：优先合并以下四种区域：颜色（颜色直方图）相近的；纹理（梯度直方图）相近的；合并后总面积小的： 保证合并操作的尺度较为均匀，避免一个大区域陆续“吃掉”其他小区域；合并后，总面积在其BBOX中所占比例大的： 保证合并后形状规则。</p>
<p>在具体的测试中，对于图像中的所有得分区域，我们应用贪婪的非最大抑制（对每个类独立）。如果该区域与得分高于学习阈值的选定区域有交叉合并（IoU）重叠，则拒绝该区域。</p>
<p>由于所有CNN参数在所有类别中共享，并且与其他常见算法相比，CNN的特征向量是低维的。因此相比与诸如UVA检测系统，CNN网络计算区域建议和特征花费的时间与所耗内存都有极大的优化。并且RCNN可以扩展到数以千计的对象类，而无需借助近似技术。</p>
<h3 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h3><p>作者使用了Krizhevsky等人描述的CNN的<strong>Caffe</strong>实现，从每个区域提取一个<strong>4096维</strong>的特征向量，将一个图像去均值的227  × 227的RGB图像通过5个卷积层和2个完全连通层前向传播来计算特征。因此，必须先将该区域的图像数据转换为与CNN兼容的形式（无论候选区域大小或宽高比如何，直接转换为227*227大小,在warp前作者还会对box进行扩张,使得在wrap处的box中有p个像素）</p>
<p>在实验中,作者发现RCNN可以扩展到数以千计的对象,而无需使用近似技术.</p>
<h3 id="有监督预训练"><a href="#有监督预训练" class="headerlink" title="有监督预训练"></a>有监督预训练</h3><p>作者使用开源的Caffe CNN库来进行预训练.</p>
<h3 id="特定领域微调fine-tuning"><a href="#特定领域微调fine-tuning" class="headerlink" title="特定领域微调fine-tuning"></a>特定领域微调fine-tuning</h3><p>为了使RCNN适应新的任务和新的领域，作者的随机梯度下降SGD训练的CNN参数仅来自于VOC数据集。分类器是随机初始化的21路分类层（VOC中的20个类与背景），其他CNN架构没有改变。</p>
<p>分类器会将IoU大于等于0.5的 region proposals，视为积极的，其余则视为消极的。</p>
<p>SGD学习率为0.001，并且在每次SGD迭代中，使用32个正样本，与96个背景样本，组成一个128大小的batch。同时为了使结果更好预测正样本，采样也偏向正样本。</p>
<h3 id="对象类别分类"><a href="#对象类别分类" class="headerlink" title="对象类别分类"></a>对象类别分类</h3><p>作者在文章中用检测的汽车的例子，如果使用二分类器检测汽车，那么一个紧紧包围汽车的box是正例子，而与汽车无关的背景区域是反面例子。现在的问题在于，我们如何去标记一个部分包含汽车的例子。作者在此使用0.3的IoU阈值，只有大于0.3IoU的区域才是积极。作者同时强调，这个阈值的设置对整个算法结果的影响极大。</p>
<p>同时为了解决训练数据过大的问题，作者使用standard hard negative mining method技术，该技术在实验中，只需要遍历所有图像一次就可以使mAP停止增长。</p>
<blockquote>
<p>standard hard negative mining method：</p>
<p>用hard negative的样本反复训练，初始的样本保证一定的正负样本比例。在每次训练中，将预测为positive的负样本（即hard negative样本）加入负样本训练集中。</p>
</blockquote>
<p>作者在补充材料中说明了使用SVM作为分类器的原因，SVM与CNN对于正负样本的定义不同，导致CNN的分类效果不如SVM。</p>
<h3 id="过滤器First-layerfilters"><a href="#过滤器First-layerfilters" class="headerlink" title="过滤器First-layerfilters"></a>过滤器First-layerfilters</h3><p>作者使用了一种简单的非参数的反卷积方法捕捉有方向的边缘和对立的颜色。</p>
<p>主要思想：在网络中挑选出一个特定的单元（特征），并将其作为自身的对象检测i器。也就是，我们在一个大规模的held-out region proposals上计算单元的激活情况，并按得分由高到低排序，通过执行非极大值抑制nonmaximum suppression，使得被选中的区域“不言自明”。</p>
<h3 id="pool5层"><a href="#pool5层" class="headerlink" title="pool5层"></a>pool5层</h3><p>pool5的 feature map是9216维（6×6×256）的，从实验结果来看仅使用pool5的效果不如加入fc6、fc7效果好。作者认为这是因为目标检测的过程中，一些经过分类调整的特征与形状、纹理、颜色等在全连接层处理过后会更好的将这些特征融合学习。</p>
<p>fc6是pool5的全连接层，为了计算特征，它将4096×9216的权重矩阵乘以pool5的feature map，再添加一个偏差向量。</p>
<p>而fc7则是将fc6的输出作为输入，乘以4096×4096的权重矩阵并添加一个偏差矩阵。</p>
<p>作者还进行了多组对照实验得到了许多令人感到意外的结论：</p>
<ol>
<li>在不进行微调的情况下，fc7的结果反而不如fc6，这表明了在不降低mAP的情况下有29%的CNN参数可以被去除。并且去除了两个全连接层的结果也是可以接受的，需要注意的是，此时只使用pool5（即仅6%的参数）。说明对目标检测结果有效的参数大多数来源于卷积层，而不是全连接层。</li>
<li>进行微调的情况下，微调普遍使实验结果提高了8%，并且在全连接层上的微调要比在pool5上的微调更有效果。这说明pool5学习的特征是通用的，微调的大部分改进是通过学习特定领域的非线性分类器获得的。</li>
</ol>
<h3 id="边界框回归"><a href="#边界框回归" class="headerlink" title="边界框回归"></a>边界框回归</h3><p>作者通过训练一个线性回归模型来预测一个新的检测窗口，用于selective search的区域建议。</p>
<h2 id="做出的贡献"><a href="#做出的贡献" class="headerlink" title="做出的贡献"></a>做出的贡献</h2><p>这篇论文首次表明，与基于更简单的hog特征的系统相比，CNN可以在PASCAL VOC上带来更高的对象检测性能。并且主要解决一下两个问题</p>
<ol>
<li>将高容量卷积神经网络(cnn)应用于<strong>自底向上的区域</strong>建议，以定位和分割对象</li>
</ol>
<p>其中为了定位物体localizing onbject，传统方法一是使用回归方法（但是在实践中的表现并不好），二是构建滑动窗口检测器（但是由于网络中接受域与step过大，图像的精确定位存在困难）</p>
<p>因此作者使用“区域识别”模式解决CNN定位问题。在实验中，作者的方法会先将输入图像划分为2k个类别无关的区域，使用CNN从每个区域提取固定长度的特征向量，然后使用类别特定的线性支持向量机SVM对每个区域进行分类</p>
<ol start="2">
<li>当标注的训练数据稀缺时，对辅助任务进行有监督的预训练，然后进行领域特定的微调，可以产生显著的性能提升。</li>
</ol>
]]></content>
      <tags>
        <tag>论文阅读</tag>
        <tag>deep learning</tag>
        <tag>Object Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>second</title>
    <url>/2022/03/08/second/</url>
    <content><![CDATA[<p>论文：<a href="https://www.mdpi.com/1424-8220/18/10/3337">Sensors | Free Full-Text | SECOND: Sparsely Embedded Convolutional Detection (mdpi.com)</a></p>
<span id="more"></span>
<h1 id="前人贡献"><a href="#前人贡献" class="headerlink" title="前人贡献"></a>前人贡献</h1><p>使用<strong>RGB-D数据</strong>的二维表示的方法分为基于鸟瞰图、基于前景两种。</p>
<h2 id="Front-View-and-Image-Based-Methods"><a href="#Front-View-and-Image-Based-Methods" class="headerlink" title="Front-View- and Image-Based Methods"></a>Front-View- and Image-Based Methods</h2><p>在一般的<strong>基于图像</strong>的方法中，先生成二维box类语义、实例语义，再使用手工方法生成特征图。另一种方法使用CNN从图像中估计3Dbox，并使用专门设计的离散连续CNN估计物体运动方向。</p>
<p>对于基于激光雷达数据的方法包括将点云转换为前景的2D map，并应用2D探测器对前景视图中的图像进行定位、和其他方法相比，这些方法在BEV检测和三维检测方面都做得很差。</p>
<p>代表论文：<a href="https://openaccess.thecvf.com/content_cvpr_2017/html/Mousavian_3D_Bounding_Box_CVPR_2017_paper.html">CVPR 2017 Open Access Repository (thecvf.com)</a>、[<a href="https://arxiv.org/abs/1608.07916">1608.07916] Vehicle Detection from 3D Lidar Using Fully Convolutional Network (arxiv.org)</a></p>
<h2 id="Bird’s-Eye-View-Based-Methods"><a href="#Bird’s-Eye-View-Based-Methods" class="headerlink" title="Bird’s-Eye-View-Based Methods"></a>Bird’s-Eye-View-Based Methods</h2><p>这种方法将点云数据转换为<strong>多个切片</strong>得到height maps（按不同高度划分），再将height maps与intensity map、density map 结合得到多通道特征。这种方法的问题是在生成BEV图时，许多数据点被丢弃，导致垂直轴上信息损失很大，这种信息丢失会严重影响在3Dbox回归中的性能</p>
<p>如MV3D（首个将点云数据转换为BEV的方法）；ComplexYOLO使用YOLO网络和复杂角度编码方法来提高速度和定位性能、但在预测3D边界框时只能固定高度）；</p>
<p>代表文章：<a href="https://openaccess.thecvf.com/content_CVPRW_2019/html/WAD/Simon_Complexer-YOLO_Real-Time_3D_Object_Detection_and_Tracking_on_Semantic_Point_CVPRW_2019_paper.html">CVPR 2019 Open Access Repository (thecvf.com)</a>、<a href="https://openaccess.thecvf.com/content_CVPRW_2019/html/WAD/Simon_Complexer-YOLO_Real-Time_3D_Object_Detection_and_Tracking_on_Semantic_Point_CVPRW_2019_paper.html">CVPR 2019 Open Access Repository (thecvf.com)</a></p>
<h2 id="3D-Based-Methods"><a href="#3D-Based-Methods" class="headerlink" title="3D-Based Methods"></a>3D-Based Methods</h2><p>多数的3D-based方法或者<strong>直接使用</strong>点云数据、或者将数据转换为3Dvoxel（而不是BEV），然后采用一种<strong>卷积式的投票算法</strong>进行检测。这种方法利用点云数据的稀疏性，以特征中心的投票方案提高计算速度。但是是使用<strong>手工制作</strong>特征方式，无法适应自动驾驶的复杂环境。</p>
<p>之后又有人提出使用<strong>CNN网络、k-领域</strong>等方法从点云中学习局部空间信息。但是这些方法不能应用于大规模的点，需要用图像检测结果对原始数据点进行滤波。</p>
<p>CNN网络应用到点云也是目前的研究热门，其基本思想是基于CNN的检测器将点云转换为voxel，有下列一些方向：</p>
<ol>
<li>将点云数据离散为二值的voxel，然后进行三维卷积</li>
<li>将点云数据分组为voxel，提取voxel特征，再将这些特征转换为密集张量，利用3D或2D卷积网络进行处理</li>
</ol>
<p>这种方法的主要问题是3D CNN的高计算成本，而且3D CNN的计算复杂度随着voxel分辨率的增加而增加。因此，使用稀疏结构的卷积网络会降低计算复杂度。而 <a href="https://openaccess.thecvf.com/content_cvpr_2018/html/Graham_3D_Semantic_Segmentation_CVPR_2018_paper.html">CVPR 2018 Open Access Repository (thecvf.com)</a>提出了一种空间结构不变的3D CNN。这种网络已经应用于三维语义分割任务，但是还没有利用稀疏卷积进行检测的方法。</p>
<p>代表论文：[<a href="https://arxiv.org/abs/1505.02890">1505.02890] Sparse 3D convolutional neural networks (arxiv.org)</a>、<a href="https://openaccess.thecvf.com/content_cvpr_2018/html/Graham_3D_Semantic_Segmentation_CVPR_2018_paper.html">CVPR 2018 Open Access Repository (thecvf.com)</a></p>
<h2 id="Fusion-Based-Methods"><a href="#Fusion-Based-Methods" class="headerlink" title="Fusion-Based Methods"></a>Fusion-Based Methods</h2><p>这种方法将<strong>相机图像与点云</strong>相结合。</p>
<ol>
<li>使用3维的RPN两个尺度不同的接受域产生三维proposal，然后将每个三维proposal的深度数据反馈到三维CNN并且将相应的二维的颜色补充到二维CNN网络来预测最终结果。</li>
<li>将点云数据转换为一个正视图和一个BEV，再从这两个图中提取特征图与图像特征图融合。但是它含有三个CNN网络并不适用于小心对象</li>
<li>将图像与BEV结合，使用一种新的结构生成高分辨率的特征图的三位对象proposal</li>
<li>使用二维检测结果过滤点云，PointNet就可以应用于三维box</li>
</ol>
<p>这些方法需要处理大量的数据，因此基于融合的方法运行缓慢。并且它对激光雷达的时间同步和校准摄像机的额外要求限制这种方法的使用环境，降低了鲁棒性。</p>
<p>代表论文：<a href="https://openaccess.thecvf.com/content_cvpr_2017/html/Chen_Multi-View_3D_Object_CVPR_2017_paper.html">CVPR 2017 Open Access Repository (thecvf.com)</a></p>
<h1 id="论文概述"><a href="#论文概述" class="headerlink" title="论文概述"></a>论文概述</h1><h2 id="SECOND-Detector"><a href="#SECOND-Detector" class="headerlink" title="SECOND Detector"></a>SECOND Detector</h2><p>SECOND Detector以原始点云作为输入，将其转换为voxel特征和坐标，并应用两个VFE层和一个线性层。然后使用稀疏CNN。最后应用RPN生成检测结果。</p>
<img src="\images\image-20220111213034747.png" alt="image-20220105165859553" style="zoom:100%;" />


<p> 作者的Point Cloud Grouping、Voxel-wise Feature Extractor与VoxelNet的处理相同此处便不再赘述。</p>
<h3 id="稀疏卷积网络"><a href="#稀疏卷积网络" class="headerlink" title="稀疏卷积网络"></a>稀疏卷积网络</h3><p>作者的主要改进体现在引入了<strong>稀疏卷积网络</strong>，替代voxelNet中的三维卷积提取特征图。常规的稀疏卷积是如果没有相关的输入点，则不计算输出点。子簇卷积（常规卷积网络的替代）限制当且仅当相应的输入位置处于活动状态是，输出位置才处于活动状态，这可以避免生成过多的活动点，提升卷积速度。</p>
<h3 id="稀疏卷积算法"><a href="#稀疏卷积算法" class="headerlink" title="稀疏卷积算法"></a>稀疏卷积算法</h3><ol>
<li><p>将稀疏的<strong>输入特征</strong>通过gather操作获得<strong>密集的gather特征；</strong></p>
</li>
<li><p>然后使用GEMM对<strong>密集的gather特征</strong>进行卷积操作，获得<strong>密集的输出特征；</strong></p>
</li>
<li><p>通过预先构建的<strong>输入-输出索引规则矩阵</strong>，将<strong>密集的输出特征</strong>映射到<strong>稀疏的输出特征</strong>。</p>
</li>
</ol>
<img src="\images\image-20220112105038074.png" alt="image-20220105165859553" style="zoom:100%;" />


<p>二维密集卷积算法中，W表示过滤元素，D表示图像元素。函数P(x,y)需要根据输出位置来计算输入位置。因此，卷积输出Y计算如下：<br>$$</p>

Y_{x,y,m}=\sum_{u,v\in P(x,y)}{\sum_{l}{W_{u-u_0,v-v_0,l,m}D_{u,v,l} } }\quad(1)

<p>$$<br>基于<strong>矩阵乘法GEMM算法</strong>可用于收集全部用于构建矩阵的数据，并执行GEMM本身。<br>$$</p>

Y_{x,y,m}={\sum_{l}{W_{*,l,m}\tilde{D}_{P(x,y),l} } }\quad(2)
{% rendrawaw %}
<p>$$<br>此处的W与上式的W相同，只是<strong>使用GEMM形式</strong>。对于稀疏数据D‘和相关联的输出Y’直接计算算法如下：<br>$$</p>
{% raw %}
Y_{x,y,m}=\sum_{i\in P'(j)}{\sum_{l}{W_{k,l,m}D'_{i,l} } }\quad(3)
{% endraw %}
<p>$$<br>其中p‘是获取输入索引和滤波器偏移量的函数。<strong>基于GEMM的版本</strong>为<br>$$</p>
{% raw %}
Y’_{j,m}={\sum_{l}{W_{*,l,m}\tilde{D'}_{P'(j),l} } }\quad(4)
{% endraw %}
<p>$$<br>因为D’中含有大量的零不用参与计算，因此引入<strong>规则矩阵R</strong>，指定输入索引i给出核偏移量k和输出索引j，公式如下：<br>$$</p>
{% raw %}
Y’_{j,m}=\sum_k{\sum_{l}{W_{k,l,m}\tilde{D'}_{R_{k,j},k,l} } }\quad(5)
{% endraw %}
<p>$$<br>而5式的inner sum无法通过GEMM的计算，因此还需要收集足够的数据构建矩阵来执行GEMM，再将数据分散回去。实际中可以利用预先构造的<strong>输入-输出索引规则矩阵</strong>从原始稀疏矩阵数据中收集数据</p>
<h3 id="生成规则算法"><a href="#生成规则算法" class="headerlink" title="生成规则算法"></a>生成规则算法</h3><p>常见的哈希表规则生成算法是基于CPU的，速度较慢，并且需要再CPU和GPU间进行数据传输。另一种方法是<strong>迭代输入点</strong>，找到每个输入点相关的输出，并将相应的索引存储到规则中。在迭代的过程中，需要使用一张表检查每个输出位置的存在性以决定是否使用全局输出索引计数器来累加数据，这也是制约并行计算在算法中使用的最大挑战。</p>
<p>作者设计了一种<strong>基于GPU的规则生成算法</strong>。</p>
<ol>
<li><strong>收集输入的索引和对应的空间索引</strong>而非输出索引（此阶段会重复获得输出索引）</li>
<li>在空间索引数据上使用一种独特的<strong>并行算法</strong>，以获得输出索引以及相关的空间索引。</li>
<li>根据前两步的结果生成一个<strong>与稀疏数据空间维度相同的缓冲区</strong>，用于下一步的表查找</li>
<li>对规则进行<strong>迭代</strong>，并使用存储的空间索引来获取每个输入索引的输出索引。</li>
</ol>
<p><img src="https://pic2.zhimg.com/80/v2-40a7e08f7a00a6e25ac4ff33a25fb849_1440w.jpg" alt="img"></p>
<h3 id="稀疏卷积中间提取器"><a href="#稀疏卷积中间提取器" class="headerlink" title="稀疏卷积中间提取器"></a>稀疏卷积中间提取器</h3><p>中间提取器用于学习z轴信息，并将稀疏的三维数据转换为二维BEV图像。它包含了稀疏卷积的两个阶段。每个阶段都有几个子流形卷积层和一个正常的稀疏卷积，用于在z轴进行下采样。在 z 维被下采样到一维或二维后，稀疏数据被转换为密集特征图。 然后，将数据简单地重新整形为类似图像的 2D 数据。</p>
<img src="\images\image-20220112212134401.png" alt="image-20220105165859553" style="zoom:100%;" />


<blockquote>
<p>黄色表示稀疏卷积，白色表示子流形卷积，红色表示稀疏到密集层，图的上半部分是稀疏数据的空间维数。</p>
</blockquote>
<h3 id="Anchors与目标"><a href="#Anchors与目标" class="headerlink" title="Anchors与目标"></a>Anchors与目标</h3><p>作者的anchor size与<strong>VoxelNet</strong>中anchor size，对正负锚点的阈值选择都是一样。作者同时为每个锚点分配一个以分类为目标的one-hot向量、一个边界框回归为目标的7维向量、一个以方向分类为目标的one-hot向量。</p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><h3 id="Sine-Error-Loss-for-Angle-Regression（方向回归）"><a href="#Sine-Error-Loss-for-Angle-Regression（方向回归）" class="headerlink" title="Sine-Error Loss for Angle Regression（方向回归）"></a>Sine-Error Loss for Angle Regression（方向回归）</h3><p>作者在RPN中增加了一个direction classifer分支，将车头是否区分正确直接通过一个softmax loss来进行约束。如果θ&gt;0则为正，θ&lt;0则为负，将其转换为了一个简单的二分类问题。<br>$$<br>L_{\theta}&#x3D;SmoothL1(\sin{(\theta_p-\theta_t)})<br>$$</p>
<p>它可以很好的解决0和Π两个角度的对抗样本问题，也可以根据角度偏移对IoU进行建模。</p>
<h3 id="Focal-Loss-for-Classification"><a href="#Focal-Loss-for-Classification" class="headerlink" title="Focal Loss for Classification"></a>Focal Loss for Classification</h3><p>该网络产生的约70k个锚点中，只有约4k~6k是有用的。作者引入RetinaNet中的单级损失single-stage loss，即focal loss<br>$$<br>FL(p_t)&#x3D;-\alpha_t(1-p_t)^{\gamma}\log(p_t)<br>$$<br>pt是模型的估计概率，α&#x3D;0.25，γ&#x3D;2.</p>
<h3 id="总训练损失"><a href="#总训练损失" class="headerlink" title="总训练损失"></a>总训练损失</h3><p>$$<br>L_{total}&#x3D;\beta_1L_{cls}+\beta_2(L_{reg-\theta}+L_{reg-other})+\beta_3L_{dir}<br>$$</p>
<p>第一个损失函数是分类损失，第二个损失函数是新角度损失，第三个损失函数是位置和尺寸回归损失，第四个损失函数是方向分类损失。β1&#x3D;1.0、β2&#x3D;2.0、β3&#x3D;0.2（将β3使用较小的值，避免网络难以识别物体发方向情况）</p>
<h2 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h2><h3 id="从数据库中采样Ground-Truths"><a href="#从数据库中采样Ground-Truths" class="headerlink" title="从数据库中采样Ground Truths"></a>从数据库中采样Ground Truths</h3><ol>
<li>从训练数据集生成一个包含所有Ground Truths及其相关点云数据（ground truths的三维box中所有点）</li>
<li>从数据库中随机选中几个ground truths，通过串联方式引入当前训练的点云中（可以增加训练中ground truths点的数量，以模拟不同环境中的物体）</li>
<li>进行碰撞测试，删除任何与其他物体碰撞的采样对象</li>
</ol>
<h3 id="目标噪音"><a href="#目标噪音" class="headerlink" title="目标噪音"></a>目标噪音</h3><p>作者使用voxelNet方法对每个ground truth与其中点云独立、随机的进行转变。</p>
<h3 id="全局旋转和放缩"><a href="#全局旋转和放缩" class="headerlink" title="全局旋转和放缩"></a>全局旋转和放缩</h3><p>作者对全部点云以及所有ground truth box进行全局放缩和旋转。从[0.95,1.05]的均匀分布提取局部噪音，从[-Π&#x2F;4,Π&#x2F;4]提取全局噪音</p>
<h2 id="网络细节"><a href="#网络细节" class="headerlink" title="网络细节"></a>网络细节</h2><p>作者使用一大一小两个网络，在摄像机视野外的点被舍弃。</p>
<h3 id="汽车检测任务"><a href="#汽车检测任务" class="headerlink" title="汽车检测任务"></a>汽车检测任务</h3><p>在SECOND中使用两个VFE层，即大型网络的VFE(32)和VFE(128)，较小的网络的VFE(32)和VFE(64)，在线性(128)层之后。因此，输出稀疏张量的维数对于大型网络为128  × 10 × 400 × 352，对于小型网络为128 × 10 × 320 ×  264。然后，我们使用两阶段稀疏CNN进行特征提取和降维。每个卷积层遵循一个BatchNorm层和一个ReLU层。所有稀疏卷积层都有一个64-output  feature map，核大小为(3,1,1)核大小，stride为(2,1,1)。对于大型网络，中间块的输出维数为64 × 2 × 400 ×  352。一旦输出被重塑为128 × 400 × 352，就可以应用RPN网络。我们使用Conv2D(cout, k,  s)来表示con2d - batchnorm - relu层，使用DeConv2D(cout, k, s)来表示DeConv2D- batchnorm -  relu层，其中cout为输出通道数，k为内核大小，s为stride。因为所有层在所有维度上都有相同的大小，所以我们对k和s使用标量值。所有Conv2D层都有相同的填充，所有DeConv2D层都有零填充。在我们的RPN的第一阶段，应用了三个Conv2D(128,  3,1(2))层。然后，在第二阶段和第三阶段分别应用5个Conv2D(128, 3, 1(2))层和5个Conv2D(256, 3,  1(2))层。在每一阶段中，只有第一卷积层的s &#x3D; 2;否则，s &#x3D; 1。我们对每个阶段的最后一次卷积应用一个单一的DeConv2D(128, 3,  s)层，三个阶段的s依次为1、2和4。</p>
<img src="\images\image-20220112220946220.png" alt="image-20220105165859553" style="zoom:150%;" />


<h3 id="行人和骑自行车者检测任务"><a href="#行人和骑自行车者检测任务" class="headerlink" title="行人和骑自行车者检测任务"></a>行人和骑自行车者检测任务</h3><p>与汽车检测方面唯一的区别是RPN中第一个卷积层的步幅为1而不是2。</p>
<h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p>在KITTI的验证集上，该网络无论大小网络都具有极高的平均准确度，以及极快的处理速度。同时该网络的角度编码速度、收敛速度也是非常快的。</p>
<img src="\images\image-20220112222654032.png" alt="image-20220105165859553" style="zoom:150%;" />


<h3 id="汽车检测任务-1"><a href="#汽车检测任务-1" class="headerlink" title="汽车检测任务"></a>汽车检测任务</h3><p>该网络在检测汽车时展示出了极强的性能，尤其是该网络可以有效的检测被遮挡的汽车。但是对于获得数据量较少的汽车任然无法做到准确检测，尤其是对于<strong>点数小于10的车辆</strong></p>
<h3 id="行人和骑自行车者检测任务-1"><a href="#行人和骑自行车者检测任务-1" class="headerlink" title="行人和骑自行车者检测任务"></a>行人和骑自行车者检测任务</h3><p>对行人和自行车的检测出现了更多的<strong>假阳性与假阴性</strong>，一些预测甚至出现在不合理的位置。这些问题可能归因于行人和自行车的实例包含的点更少，<strong>容易与其他点或是噪音混淆</strong>。此外，行人和自行车数量相对较少，导致包含他们的voxel数量较少，训练效果也就较差。过滤不相关信息并基于二维检测结果确定目标位置，应该会解决这个问题。</p>
<h1 id="做出的贡献"><a href="#做出的贡献" class="headerlink" title="做出的贡献"></a>做出的贡献</h1><ol>
<li>将稀疏卷积应用于基于激光雷达的目标检测</li>
<li>提出了一种改进的稀疏卷积方法，显著提升训练与推理的速度</li>
<li>引入一种新的<strong>角度损失回归</strong>方法</li>
<li>，提高收敛速度和性能</li>
</ol>
]]></content>
      <tags>
        <tag>论文阅读</tag>
        <tag>deep learning</tag>
        <tag>Object Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>操作系统</title>
    <url>/2022/03/11/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/</url>
    <content><![CDATA[<p><strong>操作系统是对计算机资源进行管理的软件</strong><br><a href="/download/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.xmind">Xmind下载</a></p>
<span id="more"></span>
<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><ul>
<li><p>操作系统的基本特征：并发、共享、虚拟、异步</p>
</li>
<li><p>操作系统是对计算机资源进行管理的软件（处理机管理、存储器管理、文件管理、设备管理）</p>
</li>
<li><p>分布式操作系统是以实现并行任务分配、并行进程通信、分布控制机构以及实现分散资源管理等功能为目的的系统程序。</p>
</li>
<li><p>网络操作系统是以资源共享和信息交换为目的的操作系统。</p>
</li>
<li><p>分布式操作系统是以计算机网络为基础构成的一个独立的整体，其更突出协同性，它对用户来说是透明的。</p>
</li>
<li><p>分布式操作系统相比网络操作系统本质不同在于：分布式操作系统中若干计算机相互协同完成同一任务</p>
</li>
<li><p>单处理机系统中，处理机与设备、处理机与通道、设备与设备都可以并行执行，但进程与进程不能并行执行</p>
</li>
<li><p>操作系统与用户通信接口不包括缓存管理指令（系统中的缓存全部由操作系统管理，对用户透明，因此不提供管理系统缓存的系统调用）</p>
</li>
</ul>
<blockquote>
<p> 库函数与系统调用</p>
<p> 1.库函数是语言或应用程序的一部分，可以运行在用户空间中。</p>
<p> 2.系统调用是操作系统的一部分，运行在内核空间。</p>
<p> 3.未使用系统调用的库函数，其执行效率比系统调用高（设计上下文切换、状态转换）</p>
</blockquote>
<ul>
<li><p>实时系统的进程调度，通常采用抢占式优先级算法</p>
</li>
<li><p>linux是多任务、多用户的操作系统，因此允许多个用户同时登陆（多用户）、并且允许多个用户端通过一个账号登陆（多任务）</p>
</li>
<li><p>UNIX是多任务、多用户操作系统，支持多种处理器架构，属于分时操作系统</p>
</li>
<li><p>内核：时钟管理、中断处理、进程管理、设备管理</p>
</li>
<li><p>微内核结构设计不会让系统更高效；能够有效支持多处理级运行，非常适合于分布式系统环境</p>
</li>
<li><p>模块化OS结构原则：分解和模块化</p>
</li>
<li><p>多道程序系统的四个特征：并发、共享、虚拟、异步</p>
</li>
</ul>
<h2 id="微内核"><a href="#微内核" class="headerlink" title="微内核"></a>微内核</h2><p>微内核需要频繁在核心态用户态间切换，开销较大</p>
<p>特点：内核足够小；给予C&#x2F;S模式；“机智与策略分离”原理；面向对象技术</p>
<p>功能：进程间通信；低级IO；低级进程管理和调度；中断和陷入处理</p>
<p>Windows是宏内核操作系统</p>
<p>添加系统服务时不用修改内核</p>
<h2 id="中断"><a href="#中断" class="headerlink" title="中断"></a>中断</h2><ul>
<li><p>用户程序通过执行陷入指令（访管指令或trap指令）来发起系统调用，请求操作系统提供服务</p>
</li>
<li><p>中断向量地址是中断服务例行程序的入口地址的地址</p>
</li>
<li><p>CPU处于核心态时，可以执行除了访管指令外的所有指令</p>
</li>
<li><p>访管指令在用户态使用，将程序运行由用户态转换为核心态的指令</p>
</li>
<li><p>用户程序在用户态下使用特权指令引起的中断为访管中断</p>
</li>
<li><p>内部异常检测通常由CPU内部逻辑实现</p>
</li>
<li><p>内部异常通常是CPU执行指令内部的实践，如程序非法操作码、地址越界、算术溢出、虚存系统缺页以及专门的陷入指令</p>
</li>
<li><p>内部异常不能被屏蔽，响应发生在指令执行的过程中</p>
</li>
<li><p>产生内部异常后，对于非法指令、除数为0等异常，无法通过异常处理程序恢复故障的，必须终止进程的执行</p>
</li>
<li><p>处理外部中断时，操作系统（中断服务程序）保存通用寄存器的内容（硬件保护PC值，并找到该中断信号对应的中断向量），保护中断屏蔽字，保护PSW，提供中断服务</p>
</li>
<li><p>子程序调用只需要保存PC值</p>
</li>
<li><p>时钟中断主要工作是处理和时间有关的信息（系统时间、进程时间片、使用CPU时间、各种定时器）及决定是否执行调度程序</p>
</li>
<li><p>用户态转换为核心态的唯一途径：中断或异常（访管指令通过产生一个中断事件切换状态）</p>
</li>
<li><p>缺页产生后，在用户态发生缺页中断，然后进入核心态执行缺页中断服务程序</p>
</li>
<li><p>异常或是中断的产生于现在CPU为用户态还是核心态无关，只看中断是由指令执行时产生还是外部产生</p>
</li>
<li><p>从内存取数和把运算结果放入内存的指令在用户态执行</p>
</li>
<li><p>输入&#x2F;输出指令需要使用IO设备，涉及资源使用，有可能影响其他进程及危害⚠️计算机，所以不能在用户态执行</p>
</li>
<li><p>系统调用：用户程序通过执行陷入指令来发起系统调用，请求操作系统提供服务；操作系统内核程序对系统调用做出相应处理；处理完成后，操作系统内核程序把CPU使用权还给用户程序</p>
</li>
</ul>
<h2 id="管道"><a href="#管道" class="headerlink" title="管道"></a>管道</h2><p>管道是一种特殊的文件，本质上是一种固定大小的缓冲区，且只存在于内存中。一个管道可以实现双向的数据传输，而同一时刻最多有一个方向的传输，不能两个方向同时进行。当管道满时，进程在写管道会被阻塞；当管道空时，进程在读管道会被阻塞</p>
<p> 分层</p>
<ul>
<li><p>第0层为硬件、第N层为用户接口</p>
</li>
<li><p>每一层利用低一层提供的接口为高一层提供服务</p>
</li>
</ul>
<h2 id="模块（可加载内核模块）"><a href="#模块（可加载内核模块）" class="headerlink" title="模块（可加载内核模块）"></a>模块（可加载内核模块）</h2><ul>
<li>内核提供核心服务，其他服务通过模块链入服务</li>
</ul>
<h2 id="操作系统引导"><a href="#操作系统引导" class="headerlink" title="操作系统引导"></a>操作系统引导</h2><ul>
<li>操作系统的生成与配置需要获取计算机硬件系统的特定配置</li>
</ul>
<ol>
<li><p>完全定制（系统管理员修改操作系统源代码副本，重新编译操作系统）</p>
</li>
<li><p>系统描述创建表，并从预先已编译的库中选择模块。将这些模块链接起来生成操作系统</p>
</li>
<li><p>构造完全由表驱动的系统，在执行时动态选择相应的代码模块。系统生成只是创建适当的表</p>
</li>
</ol>
<ul>
<li><p>系统引导：加载内核以启动计算机的过程（通过一个简单的引导程序（ROM）从磁盘掉入更复杂的引导程序，后者再加载内核到内存）</p>
</li>
<li><p>计算机启动过程：CPU加电；跳转到BIOS；登记BIOS中断例程入口地址；硬件自检；进行操作系统引导</p>
</li>
</ul>
<h2 id="虚拟机"><a href="#虚拟机" class="headerlink" title="虚拟机"></a>虚拟机</h2><img src="\images\image-20220315094703729.png" alt="哎呀，图片不见了" style="zoom:100%;" />


<p>单个计算机的硬件抽象为几个不同的执行部件，通过CPU调度与虚拟内存技术，使进程认为自己拥有独立的处理器与内存</p>
<p>实现方法：提供虚拟磁盘，为用户提供与底层机器完全一样的副本。用户在自己的虚拟机中运行机器上拥有的任何操作系统ISO或软件</p>
<p>虚拟机只能运行在用户态，虚拟机内部也有用户态与内核态</p>
<p>可以用硬件实现也可以用软件实现</p>
<h1 id="进程管理"><a href="#进程管理" class="headerlink" title="进程管理"></a>进程管理</h1><ul>
<li><p>PCB是进程存在的唯一标志</p>
</li>
<li><p>进程映像：程序段、相关数据段、PCB（静态概念）</p>
</li>
<li><p>线程是程序执行流的最小单元，是系统独立调度和分派的基本单元（处理机的分配单元）；进程是除CPU资源以外的系统资源的分配单元（线程不占有系统资源）</p>
</li>
<li><p>进程间通信（IPC）需要进程同步和互斥，以保证数据一致性；线程间通过直接读写数据段（全局变量）来通信</p>
</li>
<li><p>引入线程可以提升程序并发执行的程度，可进一步提高系统效率。</p>
</li>
<li><p>线程不可以脱离进程独立运行</p>
</li>
</ul>
<p><strong>1.</strong>     c语言中全局赋值变量存放在正文段</p>
<p><strong>2.</strong>    未赋值局部变量存放在栈段</p>
<p><strong>3.</strong>    函数调用实参传递值存放在栈段</p>
<p><strong>4.</strong>    用malloc()要求动态分配的存储区在堆段</p>
<p><strong>5.</strong>    常量值存放在正文段</p>
<p><strong>6.</strong>    进程优先级存放在PCB</p>
<ul>
<li><p>并发进程失去封闭性指：并发进程共享变量，其执行结果与速度有关（不同速度下执行结果不同）</p>
</li>
<li><p>程序代码经过多次创建可对应不同的进程，而同一个系统的进程（线程）可以由系统调用的方法被不同进程（线程）多次使用（而不会为每次调用创建新的系统线程）</p>
</li>
<li><p>父子进程可以共享一部分资源，但不能共享虚拟地址空间</p>
</li>
<li><p>进程间通信一定会共享某些资源：共享存储器系统，共享存储器资源；消息传递系统，共享消息文件；管道通信，共享管道文件</p>
</li>
<li><p>共享内存系统需要通信进程建立共享内存区域（通常驻留在创建共享内存段的进程地址空间中，其他希望使用这个共享内存段进行通信的进程应该将其加入自己的地址空间）</p>
</li>
<li><p>管道是半双工工作，将数据以字节流的形式写入管道。管道（缓冲区）满时，写进程被阻塞；管道空时，读进程被阻塞</p>
</li>
<li><p>中期调度：将暂时不运行的进程调至外存等待（挂起态），降低多道程序程度</p>
</li>
</ul>
<blockquote>
<p>不能进行进程调度与切换的情况</p>
<ol>
<li><p>处理中断(中断处理属于系统工作的一部分，逻辑上不属于某一个进程)</p>
</li>
<li><p>进程在操作系统内核程序临界区（加锁）</p>
</li>
<li><p>其他需要完全屏蔽中断的原子操作</p>
</li>
</ol>
</blockquote>
<ul>
<li><p>FCFS对长作业有利，适合CPU繁忙型作业</p>
</li>
<li><p>SJF对长作业不利，会导致饥饿</p>
</li>
<li><p>分时系统时间片固定，因此用户数越多，响应时间越长</p>
</li>
<li><p>高响应比优先调度不适用于交互式操作系统，高响应比调度需要知道作业预计运行时间，但是作业在交互式情况下，预计运行时间是不确定的，因此计算不出响应比</p>
</li>
<li><p>对于多道程序来说，内存中最多存放n道作业，即处于就绪态、运行态、等待态的作业个数之和最多为n（处于运行态的作业，其程序代码段、数据段都在内存中，在内存视角中与其他内存中作业无异）</p>
</li>
<li><p>阻塞队列中进程的个数最多为n个</p>
</li>
<li><p>唤醒原语是将进程从阻塞态转变为就绪态，需要一个与被唤醒进程合作或被其他相关进程调用实现的</p>
</li>
</ul>
<h2 id="信号量"><a href="#信号量" class="headerlink" title="信号量"></a>信号量</h2><p>硬件实现中使用TSL指令实现进程互斥的伪代码如下：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">do</span>&#123;<span class="comment">//  该实现下，等待进入临界区的进程不会主动放弃CPU，不满足&quot;让权等待&quot;原则</span></span><br><span class="line"></span><br><span class="line">	<span class="keyword">while</span>(TSL(&amp;lock));<span class="comment">//忙等待</span></span><br><span class="line"></span><br><span class="line">	critical section <span class="comment">//临界区</span></span><br><span class="line"></span><br><span class="line">	lock = <span class="literal">false</span>;</span><br><span class="line"></span><br><span class="line">&#125;<span class="keyword">while</span>(<span class="literal">true</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>信号量的同步（原子性）是由硬件支持，PV操作也有临界区</li>
</ul>
<blockquote>
<p>wait() 、signal()操作没有完全取消忙等，只是将忙等由整个临界区（无法控制、可能很长，或者总是被占用）转移到了PV操作的临界区上（该临界区经过合理编码，不会超过10条指令，因此忙等很少发生，临界区几乎不被占用）</p>
</blockquote>
<ul>
<li><p>PV操作是一种低级进程通信原语</p>
</li>
<li><p>PV操作时需要根据用户需要自行确定信号量初始值</p>
</li>
</ul>
<h2 id="管程"><a href="#管程" class="headerlink" title="管程"></a>管程</h2><ul>
<li><p>管程是进程同步工具，解决信号量机制大量同步操作分散的问题</p>
</li>
<li><p>管程是被进程调用的，管程是语法范围，无法创建和撤销</p>
</li>
<li><p>管程把对共享资源的操作封装起来</p>
</li>
<li><p>每次仅允许一个进程进入管程</p>
</li>
<li><p>管程使用条件变量condition定义为阻塞原因，不同阻塞原因对于相应条件变量</p>
</li>
<li><p>管程中的signal操作的作用和信号量机制中的V操作不完全相同，signal操作唤醒一个因x条件而阻塞的进程；wait操作会阻塞该进程（x对应的条件不满足时调用）</p>
</li>
<li><p>条件变量没有值，仅实现排队等待功能；信号量有值，反映剩余资源数</p>
</li>
<li><p>管程使用共享数据结构记录剩余资源数</p>
</li>
</ul>
<h2 id="死锁"><a href="#死锁" class="headerlink" title="死锁"></a>死锁</h2><blockquote>
<p>死锁定理（在资源分配图中检测系统状态是否为死锁状态）：</p>
<p>将资源分配图中既不阻塞又不独立的进程逐个删去。如果能够消去图中所有的边，则不存在死锁    </p>
<p>资源分配方法</p>
<ol>
<li><p>静态分配：执行前获得所有资源</p>
</li>
<li><p>按序分配：进程$R_i$提出申请分配资源后，其他进程只能申请编号大于$i$的资源</p>
</li>
<li><p>银行家算法</p>
</li>
</ol>
</blockquote>
<ul>
<li><p>当系统处于不安全状态时，不会立即进入死锁状态。只有所有进程均因申请资源没有得到满足而进入阻塞态，系统才进入死锁状态</p>
</li>
<li><p>在死锁检测的进程-资源图中，如果申请边的申请得到满足，则删去申请边，添加从资源到进程的分配边</p>
</li>
<li><p>银行家算法进行安全🔐序列检查时，不需要的参数是满足系统安全的最少资源数</p>
</li>
</ul>
<h2 id="锁"><a href="#锁" class="headerlink" title="锁"></a>锁</h2><p>🔒的本质在内存中的一个整形数，不同数值表示不同状态</p>
<h3 id="锁的分类"><a href="#锁的分类" class="headerlink" title="锁的分类"></a>锁的分类</h3><p>I. 重量级锁：得到锁就加锁，得不到锁立即进入阻塞状态</p>
<p>II. 自旋锁：在获取锁失败的时候不会使得线程阻塞而是<strong>一直自旋尝试获取锁</strong>。当线程等待自旋锁的时候，CPU不能做其他事情，而是一直处于<strong>轮询忙等</strong>的状态。（常适用在多CPU场景下，避免不必要的切换进程上下文开销）</p>
<p>III. 自适应自旋锁：根据线程最近获得锁的状态来调整循环次数的自旋锁</p>
<p>IV. 轻量级锁：进入方法时不加锁，只做一个标志（有人在执行），CAS机制</p>
<p>V. 偏向锁：没必要加锁，大部分时候只有一个线程在执行该方法</p>
<p>VI. 悲观锁：认为不加锁就会出事，因此无论是否并发竞争资源，都会锁住资源，并等待资源释放下一个线程才能获取到锁。例如：重量级锁、自旋锁、自适应自旋锁</p>
<p>VII. 乐观锁：不加锁，当出现冲突时再想办法解决。当线程开始竞争资源时，不是立马给资源上锁，而是进行一些前后值比对，以此来操作资源。例如：CAS机制</p>
<p>VIII. 互斥锁（Barkey锁（对进程编号）、Perterson锁）：常用于多处理器系统，等待期间不用切换进程上下文，只有等到时间片用完才下处理级（违反“让权等待”）</p>
<p>IX. 共享锁：其他任何线程都无法获取<em>互斥锁</em>，但是可以获取<em>共享锁</em>。</p>
<h3 id="条件变量"><a href="#条件变量" class="headerlink" title="条件变量"></a>条件变量</h3><ol>
<li><p>等待队列</p>
</li>
<li><p>条件变量没有值，只有队列</p>
</li>
<li><p>对条件变量操作：wait&#x2F;signal</p>
</li>
<li><p>signal操作时如果一个条件变量上没有等待进程，则signal操作无效</p>
</li>
<li><p>常和锁共同使用（锁实现互斥、配合条件变量实现同步）</p>
</li>
</ol>
<h2 id="线程"><a href="#线程" class="headerlink" title="线程"></a>线程</h2><p>线程的状态与转换与进程的状态基本一致 </p>
<h3 id="线程的组织与控制（TCB）"><a href="#线程的组织与控制（TCB）" class="headerlink" title="线程的组织与控制（TCB）"></a>线程的组织与控制（TCB）</h3><p>保存程序计数器PC其他寄存器与线程的堆栈指针</p>
<p>对于不支持内核级线程的操作系统，调度程序处理的对象是进程（进程内的线程由线程库管理）；支持内核级线程的操作系统，调度程序处理对象是内核级线程</p>
<h3 id="用户级线程、内核级线程"><a href="#用户级线程、内核级线程" class="headerlink" title="用户级线程、内核级线程"></a>用户级线程、内核级线程</h3><ol>
<li><p>用户级线程由线程库管理用户线程</p>
</li>
<li><p>用户级线程的调度：代价低，不需要切换完整的上下文；一个线程阻塞整个进程阻塞</p>
</li>
<li><p>内核级线程的调度：代价高，需要切换到内核态，切换完整上下文；一个线程阻塞。其他线程仍可以运行</p>
</li>
</ol>
<h3 id="闲逛进程"><a href="#闲逛进程" class="headerlink" title="闲逛进程"></a>闲逛进程</h3><ol>
<li>没有其他就绪进程时，执行闲逛进程</li>
<li>优先级最低</li>
<li>可以是0地址指令，占一个完整的指令周期（指令周期末尾例行检查中断）</li>
<li>能耗低</li>
</ol>
<h3 id="孤儿进程"><a href="#孤儿进程" class="headerlink" title="孤儿进程"></a>孤儿进程</h3><p>一个父进程退出，而它的一个或多个子进程还在运行，那么那些<strong>子进程</strong>将成为孤儿进程。孤儿进程将被init进程(进程号为1)所收养，并由init进程对它们完成状态收集工作。</p>
<h3 id="僵尸进程"><a href="#僵尸进程" class="headerlink" title="僵尸进程"></a>僵尸进程</h3><p>一个进程使用fork创建子进程，如果子进程退出，而父进程并没有调用wait或waitpid获取子进程的状态信息，那么<strong>子进程的进程描述符</strong>仍然保存在系统中。这种进程称之为僵尸进程。</p>
<h3 id="协程"><a href="#协程" class="headerlink" title="协程"></a>协程</h3><p>协程是微线程，在子程序内部执行，可在子程序内部中断，转而执行别的子程序，在适当的时候再返回来接着执行。</p>
<h4 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h4><ol>
<li><strong>协程调用跟切换比线程效率高</strong>：协程执行效率极高。协程<strong>不需要多线程的锁机制</strong>，多个协程从属于一个线程，不存在同时写变量冲突，可以不加锁的访问全局变量，所以上下文的切换非常快。</li>
<li><strong>协程占用内存少</strong>：执行协程只需要极少的栈内存（大概是4～5KB），而默认情况下，线程栈的大小为1MB。</li>
<li><strong>切换开销更少</strong>：协程直接操作栈基本<strong>没有内核切换</strong>的开销，所以切换开销比线程少。</li>
</ol>
<h2 id="进程上下文"><a href="#进程上下文" class="headerlink" title="进程上下文"></a>进程上下文</h2><h3 id="上下文切换"><a href="#上下文切换" class="headerlink" title="上下文切换"></a>上下文切换</h3><ul>
<li><p>进程上下文通常用PCB表示，包括CPU寄存器的值、进程状态等</p>
</li>
<li><p>切换CPU到另一个进程需要保存当前进程状态和恢复另一个进程的状态</p>
</li>
</ul>
<h4 id="切换进程"><a href="#切换进程" class="headerlink" title="切换进程"></a>切换进程</h4><ol>
<li><p>需要保存地址空间（页表寄存器）</p>
</li>
<li><p>TLB全部失效</p>
</li>
<li><p>Cache全部失效</p>
</li>
<li><p>新进程运行初期缺页率可能高</p>
</li>
</ol>
<h4 id="切换线程"><a href="#切换线程" class="headerlink" title="切换线程"></a>切换线程</h4><p>需要保存程序计数器、寄存器、堆栈</p>
<h2 id="进程间通信"><a href="#进程间通信" class="headerlink" title="进程间通信"></a>进程间通信</h2><h3 id="匿名管道"><a href="#匿名管道" class="headerlink" title="匿名管道"></a>匿名管道</h3><p>管道是一种半双工的通信方式，数据只能单向流动，而且只能在<strong>具有亲缘关系的进程</strong>间使用。进程的亲缘关系通常是指父子进程关系。</p>
<h3 id="命名管道"><a href="#命名管道" class="headerlink" title="命名管道"></a>命名管道</h3><p>命名管道提供了一个路径名与之关联，<strong>以命名管道的文件形式存在于文件系统中</strong>，这样，<strong>即使与命名管道的创建进程不存在亲缘关系的进程，只要可以访问该路径，就能够彼此通过有名管道相互通信</strong>。</p>
<p><strong>命名管道的名字存在于文件系统中，内容存放在内存中。</strong>遵循先进先出原则</p>
<h3 id="信号"><a href="#信号" class="headerlink" title="信号"></a>信号</h3><ul>
<li>信号是Linux系统中用于进程间互相通信或者操作的一种机制，信号可以在<strong>任何时候</strong>发给某一进程，而无需知道该进程的状态。（异步通信）</li>
<li>如果该进程当前并未处于执行状态，则该信号就有内核<strong>保存</strong>起来，知道该进程回复执行并传递给它为止。</li>
<li>如果一个信号被进程设置为<strong>阻塞</strong>，则该信号的传递被延迟，直到其阻塞被取消是才被传递给进程。</li>
</ul>
<table>
<thead>
<tr>
<th align="left">信号</th>
<th align="left">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="left">SIGHUP</td>
<td align="left">当用户退出终端时，由该终端开启的所有进程都退接收到这个信号，默认动作为终止进程。</td>
</tr>
<tr>
<td align="left">SIGINT</td>
<td align="left">程序终止(interrupt)信号, 在用户键入INTR字符(通常是Ctrl+C)时发出，用于通知前台进程组终止进程。</td>
</tr>
<tr>
<td align="left">SIGQUIT</td>
<td align="left">和SIGINT类似, 但由QUIT字符(通常是Ctrl+\)来控制. 进程在因收到SIGQUIT退出时会产生core文件, 在这个意义上类似于一个程序错误信号。</td>
</tr>
<tr>
<td align="left">SIGKILL</td>
<td align="left">用来立即结束程序的运行. <strong>本信号不能被阻塞、处理和忽略</strong>。</td>
</tr>
<tr>
<td align="left">SIGTERM</td>
<td align="left">程序结束(terminate)信号, 与SIGKILL不同的是该信号可以被阻塞和处理。通常用来要求程序自己正常退出。</td>
</tr>
<tr>
<td align="left">SIGSTOP</td>
<td align="left">停止(stopped)进程的执行. 注意它和terminate以及interrupt的区别:该进程还未结束, 只是暂停执行. <strong>本信号不能被阻塞, 处理或忽略</strong>.</td>
</tr>
</tbody></table>
<h3 id="消息队列"><a href="#消息队列" class="headerlink" title="消息队列"></a>消息队列</h3><ul>
<li>消息队列是<strong>存放在内核中的消息链表</strong>，每个消息队列由消息队列标识符表示。</li>
<li>与管道（匿名管道：只存在于内存中的文件；命名管道：存在于实际的磁盘介质或者文件系统）不同的是消息队列<strong>存放在内核中</strong>，只有在内核重启(即，操作系统重启)或者显示地删除一个消息队列时，该消息队列才会被真正的删除。</li>
<li>另外与管道不同的是，消息队列在某个进程往一个队列写入消息之前，并<strong>不需要另外某个进程在该队列上等待消息的到达</strong>。</li>
<li>消息队列的通信数据遵循先进先出的原则</li>
<li>消息队列可以实现消息的随机查询,消息不一定要以先进先出的次序读取,也可以按消息的类型读取.比FIFO更有优势。</li>
</ul>
<h3 id="共享内存"><a href="#共享内存" class="headerlink" title="共享内存"></a>共享内存</h3><p>共享内存就是映射一段能被其他进程所访问的内存，这段共享内存由一个进程创建，但多个进程都可以访问。同时需要使用信号量来控制多个进程对共享内存空间的访问</p>
<h3 id="信号量-1"><a href="#信号量-1" class="headerlink" title="信号量"></a>信号量</h3><p>信号量主要用来解决进程和线程间并发执行时的同步问题。信号量是一个计数器，可以用来控制多个进程对<strong>共享资源</strong>的访问。它常作为一种锁机制，防止某进程正在访问共享资源时，其他进程也访问该资源。</p>
<h3 id="socket"><a href="#socket" class="headerlink" title="socket"></a>socket</h3><p>套接字：IP+PORT。可以实现不同主机间的通信</p>
<img src="\images\image-20220321110802767.png" alt="哎呀，图片不见了" style="zoom:100%;" />


<h2 id="线程间通信"><a href="#线程间通信" class="headerlink" title="线程间通信"></a>线程间通信</h2><h3 id="锁机制"><a href="#锁机制" class="headerlink" title="锁机制"></a>锁机制</h3><p>包括互斥锁、条件变量、读写锁互斥锁提供了以排他方式防止数据结构被并发修改的方法。读写锁允许多个线程同时读共享数据，而对写操作是互斥的。条件变量可以以原子的方式阻塞进程，直到某个特定条件为真为止。对条件的测试是在互斥锁的保护下进行的。条件变量始终与互斥锁一起使用。</p>
<h3 id="信号量机制"><a href="#信号量机制" class="headerlink" title="信号量机制"></a>信号量机制</h3><p>包括无名线程信号量和命名线程信号量</p>
<h3 id="信号机制"><a href="#信号机制" class="headerlink" title="信号机制"></a>信号机制</h3><p>类似进程间的信号处理线程间的通信目的主要是用于线程同步，所以线程没有像进程通信中的用于数据交换的通信机制。</p>
<h1 id="内存管理"><a href="#内存管理" class="headerlink" title="内存管理"></a>内存管理</h1><ul>
<li><p>存储管理是为了方便用户以及提高内存利用率</p>
</li>
<li><p>页表寄存器存放的是顶级页表的起始物理地址</p>
</li>
<li><p>页式存储方式只能采用动态重定位</p>
</li>
<li><p>空间、时间局部性良好的数据结构和技术适用于虚拟存储系统（数据常在一页中）</p>
</li>
<li><p>虚拟存储器中，程序正在执行时，由操作系统完成地址映射；硬件完成Cache映射</p>
</li>
<li><p>页表应该包含作业页号、状态位、存取方式（执行、读&#x2F;写）、外存页号、内存页号、修改位等</p>
</li>
<li><p>进程在地址变换的过程中，可能因为地址越界而被撤销、也可能因为缺页而被阻塞</p>
</li>
<li><p>处理缺页中断时，会更新TLB表与页表</p>
</li>
<li><p>内存泄露：当以前分配的一片内存不再需要使用或无法访问时，但是并没有释放它，那么对于该进程来说，会因此导致总可用内存的减少，这时就出现了内存泄漏。</p>
</li>
<li><p>内存越界访问：简单地说，进程访问了不属于该进程的内存空间</p>
</li>
</ul>
<h2 id="C语言编译"><a href="#C语言编译" class="headerlink" title="C语言编译"></a>C语言编译</h2><ul>
<li><p>过程中，形成逻辑地址的阶段是链接（将各个目标模块的逻辑地址重定位，形成完整的逻辑地址）</p>
</li>
<li><p>编译阶段形成各个目标模块的独立的逻辑地址（从0开始）</p>
</li>
<li><p>将逻辑地址转换为物理地址的阶段是装载阶段 </p>
</li>
<li><p>使用交换技术时，若一个进程处于IO操作，则不能交换出主存（处于创建、临界区、死锁时都可以）</p>
</li>
<li><p>一个进程中段表只能有一个，页表可以有多个（每个分段对应一个页表）</p>
</li>
<li><p>内存保护完全由硬件完成（重定位寄存器、界地址寄存器）</p>
</li>
<li><p>操作系统通过内存保护实现多进程在主存中彼此互不干扰的环境下运行</p>
</li>
<li><p>分页系统中的页面是为操作系统感知的（由操作系统管理）</p>
</li>
<li><p>整个系统中只有一个重定位寄存器，在切换进程时重制寄存器的内容</p>
</li>
<li><p>使用最佳适应算法、最差适应算法管理动态分区内存时，在每次分配与回收内存后，都会对空闲分区链进行重新排序，按空闲内存大小由小到大排序</p>
</li>
<li><p>操作系统采用分页式存储管理方式时，每个进程拥有一张页表，且进程的页表驻留在内存中（进程使用多级页表时，一开始只将一级页表掉入内存）</p>
</li>
<li><p>多级页表中，要使用几级页表，需要看虚拟地址中虚拟页号的位数</p>
</li>
<li><p>多级页表可以减少页表所占连续内存空间，但增加页表项所占字节数，减慢地址变换速度（多次访存）</p>
</li>
<li><p>在分段存储管理系统中，用共享段表描述所有被共享的段。若进程$P_1,P_2$共享段S，则在物理内存中仅保留一份段S的内容；段S在$P_1,P_2$中具有相同的段号；$P_1,P_2$都不使用段S时才回收段S段内存空间。</p>
</li>
<li><p>虚拟存储只能基于非连续分配技术</p>
</li>
<li><p>虚拟存储的容量≤内存容量+外存容量</p>
</li>
<li><p>虚拟存储的容量≤计算机的地址位数能容纳的最大容量（虚拟存储容量受上式两个条件的制约）</p>
</li>
<li><p>处理缺页错误时，操作系统执行置换页、分配内容等。但❌不会处理越界错误（地址变换前期由硬件检测）</p>
</li>
<li><p>LRU算法实现耗费高的原因：需要对所有页进行排序</p>
</li>
<li><p>使用覆盖、交换可以实现虚拟存储</p>
</li>
<li><p>覆盖技术用于同一个进程、交换技术用于不同进程</p>
</li>
<li><p>产生内存抖动的主要原因是页面置换算法不合理</p>
</li>
<li><p>在多级页表中，快表存放的是完整的页面号，而非某一级的页面号，因此快表命中即可到达物理地址</p>
</li>
<li><p>当进行缺页处理需要置换页时，旧页的物理地址给新页用来写入数据（即物理地址不变）</p>
</li>
<li><p>最高级页表项不能超过一页的大小</p>
</li>
</ul>
<h2 id="页面分配策略"><a href="#页面分配策略" class="headerlink" title="页面分配策略"></a>页面分配策略</h2><ul>
<li><p>固定分配局部置换：每个进程分配固定数量的物理块，运行期间不会改变，发生缺页时，调出该进程分配的页帧</p>
</li>
<li><p>可变分配全局置换：操作系统自身维护一个空闲物理块队列，进程发生缺页时，系统从空闲物理块队列中取出一个物理块分配给该进程，但进程不会自动归还物理块</p>
</li>
<li><p>可变分配局部置换：发生缺页时，只能从该进程已分配的页帧中选择调出，若进程频繁缺页，系统会增加若干物理块；若进程缺页率低，系统适当减少分配的物理块</p>
</li>
</ul>
<h3 id="CLOCK算法"><a href="#CLOCK算法" class="headerlink" title="CLOCK算法"></a>CLOCK算法</h3><img src="\images\image-20220315095345442.png" alt="哎呀，图片不见了" style="zoom:100%;" />


<p>调入一页则将指针向后移动一块，若命中则不移动指针</p>
<p>注意：若循环链表存在当前访问页时（访问页在某物理块中），直接将其访问位改为1，指针p不移动（命中后指针不移动）；</p>
<p>否则，若当前p指针指向页面的访问位为0，则淘汰该页，调入新页，将其访问位改为 1，指针p移到下一个物理块：若当前p指针指向页面的访问位为1，则将其访问位改为 0，并移动口指针到下一个物理块。</p>
<h3 id="改进型CLOCK算法（使用位u、修改位m）"><a href="#改进型CLOCK算法（使用位u、修改位m）" class="headerlink" title="改进型CLOCK算法（使用位u、修改位m）"></a>改进型CLOCK算法（使用位u、修改位m）</h3><ol>
<li><p>从当前位置开始，扫描帧缓冲区，不修改使用位，寻找（u&#x3D;0，m&#x3D;0）的帧</p>
</li>
<li><p>重新扫描，寻找（u&#x3D;0，m&#x3D;1）的帧，将不符合的帧的使用位u改为0.</p>
</li>
<li><p>重复1、2步</p>
</li>
</ol>
<h2 id="内存共享"><a href="#内存共享" class="headerlink" title="内存共享"></a>内存共享</h2><p>内存共享通过内存映射实现，将多个进程的虚拟地址空间映射到同一片物理地址（可以是“页”映射，也可以是“段”映射）</p>
<p>内存映射文件</p>
<p>将文件页映射到进程页表，进程按内存访问方式读写文件；还可以实现文件共享<img src="/%5Cimages%5Cclip_image010.png" alt="图像"></p>
<h1 id="文件管理"><a href="#文件管理" class="headerlink" title="文件管理"></a>文件管理</h1><ul>
<li>操作系统采用两级内部表记录文件信息，即：每个进程表和整个系统表。</li>
<li>每个进程表跟踪它打开的所有文件，保存进程对文件的使用记录。如：当前文件指针，文件访问权限</li>
<li>单个进程表的每个条目相应的指向整个系统的打开表。</li>
<li>系统表包含与进程无关的信息，包括每个打开文件的FCB的副本及其它信息</li>
<li>文件控制块（FCB）由逻辑文件系统操作，维护文件的相关信息（一个FCB就是一个文件目录项）</li>
<li>文件目录即为文件控制块的有序集合</li>
<li><img src="\images\clip_image026.jpg" alt="哎呀，图片不见了" style="zoom:100%;" /></li>
<li>创建一个新文件后，系统将分配一个FCB并存放在文件的目录中</li>
<li>FCB必须连续存放</li>
<li>索引节点存放在磁盘中，由文件目录项中的指针指向</li>
<li>索引节点是一块存储文件原信息的磁盘空间为了，为inode。</li>
<li>普通文件由目录块里的一个FCB加上多个数据块组成</li>
<li>目录文件由目录块里的一个FCB加上多个其他多个目录块组成</li>
<li>一个索引节点只能被一个文件（无论是目录文件，还是普通文件）所用，不能同时被其他文件所用。</li>
</ul>
<blockquote>
<p>执行一条open指令的工作过程:</p>
<p>open指令先根据文件路径找到相应的目录文件（对于目录文件的查找与其他文件查找方式相同），根据目录文件的数据找到文件的目录项。为了增加检索文件时的效率，文件目录中的目录项只记录文件名以及一个指向索引结点的指针，找到该文件后再根据指针找到索引结点，读出该文件的物理地址。文件被打开后，磁盘索引结点（目录）被复制到内存索引结点。对索引结点添加count字段后支持文件的硬链接。</p>
</blockquote>
<ul>
<li><p>文件系统采用多级目录结构的目的：解决命名冲突</p>
</li>
<li><p>逻辑记录是对文件进行存取操作的基本单位</p>
</li>
<li><p>在物理存储器中，文件数据（文件区）与文件控制信息（目录区）分离存储</p>
</li>
<li><p>文件是逻辑记录的一个序列，逻辑记录可以是字节、行或更为复杂的数据项</p>
</li>
<li><p>索引节点的总数表示系统中可拥有文件的最大数量</p>
</li>
</ul>
<h2 id="软链接、硬链接"><a href="#软链接、硬链接" class="headerlink" title="软链接、硬链接"></a>软链接、硬链接</h2><img src="\images\clip_image028.png" alt="哎呀，图片不见了" style="zoom:100%;" />


<img src="\images\clip_image030.jpg" alt="哎呀，图片不见了" style="zoom:100%;" />


<ul>
<li><p>linux下创建目录后，文件链接数2.因为目录中有上一级目录与当前目录，在该目录中每新加一个文件就会增加目录的链接数</p>
</li>
<li><p>建立符号链接时，引用计数值直接复制；建立硬链接时，引用计数值+1（引用计数值存放在索引节点中，即不在文件目录中）</p>
</li>
<li><p>硬链接通过索引结点进行连接，一个文件在物理存储器上有一个索引结点号。存在多个文件名指向同一个索引结点。共享文件的不同进程各自维护自己的文件描述符（记录进程的读写指针位置）。</p>
</li>
<li><p>软链接可以在目录与文件链接时使用、及其网络文件</p>
</li>
<li><p>多个进程共享一个文件F，则在系统打开表中仅有一个表项包含F的属性（硬链接直接使用指针与索引节点相连；软链接会新建一个链接文件）</p>
</li>
<li><p>访问控制机制必须由系统实现，而且安全性较差，灵活性较强</p>
</li>
<li><p>对一个文件的访问，常由用户访问权限和文件属性共同限制🚫</p>
</li>
<li><p>常使用备份方法保护文件</p>
</li>
<li><p>存取控制矩阵用于多用户间的存取权限保护</p>
</li>
<li><p>将用户访问权限抽象为矩阵，行代表用户；列代表权限</p>
</li>
<li><p>FAT（文件配置表）表项与全部磁盘块一一对应，索引分配将整个文件的盘块号集中构成索引块</p>
</li>
<li><p>链接分配不适合直接存取的外存分配方式</p>
</li>
<li><p>为支持CD-ROM中视频文件的快速播放，播放性能最好的文件数据块组织形式为连续结构</p>
</li>
<li><p>文件系统为每个文件创建一张索引表，存放文件数据块的磁盘存放位置</p>
</li>
<li><p>在FAT中第i项存放第i项的下一项的项号</p>
</li>
<li><p>光盘、U盘、磁盘既可以随机访问又可以顺序</p>
</li>
<li><p>索引顺序文件既可以顺序访问又可以随机访问</p>
</li>
</ul>
<h2 id="虚拟文件系统"><a href="#虚拟文件系统" class="headerlink" title="虚拟文件系统"></a>虚拟文件系统</h2><img src="\images\clip_image012.png" alt="哎呀，图片不见了" style="zoom:100%;" />

<p>采用面向对象技术（多态技术）同时支持多种类型的文件系统</p>
<ol>
<li><p>提供清晰的VFS接口，将文件系统的通用操作与实现分开，屏蔽底层具体文件系统的实现差异</p>
</li>
<li><p>VFS要求下层文件系统必须实现某些规定的函数功能，一个新的文件系统想要在操作系统上被使用，就必须满足该操作系统的VFS要求</p>
</li>
<li><p>调用open后，VFS将具体文件系统传来的不同的FCB统一转换为vnode（vnode只存在于主存）</p>
</li>
<li><p>函数功能指针：记录对于的文件系统提供的函数</p>
</li>
<li><p>唯一的表示网络上的文件</p>
</li>
</ol>
<h3 id="VFS四个对象"><a href="#VFS四个对象" class="headerlink" title="VFS四个对象"></a>VFS四个对象</h3><p>A.   超级块：一个超级块对应一个文件系统</p>
<p>B.   索引结点inode：保存元数据（文件大小、设备标识符、指向该内容的磁盘区块指针等）</p>
<p>C.  目录项：描述文件的逻辑属性，只存在内存中。目录也是一种文件</p>
<p>D.  文件对象：进程通过文件描述符操作文件</p>
<p>数据有元数据+数据本身</p>
<p>inode也有两种：VFS的inode（内存中）、具体文件系统的inode磁盘中。需要将磁盘inode调进填充内存中的inode，才能使用磁盘inode</p>
<p>一个文件对应一个inode，inode号唯一</p>
<h2 id="文件系统挂载"><a href="#文件系统挂载" class="headerlink" title="文件系统挂载"></a>文件系统挂载</h2><p>挂载：将新的文件系统关联到当前根文件的文件系统</p>
<p>挂载点：要挂载文件系统的访问入口；挂载点必须事先存在；挂载点下的原有文件被暂时隐藏</p>
<ol>
<li><p>在VFS中注册新挂载的文件系统，内存中的挂载表包含每个文件系统的相关信息，包括文件系统类型、容量大小等</p>
</li>
<li><p>新挂载的文件系统，要向VFS提供一个函数地址列表（函数功能指针）</p>
</li>
<li><p>将新文件系统加到挂载点，即将新文件系统挂载到某个父目录下</p>
</li>
</ol>
<h1 id="IO管理"><a href="#IO管理" class="headerlink" title="IO管理"></a>IO管理</h1><ul>
<li><p>共享设备必须是可寻址可随机访问的设备</p>
</li>
<li><p>分享共享设备不会导致进程死锁</p>
</li>
<li><p>实现的功能有：实现外围设备的分配和回收；实现虚拟设备；实现对磁盘的驱动调度</p>
</li>
<li><p>直接存取存储器（磁盘）既不像RAM那样随机访问任何一个存储单元，又不像顺序存取存储器那样完全顺序存取，而是介于两者之间，存取信息时通常先寻找存储器的某个小区域（磁道），再在小区域内顺序查找。</p>
</li>
</ul>
<h2 id="设备控制器"><a href="#设备控制器" class="headerlink" title="设备控制器"></a>设备控制器</h2><ul>
<li><p>为了便于上层软件编制，设备控制器通常要提供控制寄存器（存放CPU来的控制信号）、状态寄存器（设备来的设备状态信息）和控制命令</p>
</li>
<li><p>设备控制器中实现设备控制的是IO逻辑</p>
</li>
<li><p>通道控制设备控制器、设备控制器控制设备工作</p>
</li>
<li><p>设备控制器需要请求通道为其服务。因此，控制器控制表COCT中定有一个表项存放指向相应通道控制表CHCT的指针</p>
</li>
<li><p>一个通道为多个设备控制器服务。因此，CHCT中有一个指针指向一张记录CHCT提供服务的设备控制器的表（CHCT与COCT是一对多的关系）</p>
</li>
<li><p>通道技术是硬件技术</p>
</li>
<li><p>通道指令保存在主存中，是一系列通道指令</p>
</li>
<li><p>IO设备与存储设备使用DMA方式进行数据交换，不经过CPU来完成</p>
</li>
<li><p>堆栈指针寄存器不属于DMA控制器，内存地址寄存器属于（用来存放DMA作业时的源地址与目标地址）</p>
</li>
<li><p>将系统中的每台设备按照某种原则编号，这些编号作为区分硬件和识别设备的代号，该编号称为设备的绝对号</p>
</li>
<li><p>将系统调用的参数翻译成设备操作命令的工作由设备无关的操作系统软件实现</p>
</li>
<li><p>IO层次组织排列为：用户级IO软件、设备无关软件（系统调用处理程序）、设备驱动程序、中断处理程序</p>
</li>
<li><p>用户下达read指令；设备无关软件对read指令进行解析；设备驱动程序针对设备解析为不同的指令；中断服务程序中断CPU正在运行的进程，执行解析后的read命令；最后命令到达硬件设备，硬件设备控制器按照相应命令操控硬件，完成相应功能</p>
</li>
</ul>
<blockquote>
<p>系统将数据从磁盘读入内存包括：</p>
<ol>
<li><p>初始化DMA控制器并启动磁盘</p>
</li>
<li><p>从磁盘传输一块数据到内存缓冲区</p>
</li>
<li><p>DMA控制器发出中断请求</p>
</li>
<li><p>执行DMA结束中断处理程序</p>
</li>
</ol>
</blockquote>
<ul>
<li><p>设备独立性：用户编程时使用的设备与实际设备无关</p>
</li>
<li><p>磁盘属于共享设备</p>
</li>
<li><p>SPOOLing技术将独占设备变成共享设备，但是只是逻辑上改变。实际设备仍是独占设备</p>
</li>
<li><p>SPOOLing系统由预输入程序、预输出程序、井管理程序组成</p>
</li>
<li><p>使用SPOOLing技术时，用户打印结果首先被送到磁盘的固定区域（输入井中）</p>
</li>
<li><p>SPOOLing技术可以当输出设备忙时，减少进程等待时间，加快作业完成速度</p>
</li>
<li><p>图形用户界面使用鼠标、多任务操作系统下磁带驱动器、包含用户文件的磁盘驱动器、使用存储器映射IO，直接和总线相连的图形卡都需要使用缓冲技术（IO速度不相匹配就需要使用缓冲）</p>
</li>
</ul>
<h2 id="磁盘初始化"><a href="#磁盘初始化" class="headerlink" title="磁盘初始化"></a>磁盘初始化</h2><ul>
<li><p>执行顺序：ROM引导程序、磁盘引导程序、分区引导程序、操作系统初始化程序</p>
</li>
<li><p>物理格式化（低级格式化）：对每个磁道划分扇区，安排扇区在磁道中的排列顺序，并对已损坏的磁道和扇区做“坏”标记</p>
</li>
<li><p>分区：将磁盘分为C盘、D盘等相互独立的分区</p>
</li>
<li><p>进行逻辑格式化（高级格式化）：对扇区进行逻辑编号、建立逻辑盘的引导记录、文件分配表、文件目录表和数据区等，还包括建立文件系统根目录</p>
</li>
<li><p>硬盘的操作系统引导扇区产生在对硬盘进行高级格式化时</p>
</li>
</ul>
]]></content>
      <tags>
        <tag>408</tag>
      </tags>
  </entry>
  <entry>
    <title>数据库</title>
    <url>/2022/03/18/%E6%95%B0%E6%8D%AE%E5%BA%93/</url>
    <content><![CDATA[<p><a href="/download/%E6%95%B0%E6%8D%AE%E5%BA%93.xmind">Xmind下载</a></p>
<span id="more"></span>
<h2 id="SQL"><a href="#SQL" class="headerlink" title="SQL"></a>SQL</h2><h3 id="操作符的计算顺序"><a href="#操作符的计算顺序" class="headerlink" title="操作符的计算顺序"></a>操作符的计算顺序</h3><p>AND在计算次序中优先级比OR更高，因此下列命令会返回不符合我们预期的结果</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> id</span><br><span class="line"><span class="keyword">FROM</span> mytable</span><br><span class="line"><span class="keyword">WHERE</span> col1<span class="operator">=</span><span class="number">1</span> <span class="keyword">OR</span> col2<span class="operator">=</span><span class="number">2</span> <span class="keyword">AND</span> col3<span class="operator">=</span><span class="number">3</span>;</span><br></pre></td></tr></table></figure>

<p>此时的计算顺序并不是从左至右，而是先计算<code>col2=2 AND col3=3</code>，如果想按由左至右的顺序计算，则应该按照如下写法</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> id</span><br><span class="line"><span class="keyword">FROM</span> mytable</span><br><span class="line"><span class="keyword">WHERE</span> (col1<span class="operator">=</span><span class="number">1</span> <span class="keyword">OR</span> col2<span class="operator">=</span><span class="number">2</span>) <span class="keyword">AND</span> col3<span class="operator">=</span><span class="number">3</span>;</span><br></pre></td></tr></table></figure>

<p>因为圆括号具有比AND与OR更高的计算顺序</p>
<h3 id="通配符"><a href="#通配符" class="headerlink" title="通配符"></a>通配符</h3><p>在where子句中使用LIKE谓词使用搜索模式利用通配符匹配而不是直接相等进行比较</p>
<p>%（百分号）：表示<strong>任何字符</strong>出现任何次数（可以是0次）</p>
<p>_（下划线）：表示匹配任何<strong>一个字符</strong></p>
<blockquote>
<p>需要注意的是通配符搜索处理需要花费较多的时间，因此不要过度使用通配符。</p>
</blockquote>
<h4 id="通配符与正则表达式的区别"><a href="#通配符与正则表达式的区别" class="headerlink" title="通配符与正则表达式的区别"></a>通配符与正则表达式的区别</h4><p>LIKE谓词是对整个列进行匹配（利用通配符%val%的形式可以实现对列值的匹配），REGEXP在列值内进行匹配，如果被匹配的文本在列值中出现，REGEXP就会找到它。同时正则如果使用^和￥定位符也可以做到匹配整个列值。</p>
<p>更详细的正则语法可以参考：<a href="https://www.runoob.com/regexp/regexp-tutorial.html">正则表达式 – 教程 | 菜鸟教程 (runoob.com)</a></p>
<h3 id="别名"><a href="#别名" class="headerlink" title="别名"></a>别名</h3><p>在SELECT子句中，我们常会使用聚集函数或是Conct拼接来输出想要的结果。需要注意的是，使用Concat拼接后的列是没有名字的，它只是一个值。而一个未命名的列不能用于客户机的应用中，为了之后更好的引用SQL支持使用AS来对一个字段或值进行替换名。同时AS还可以在实际表列中出现不符合规定的字符（空格）或是原有名字存在混淆容易误解的情况下重新命名。</p>
<h3 id="日期和时间处理函数"><a href="#日期和时间处理函数" class="headerlink" title="日期和时间处理函数"></a>日期和时间处理函数</h3><p><img src="\images\image-20220306115702702.png" alt="哎呀，图片不见了" style="zoom:100%;" /><img src="/" alt="image-20220306115702702"></p>
<h3 id="HAVING和WHERE的区别"><a href="#HAVING和WHERE的区别" class="headerlink" title="HAVING和WHERE的区别"></a>HAVING和WHERE的区别</h3><p>WHERE子句是在数据分组前进行约束，是约束声明，不能使用聚合函数。HAVING是在分组后进行过滤，不能使用聚合函数。也就是说HAVING是对已经经过WHERE过滤后的分组数据进行处理的。因此尽可能在where子句中对查询结果进行约束。</p>
<h3 id="连接"><a href="#连接" class="headerlink" title="连接"></a>连接</h3><h4 id="自然连接"><a href="#自然连接" class="headerlink" title="自然连接"></a>自然连接</h4><p>自然连接首先形成它的两个参数的笛卡尔积，然后基于两个关系模式中都出现的属性上的相等性进行选择，最后去除重复属性。</p>
<blockquote>
<p>我们很有可能不会使用到不是自然连接的内部连接</p>
</blockquote>
<h4 id="内连接和自然连接的区别"><a href="#内连接和自然连接的区别" class="headerlink" title="内连接和自然连接的区别"></a>内连接和自然连接的区别</h4><p>内连接提供连接的列，而自然连接自动连接所有同名列</p>
<h4 id="左外连接与右外连接"><a href="#左外连接与右外连接" class="headerlink" title="左外连接与右外连接"></a>左外连接与右外连接</h4><p>左外连接：取出左侧关系中所有与右侧关系的任一元组都不匹配的元组。用空值填充所有来自右侧关系的属性，再把产生的元组加入自然连接结果中。即最后得到的结果是左表中的所有行＋右表中对应的行（不匹配的行填充空值）</p>
<h4 id="条件语句ON与USING的区别"><a href="#条件语句ON与USING的区别" class="headerlink" title="条件语句ON与USING的区别"></a>条件语句ON与USING的区别</h4><p>USING：只需要在指定属性上进行取值匹配</p>
<p>ON：需要接一个predicate，是在参与连接的关系上设置通用的谓词</p>
<h3 id="SQL注入"><a href="#SQL注入" class="headerlink" title="SQL注入"></a>SQL注入</h3><p>SQL注入的原理是将SQL代码伪装到输入参数中，传递到服务器解析并执行的一种攻击手法。也就是说，在一些对SERVER端发起的请求参数中植入一些SQL代码，SERVER端在执行SQL操作时，会拼接对应参数，同时也将一些SQL注入攻击的“SQL”拼接起来，导致会执行一些预期之外的操作。</p>
<h4 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h4><ol>
<li>严格的参数校验：不符合参数类型要求的请求即认为是非法的</li>
<li>SQL预编译：在服务器启动时，MySQL Client把SQL语句的模板（变量采用<strong>占位符</strong>进行占位）发送给MySQL服务器，MySQL服务器对SQL语句的模板进行编译，编译之后根据语句的优化分析对相应的索引进行优化，在最终绑定参数时把相应的参数传送给MySQL服务器，直接进行执行，节省了SQL查询时间，以及MySQL服务器的资源，达到一次编译、多次执行的目的，除此之外，还可以防止SQL注入。</li>
</ol>
<h2 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h2><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ol>
<li>通过创建唯一索引，可以保证数据库表中每一行数据的<strong>唯一性</strong>。</li>
<li>可以大大加快数据的<strong>查询速度</strong>，这也是创建索引的主要原因。</li>
<li>在实现数据的参考完整性方面，可以加速表和表之间的<strong>连接</strong>。</li>
<li>在使用<strong>分组和排序子句</strong>进行数据查询时，也可以显著减少查询中分组和排序的时间。（可以在频繁进行排序或分组（即进行group by或order by操作）的列上建立索引，如果待排序的列有多个，可以在这些列上建立组合索引。）</li>
</ol>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ol>
<li>创建索引和维护索引要<strong>耗费时间</strong>，并且随着数据量的增加所耗费的时间也会增加。</li>
<li>索引需要占<strong>磁盘空间</strong>，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果有大量的索引，索引文件可能比数据文件更快达到最大文件尺寸。</li>
<li>当对表中的数据进行增加、删除和修改的时候，索引也要动态地<strong>维护</strong>，这样就降低了数据的维护速度。</li>
</ol>
<h3 id="MySQL的Hash索引和B树索引的区别"><a href="#MySQL的Hash索引和B树索引的区别" class="headerlink" title="MySQL的Hash索引和B树索引的区别"></a>MySQL的Hash索引和B树索引的区别</h3><ol>
<li>hash索引进行等值查询更快(一般情况下)，但是却无法进行范围查询。因为在hash索引中经过hash函数建立索引之后，索引的顺序与原顺序<strong>无法保持一致</strong>，不能支持范围查询。而B+树的的所有节点皆遵循(左节点小于父节点，右节点大于父节点，多叉树也类似)，天然支持范围。</li>
<li>hash索引不支持使用索引进行<strong>排序</strong>，原理同上。</li>
<li>hash索引不支持<strong>模糊查询</strong>以及多列索引的<strong>最左前缀匹配</strong>，原理也是因为hash函数的不可预测。</li>
<li>hash索引任何时候都避免不了<strong>回表查询数据</strong>，而B+树在符合某些条件(聚簇索引，覆盖索引等)的时候可以只通过索引完成查询。</li>
<li>hash索引虽然在<strong>等值查询上较快</strong>，但是不稳定，性能不可预测，当某个键值存在大量重复的时候，发生<strong>hash碰撞</strong>，此时效率可能极差。而B+树的查询效率比较稳定，对于所有的查询都是从根节点到叶子节点，且树的高度较低。</li>
</ol>
<h3 id="最左匹配原则"><a href="#最左匹配原则" class="headerlink" title="最左匹配原则"></a>最左匹配原则</h3><p>在联合索引中，如果 SQL 语句中用到了<strong>联合索引</strong>中的最左边的索引，那么这条 SQL 语句就可以利用这个联合索引去进行匹配。</p>
<p>数据库的索引是使用B+树来实现的，联合索引也是一颗B+树。而B+树只能根据一个列值来构建，也就是联合索引的最左字段。一个形如(a,b,c)联合索引的 b+ 树，其中的<strong>非叶子节点</strong>存储的是第一个关键字的索引 a，而<strong>叶子节点</strong>存储的是三个关键字的数据。这里可以看出 a 是有序的，而 b，c 都是无序的。但是当在 a 相同的时候，b 是有序的，b 相同的时候，c 又是有序的。</p>
<p>这也就是大部分索引失效的原因所在，<code>select * from t where a=5 and b&gt;0 and c =1;</code>为例，使用范围查询查找到b后，c就是无序的状态，因此无法使用联合索引确定c的位置</p>
<h3 id="索引失效的场景"><a href="#索引失效的场景" class="headerlink" title="索引失效的场景"></a>索引失效的场景</h3><ol>
<li>不满足最左匹配原则：在where子句中，最左谓词中含有索引项才会使用索引；<strong>当遇到范围查询(&gt;、&lt;、between、like)也会停止匹配</strong></li>
<li>使用<code>select *</code>：使用*查询所有列的数据，大概率会非索引的数据，这样会使用全表扫描。而如果查询的都是索引列，被称为覆盖索引，会提升查询效率</li>
<li>索引列上有计算</li>
<li>索引列使用函数：3、4索引失效的原因相同。索引列如果进行计算或是函数计算，mysql在B+树上的索引搜索就会失效</li>
<li>字段类型不同：mysql会将字符串类型自动转换为int类型，而int无法转换为varchar</li>
<li>like左边包含%</li>
<li>列对比：如果把两个单独建了索引的列，用来做列对比时，索引会失效。</li>
<li>使用or关键字：如果使用了<code>or</code>关键字，那么它<strong>前面和后面</strong>的字段都要加索引，不然所有的索引都会失效</li>
<li>not in和not exists：主键字段中使用not in关键字查询数据范围，仍然可以走索引。而普通索引字段使用了<code>not in</code>关键字查询数据范围，索引会失效。如果sql语句中使用<code>not exists</code>时，索引也会失效</li>
<li>order by：<code>order by</code>后面的条件，也要遵循联合索引的最左匹配原则。同时还需要添加limit关键字,或是使用where子句；<code>order by</code>后面如果包含了联合索引的多个排序字段，只要它们的排序规律是相同的（要么同时升序，要么同时降序），也可以走索引。</li>
</ol>
<h2 id="事务"><a href="#事务" class="headerlink" title="事务"></a>事务</h2><h3 id="ACID特性"><a href="#ACID特性" class="headerlink" title="ACID特性"></a>ACID特性</h3><ul>
<li>A（atomicity），原子性。原子性指整个数据库事务是不可分割的工作单位。只有使事务中所有的数据库操作都执行成功，整个事务的执行才算成功。事务中任何一个SQL语句执行失败，那么已经执行成功的SQL语句也必须撤销，数据库状态应该退回到执行事务前的状态。</li>
<li>C（consistency），一致性。一致性指事务将数据库从一种状态转变为另一种一致的状态。在事务开始之前和事务结束以后，数据库的完整性约束没有被破坏。</li>
<li>I（isolation），隔离性。事务的隔离性要求每个读写事务的对象与其他事务的操作对象能相互分离，即该事务提交前对其他事务都不可见，这通常使用锁来实现。</li>
<li>D（durability） ，持久性。事务一旦提交，其结果就是永久性的，即使发生宕机等故障，数据库也能将数据恢复。持久性保证的是事务系统的高可靠性，而不是高可用性。</li>
</ul>
<h3 id="事务类型"><a href="#事务类型" class="headerlink" title="事务类型"></a>事务类型</h3><ul>
<li>扁平事务：是事务类型中最简单的一种，而在实际生产环境中，这可能是使用最为频繁的事务。在扁平事务中，所有操作都处于同一层次，其由BEGIN WORK开始，由COMMIT WORK或ROLLBACK WORK结束。处于之间的操作是原子的，要么都执行，要么都回滚。</li>
<li>带有<strong>保存点</strong>的扁平事务：除了支持扁平事务支持的操作外，允许在事务执行过程中回滚到同一事务中较早的一个状态，这是因为可能某些事务在执行过程中出现的错误并不会对所有的操作都无效，放弃整个事务不合乎要求，开销也太大。保存点（savepoint）用来通知系统应该记住事务当前的状态，以便以后发生错误时，事务能回到该状态。</li>
<li>链事务：可视为保存点模式的一个变种。链事务的思想是：在提交一个事务时，释放不需要的数据对象，将必要的处理上下文隐式地传给下一个要开始的事务。注意，提交事务操作和开始下一个事务操作将合并为一个原子操作。这意味着下一个事务将看到上一个事务的结果，就好像在一个事务中进行的。</li>
<li>嵌套事务：是一个层次结构框架。有一个顶层事务（top-level transaction）控制着各个层次的事务。顶层事务之下嵌套的事务被称为子事务（subtransaction），其控制每一个局部的变换。</li>
<li>分布式事务：通常是一个在分布式环境下运行的扁平事务，因此需要根据数据所在位置访问网络中的不同节点。对于分布式事务，同样需要满足ACID特性，要么都发生，要么都失效。</li>
</ul>
<h3 id="ACID特性的实现"><a href="#ACID特性的实现" class="headerlink" title="ACID特性的实现"></a>ACID特性的实现</h3><h4 id="实现原子性"><a href="#实现原子性" class="headerlink" title="实现原子性"></a>实现原子性</h4><p>原子性实现的关键，是当事务回滚时能够撤销所有已经成功执行的sql语句。InnoDB实现回滚靠的是undo log，当事务对数据库进行修改时，InnoDB会生成对应的undo log。如果事务执行失败或调用了rollback，导致事务需要回滚，便可以利用undo log中的信息将数据回滚到修改之前的样子。</p>
<h4 id="实现持久性"><a href="#实现持久性" class="headerlink" title="实现持久性"></a>实现持久性</h4><p>数据库使用redo log实现持久性。当数据修改时，除了修改Buffer Pool中的数据，还会在redo log记录这次操作。当事务提交时，会调用fsync接口对redo log进行刷盘。如果MySQL宕机，重启时可以读取redo log中的数据，对数据库进行恢复。redo log采用的是WAL（Write-ahead logging，预写式日志），所有修改先写入日志，再更新到Buffer Pool，保证了数据不会因MySQL宕机而丢失，从而满足了持久性要求。</p>
<blockquote>
<p>Buffer Pool</p>
<p>当向数据库写入数据时，会首先写入Buffer Pool，Buffer Pool中修改的数据会定期刷新到磁盘中（这一过程称为刷脏）。</p>
<p>而redo log写入磁盘比直接将Buffer Pool中修改的数据写入磁盘(即刷脏)要快</p>
<ol>
<li>刷脏是随机IO，因为每次修改的数据位置随机，但写redo log是追加操作，属于顺序IO。</li>
<li>刷脏是以数据页（Page）为单位的，MySQL默认页大小是16KB，一个Page上一个小修改都要整页写入。而redo log中只包含真正需要写入的部分，无效IO大大减少。</li>
</ol>
</blockquote>
<h4 id="实现隔离性"><a href="#实现隔离性" class="headerlink" title="实现隔离性"></a>实现隔离性</h4><ol>
<li>锁机制保证写写操作的隔离性。事务在修改数据之前，需要先获得相应的锁。获得锁之后，事务便可以修改数据。该事务操作期间，这部分数据是锁定的，其他事务如果需要修改数据，需要等待当前事务提交或回滚后释放锁。</li>
<li>MVCC保证写读操作的隔离性。读不加锁，因此读写不冲突，并发性能好。</li>
</ol>
<blockquote>
<p>锁</p>
<p>InnoDB存储引擎支持多粒度锁定，按照粒度，锁可以分为表锁、行锁以及其他位于二者之间的锁。</p>
<ol>
<li>表锁在操作数据时会锁定整张表，并发性能较差。</li>
<li>行锁则只锁定需要操作的数据，并发性能好。但是由于加锁本身需要消耗资源，因此在锁定数据较多情况下使用表锁可以节省大量资源。</li>
</ol>
</blockquote>
<blockquote>
<p>MVCC（多版本的并发控制协议）</p>
<ol>
<li>隐藏列：InnoDB中每行数据都有隐藏列，隐藏列中包含了本行数据的事务id、指向undo log的指针等。</li>
<li>基于undo log的版本链：每行数据的隐藏列中包含了指向undo log的指针，而每条undo log也会指向更早版本的undo log，从而形成一条版本链。</li>
<li>ReadView：通过隐藏列和版本链，MySQL可以将数据恢复到指定版本。但是具体要恢复到哪个版本，则需要根据ReadView来确定。所谓ReadView，是指事务（记做事务A）在某一时刻给整个事务系统（trx_sys）打快照，之后再进行读操作时，会将读取到的数据中的事务id与trx_sys快照比较，从而判断数据对该ReadView是否可见，即对事务A是否可见。</li>
</ol>
</blockquote>
<h4 id="实现一致性"><a href="#实现一致性" class="headerlink" title="实现一致性"></a>实现一致性</h4><ul>
<li>保证原子性、持久性和隔离性，如果这些特性无法保证，事务的一致性也无法保证。</li>
<li>数据库本身提供保障，例如不允许向整形列插入字符串值、字符串长度不能超过列的限制等。</li>
<li>应用层面进行保障，例如如果转账操作只扣除转账者的余额，而没有增加接收者的余额，无论数据库实现的多么完美，也无法保证状态的一致。</li>
</ul>
<h3 id="事务隔离级别"><a href="#事务隔离级别" class="headerlink" title="事务隔离级别"></a>事务隔离级别</h3><table>
<thead>
<tr>
<th align="center">隔离级别</th>
<th align="left">脏读</th>
<th align="left">不可重复读</th>
<th align="center">幻读</th>
</tr>
</thead>
<tbody><tr>
<td align="center">READ UNCOMMITTED（读未提交）</td>
<td align="left">可能</td>
<td align="left">可能</td>
<td align="center">可能</td>
</tr>
<tr>
<td align="center">READ COMMITTED（读提交 ）</td>
<td align="left">不可能</td>
<td align="left">可能</td>
<td align="center">可能</td>
</tr>
<tr>
<td align="center">REPEATABLE READ（可重复读 ）</td>
<td align="left">不可能</td>
<td align="left">不可能</td>
<td align="center">可能</td>
</tr>
<tr>
<td align="center">SERIALIZABLE（串行化）</td>
<td align="left">不可能</td>
<td align="left">不可能</td>
<td align="center">不可能</td>
</tr>
</tbody></table>
<ul>
<li>脏读：当前事务(A)中可以读到其他事务(B)未提交的数据（脏数据），这种现象是脏读。</li>
<li>不可重复读：在事务A中先后两次读取同一个数据，两次读取的结果不一样，这种现象称为不可重复读。脏读与不可重复读的区别在于：前者读到的是其他事务未提交的数据，后者读到的是其他事务已提交的数据。</li>
<li>幻读：在事务A中按照某个条件先后两次查询数据库，两次查询结果的条数不同，这种现象称为幻读。不可重复读与幻读的区别可以通俗的理解为：前者是数据变了，后者是数据的行数变了。</li>
</ul>
<h4 id="四种隔离级别的实现机制"><a href="#四种隔离级别的实现机制" class="headerlink" title="四种隔离级别的实现机制"></a>四种隔离级别的实现机制</h4><ol>
<li>READ UNCOMMITTED &amp; READ COMMITTED：通过<strong>Record Lock算法</strong>实现了行锁，但READ UNCOMMITTED允许读取未提交数据，所以存在脏读问题。而READ COMMITTED允许读取提交数据，所以不存在脏读问题，但存在不可重复读问题。</li>
<li>REPEATABLE READ：使用<strong>Next-Key Lock算法</strong>实现了行锁，并且不允许读取已提交的数据，所以解决了不可重复读的问题。另外，该算法包含了间隙锁，会锁定一个范围，因此也解决了幻读的问题。</li>
<li>SERIALIZABLE：对每个SELECT语句后自动加上<code>LOCK IN SHARE MODE</code>，即为每个读取操作加一个<strong>共享锁</strong>。因此在这个事务隔离级别下，读占用了锁，对一致性的非锁定读不再予以支持。</li>
</ol>
<h2 id="锁"><a href="#锁" class="headerlink" title="锁"></a>锁</h2><h4 id="锁（行级锁）的类型"><a href="#锁（行级锁）的类型" class="headerlink" title="锁（行级锁）的类型"></a>锁（行级锁）的类型</h4><ul>
<li>共享锁（S Lock），允许事务读一行数据。</li>
<li>排他锁（X Lock），允许事务删除或更新一行数据。</li>
</ul>
<p>如果一个事务T1已经获得了行r的共享锁，那么另外的事务T2可以立即获得行r的共享锁，因为读取并没有改变行r的数据，称这种情况为锁兼容。但若有其他的事务T3想获得行r的排他锁，则其必须等待事务T1、T2释放行r上的共享锁，这种情况称为锁不兼容。<br><img src="\images\image-20220316200553339.png" alt="哎呀，图片不见了" style="zoom:100%;" /></p>
<blockquote>
<p>InnoDB行级锁是通过给索引上的索引项加锁来实现的。只有通过索引条件检索数据，InnoDB才使用行级锁，否则，InnoDB将使用表锁。当表中锁定其中的某几行时，不同的事务可以使用不同的索引锁定不同的行。另外，不论使用主键索引、唯一索引还是普通索引，InnoDB都会使用行锁来对数据加锁。</p>
</blockquote>
<h4 id="意向锁（表级锁）"><a href="#意向锁（表级锁）" class="headerlink" title="意向锁（表级锁）"></a>意向锁（表级锁）</h4><ul>
<li>意向共享锁（IS Lock），事务想要获得一张表中某几行的共享锁。</li>
<li>意向排他锁（IX Lock），事务想要获得一张表中某几行的排他锁。</li>
</ul>
<p>由于InnoDB存储引擎支持的是行级别的锁，因此意向锁其实不会阻塞除全表扫以外的任何请求。<br><img src="\images\image-20220316200714312.png" alt="哎呀，图片不见了" style="zoom:100%;" /></p>
<h4 id="锁的算法"><a href="#锁的算法" class="headerlink" title="锁的算法"></a>锁的算法</h4><ul>
<li>Record Lock：单个行记录上的锁。</li>
<li>Gap Lock：间隙锁，锁定一个范围，但不包含记录本身。它的作用是为了阻止多个事务将记录插入到同一范围内，而这会导致幻读问题的产生。</li>
<li>Next-Key Lock∶Gap Lock+Record Lock，锁定一个范围，并且锁定记录本身。</li>
</ul>
<h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2><h3 id="优化查询"><a href="#优化查询" class="headerlink" title="优化查询"></a>优化查询</h3><p>可以通过使用索引、使用连接代替子查询的方式来提高查询速度。</p>
<ol>
<li>需要注意索引失效的情况</li>
<li>优化子查询：子查询可以一次性完成很多逻辑上需要多个步骤才能完成的SQL操作。但执行效率不高。执行子查询时，MySQL需要为内层查询语句的查询结果建立一个临时表。在MySQL中，可以使用连接（JOIN）查询来替代子查询。连接查询不需要建立临时表，其速度比子查询要快，如果查询中使用索引，性能会更好。</li>
</ol>
<h3 id="优化插入"><a href="#优化插入" class="headerlink" title="优化插入"></a>优化插入</h3><h4 id="MyISAM引擎"><a href="#MyISAM引擎" class="headerlink" title="MyISAM引擎"></a>MyISAM引擎</h4><ol>
<li><p>禁用索引：对于非空表，插入记录时，MySQL会根据表的索引对插入的记录建立索引。如果插入大量数据，建立索引会降低插入记录的速度。为了解决这种情况，可以在插入记录之前禁用索引，数据插入完毕后再开启索引。对于空表批量导入数据，则不需要进行此操作，因为MyISAM引擎的表是在导入数据之后才建立索引的。</p>
</li>
<li><p>禁用唯一性检查：插入数据时，MySQL会对插入的记录进行唯一性校验。这种唯一性校验也会降低插入记录的速度。为了降低这种情况对查询速度的影响，可以在插入记录之前禁用唯一性检查，等到记录插入完毕后再开启。</p>
</li>
<li><p>使用批量插入：插入多条记录时，可以使用一条INSERT语句插入多条记录会比使用一条INSERT语句插入一条记录的插入速度更快</p>
</li>
<li><p>使用LOAD DATA INFILE批量导入：当需要批量导入数据时，<code>LOAD DATA INFILE</code>语句导入速度比INSERT语句快</p>
</li>
</ol>
<h4 id="InnoDB引擎"><a href="#InnoDB引擎" class="headerlink" title="InnoDB引擎"></a>InnoDB引擎</h4><ol>
<li>禁用外键检查：插入数据之前执行禁止对外键的检查，数据插入完成之后再恢复对外键的检查。</li>
<li>禁用唯一性检查</li>
<li>禁用自动提交：插入数据之前禁止事务的自动提交，数据导入完成之后，执行恢复自动提交操作。</li>
</ol>
<h2 id="范式"><a href="#范式" class="headerlink" title="范式"></a>范式</h2><h3 id="第一范式-1NF"><a href="#第一范式-1NF" class="headerlink" title="第一范式(1NF)"></a>第一范式(1NF)</h3><p>在关系模型中，对于添加的一个规范要求，所有的域都应该是<strong>原子性</strong>的.如果实体中的某个属性有多个值，必须拆分为不同的属性。</p>
<h3 id="第二范式-2NF"><a href="#第二范式-2NF" class="headerlink" title="第二范式(2NF)"></a>第二范式(2NF)</h3><p>在1NF的基础上，非码属性必须完全依赖于<strong>候选码</strong>（在1NF基础上消除非主属性对主码的部分函数依赖）。第二范式要求数据库表中的每个实例或记录必须可以被唯一地区分。选取一个能区分每个实体的属性或属性组，作为实体的<strong>唯一标识</strong>。</p>
<h3 id="第三范式-3NF"><a href="#第三范式-3NF" class="headerlink" title="第三范式(3NF)"></a>第三范式(3NF)</h3><p>在2NF基础上，任何非主属性不依赖于其它非主属性（在2NF基础上消除传递依赖）。第三范式要求一个关系中不包含已在其它关系已包含的非主关键字信息。</p>
<p>解决方法：将非主属性与其依赖的码拿出来单独成表，并设置被依赖的属性为主键，在原表中用外键表示</p>
<blockquote>
<p>第三范式可以减少数据库数据的冗余<br>如：订单表(订单号, 订购日期, 顾客编号, 顾客名)并不符合第三范式，（订单号、顾客编号、顾客名形成了依赖传递）<br>可以修改为：<br>订单表(订单号，订购日期，顾客编号)<br>顾客表(顾客编号，顾客名)</p>
</blockquote>
<h3 id="BC范式"><a href="#BC范式" class="headerlink" title="BC范式"></a>BC范式</h3><p>消除主属性对于码的部分函数依赖与传递函数依赖，即每个表中只有一个候选键。</p>
<p>解决方法：将多余的候选码提取出来单独成表，将原表中的主键放入该表</p>
]]></content>
  </entry>
  <entry>
    <title>数据结构</title>
    <url>/2022/03/11/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/</url>
    <content><![CDATA[<p>算法应该是问题求解步骤的描述<br><a href="/download/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84.xmind">Xmind下载</a></p>
<span id="more"></span>

<h1 id="线性表"><a href="#线性表" class="headerlink" title="线性表"></a>线性表</h1><ul>
<li><p>顺序存储方式适用于图、树、线性表</p>
</li>
<li><p>定义一个顺序表需要包含的数据成员为：数组指针*data、表中元素个数n、表的大小MaxSize2 xde</p>
</li>
<li><p>kmp算法中，next数组看next[j],j，nextval数组中看next[i]（i为本位、j为上一位）</p>
</li>
</ul>
<h1 id="树"><a href="#树" class="headerlink" title="树"></a>树</h1><ul>
<li>出现树的题要考虑树是一颗空树</li>
<li>树中结点数等于所有结点的度数之和加1</li>
<li>高度为n个结点的m叉树至多有$\frac{(m^k-1)}{m-1}$个结点</li>
<li>具有n个结点的m叉树最小高度为$\lceil\log_m(n(m-1)+1)\rceil$（向上取整）</li>
<li>非空二叉树上的叶子结点数等于度为2的结点数加1（$n_0&#x3D;n_2+1$）</li>
<li>具有n个结点的完全二叉树的高度为$\lceil\log_2(n+1)\rceil$或$\lfloor\log_2n\rfloor+1$</li>
<li>非空二叉树上第k层至多有$2^{k-1}$个结点</li>
<li>高度为h的二叉树至多有$2^k-1$个结点</li>
<li>完全二叉树中按层次编号，则结点i的双亲为$\lfloor\frac{i}{2}\rfloor$，左孩子为$2i$，右孩子为$2i+1$，所在层次（深度）为$\lfloor\log_2i\rfloor+1$</li>
<li>红黑树黑高为h时，内部结点数最少的情况$2^k-1$，内部结点数最多的情况$2^{2k}-1$</li>
</ul>
 <img src="\images\/clip_image004.png" alt="哎呀，图片不见了" style="zoom:100%;" />

<ul>
<li>红黑树上任一查找路径的红结点数量不可能超过一半，但整棵树的红结点总数有可能过半</li>
<li>哈夫曼树的WPL计算过程中：路径长度比结点在树中的高度少1</li>
</ul>
<h1 id="图"><a href="#图" class="headerlink" title="图"></a>图</h1><ul>
<li><p>当带权连通图的任意一个环中包含的边的权值均不同时，其MST是唯一的</p>
</li>
<li><p>非连通的情况下边最多的情况：n-1个结点构成一个完全图，再任意加一条边变成连通图</p>
</li>
<li><p>一个有n条边n个结点的无向图是有环的</p>
</li>
<li><p>连通无向图边数至少是n-1条</p>
</li>
<li><p>有向图强连通情况下边最少的情况：至少需要n条边构成一个环路</p>
</li>
<li><p>在无向图中常讨论连通性，有向图中讨论强连通性</p>
</li>
<li><p>有向图的邻接表存储中，顶点v在边表中出现的次数是顶点v的入度</p>
</li>
<li><p>AOE图中只有关键路径上的活动同时减少时，才能缩短工期</p>
</li>
<li><p>增加任一关键活动的时间都会延长工程的工期</p>
</li>
<li><p>邻接表在更新边的操作时效率优于邻接矩阵（需要搜索所有结点）</p>
</li>
<li><p>广搜、深搜、拓扑排序、prime算法需要更新结点的边，因此选择邻接表表示图更好</p>
</li>
<li><p>一个单独的顶点也可以构成一个强连通分量</p>
</li>
</ul>
<h1 id="查找"><a href="#查找" class="headerlink" title="查找"></a>查找</h1><ul>
<li>折半查找判定树即在根据折半查找的过程构造的二叉树，其中mid的选择可以是向上取整、或是向下取整。但是需要在整个查找过程中保持一个取整方向</li>
<li>B+树适用于关系数据库系统的索引</li>
<li>折半查找的平均查找长度为$\log_2(n+1)+1$。对折半查找进一步优化，可以使用索引顺序查找（将查找元素分块，为每块建立一个索引项，对索引项和块使用折半查找）</li>
<li>索引块的最佳大小是$\sqrt{n}$</li>
<li>折半查找的判定树高度为$h&#x3D;\lceil\log_2(n+1)\rceil$（查找不存在的元素进行的关键字比较次数、查找存在元素的最多比较次数）</li>
</ul>
<img src="\images\image-20220315103315000.png" alt="哎呀，图片不见了" style="zoom:100%;" />

<ul>
<li><p>B树与B+树内部结点的子树个数$[\lceil m&#x2F;2\rceil,m]$只是结点所含关键字不同（B树中结点的关键字要比子树少1）</p>
</li>
<li><p>装填因子$\alpha&#x3D;\frac{n}{m}$（n：表中记录数，m：散列表长度）</p>
</li>
<li><p>选择排序和归并排序时间性能与初始状态无关</p>
</li>
<li><p>$\alpha&lt;1$不表示可以避免碰撞（与散列函数有关）</p>
</li>
<li><p>求散列表的平均查找失败次数时，可能的取值是散列函数的取值范围（mod 7，表示散列只能取0-6，因此分母就是7，与散列表长度无关）</p>
</li>
</ul>
<h1 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h1><img src="\images\image-20220315103520025.png" alt="哎呀，图片不见了" style="zoom:100%;" />

<ul>
<li><p>向具有n个关键字的堆中插入一个新元素的时间复杂度为$O(\log_2n)$，删除一个元素的时间复杂度为$O(\log_2n)$</p>
</li>
<li><p>堆排序适用于关键字较多的情况，建一个大小为n的小（大）根堆，遍历一遍 数据，即可得到最大（小）的n个数（建立小根堆，，而后依次读入余下的数，小于堆顶则舍弃，否则用该数取代堆顶并重新调整堆）</p>
</li>
<li><p>小根堆堆最大关键字一定在叶子结点上，二叉树最后一个非叶子结点存储在$\lceil n&#x2F;2\rceil$，最大关键字存储范围$\lceil n&#x2F;2\rceil+1$～$n$</p>
</li>
<li><p>构建堆从n&#x2F;2出开始调整，递减处理其他子堆（n&#x2F;2为最后一个非叶子结点）</p>
</li>
<li><p>将两个各有N个元素的有序表合并为一个有序表，最少比较次数N次，最多比较次数2N-1次</p>
</li>
<li><p>有序顺序表、二叉排序树、堆、平衡二叉树相比，堆的查找效率最低（查找时是无序的）</p>
</li>
<li><p>排序趟数与原始状态无关的有：插入排序、选择排序、基数排序</p>
</li>
<li><p>就地算法要求空间复杂度为O(1),即所用空间大小为常数</p>
</li>
<li><p>元素规模较小时，使用直接插入、冒泡、简单选择排序</p>
</li>
<li><p>元素规模中等时，使用希尔排序</p>
</li>
<li><p>元素规模较大时，使用快排、堆排序、或基数排序；</p>
</li>
<li><p>快速排序算法中，不产生有序子序列，但每趟排序后会将枢轴元素放到最终位置上</p>
</li>
<li><p>外部排序进行多路归并时，能选取的最大归并段数取决于用于归并的内存大小。内存大小为一个输出缓存区+n个输入缓冲区（n即为归并段数）</p>
</li>
</ul>
]]></content>
      <tags>
        <tag>408</tag>
      </tags>
  </entry>
  <entry>
    <title>计算机组成原理</title>
    <url>/2022/03/11/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<p><strong>计算机系统由两大部分组成：硬件和软件。软件又包括系统软件和应用软件。</strong><br><a href="/download/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86.xmind">Xmind下载</a><br><a href="/download/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E7%BB%93%E6%9E%84.xmind">Xmind下载</a></p>
<span id="more"></span>

<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><h2 id="翻译、汇编、编译"><a href="#翻译、汇编、编译" class="headerlink" title="翻译、汇编、编译"></a>翻译、汇编、编译</h2><ul>
<li><p>翻译程序：翻译一句执行一句</p>
</li>
<li><p>编译：把高级语言转化为汇编语言的过程</p>
</li>
<li><p>汇编：把汇编语言翻译成机器语言的过程</p>
</li>
<li><p>将高级语言源程序转换为机器级目标代码文件的程序是编译程序</p>
</li>
</ul>
<blockquote>
<p>翻译程序分有编译程序、解释程序</p>
<ol>
<li><p>编译程序：将高级语言全部翻译为目标程序，生成目标程序</p>
</li>
<li><p>翻译程序：不会生产目标程序（汇编程序也为翻译程序的一种）</p>
</li>
</ol>
</blockquote>
<ul>
<li><p>地址寄存器（MAR）、数据寄存器（MDR）、高速缓存（Cache）都属于存储器的一部分，但存在于CPU中。</p>
</li>
<li><p>系统软件：操作系统OS、数据库管理系统DBMS、语言处理程序、分布式软件系统、网络软件系统、标准库程序、服务性程序等</p>
</li>
<li><p>完整的计算机系统应包括配套的硬件设备与软件设备</p>
</li>
<li><p>冯诺依曼机的基本工作方式是控制流驱动方式</p>
</li>
<li><p>❌软件功能不能被硬件取代</p>
</li>
</ul>
<blockquote>
<p>逻辑等效：某一功能既可以用软件实现，也可以用硬件实现。但并不是说他们在逻辑功能上是等价的。硬件实现可扩展性差、效率高；软件实现效率低、可扩展性强。</p>
</blockquote>
<ul>
<li><p>存放欲执行指令的寄存器是IR</p>
</li>
<li><p>CPU不包括地址译码器</p>
</li>
</ul>
<blockquote>
<p>地址译码器存在主存中，负责将CPU传来的逻辑地址转换为物理地址</p>
</blockquote>
<ul>
<li><p>MAR的位数是地址码长度（可寻址范围）；MDR的位数是存储字长（存储主存的存储单元传来的数据）</p>
</li>
<li><p>在CPU寄存器中，指令寄存器IR、地址寄存器MAR、数据寄存器MDR对用户完全透明</p>
</li>
<li><p>系列机的基本特性是向后兼容</p>
</li>
</ul>
<blockquote>
<p>向后兼容：时间上向后兼容，新机器可以使用以前机器的指令系统</p>
</blockquote>
<ul>
<li>相联寄存器既可以按地址寻址也可以按内容寻址（快表）</li>
</ul>
<blockquote>
<p>存储程序原理：</p>
<p>将指令以代码形式事先输入计算机主存储器，然后按其在主存中的首地址执行程序第一条指令，以后按该程序规定的顺序执行其他指令，直至程序执行结束。</p>
</blockquote>
<blockquote>
<p>计算机按照此原理的五大功能：数据传送功能、数据存储功能、数据处理功能、操作控制功能、操作判断功能</p>
</blockquote>
<ul>
<li><p>兼容：指计算机软件与硬件的通用性，通常在同一系列不同型号的计算机间通用（❌软硬件间的通用性）</p>
</li>
<li><p>平均指令执行速度：MIPS；平均指令周期：$\frac{1}{MIPS}$</p>
</li>
</ul>
<h2 id="机器字长、指令字长、存储字长"><a href="#机器字长、指令字长、存储字长" class="headerlink" title="机器字长、指令字长、存储字长"></a>机器字长、指令字长、存储字长</h2><blockquote>
<p>机器字长、指令字长、存储字长都必须是字节的整数倍</p>
<ol>
<li><p>机器字长：计算机能够直接处理的二进制数据的位数，一般等于内部寄存器的大小。它决定了计算机的运算精度</p>
</li>
<li><p>指令字长：一个指令字包含的二进制代码位数</p>
</li>
</ol>
<p> 指令字长一般为存储字长的整数倍。若指令字长为存储字长的2倍，则需要2次访存取出一条指令。</p>
<ol start="3">
<li>存储字长：一个存储单元存储的二进制代码</li>
</ol>
</blockquote>
<h2 id="多处理器"><a href="#多处理器" class="headerlink" title="多处理器"></a>多处理器</h2><p>SISD：单指令流单数据流</p>
<p>SIMD：每条指令可以处理多个相同的数据（显卡处理图像、for循环对数组元素处理）数据级并行</p>
<p>一个指令控制部件（CU）、多个ALU、多个局部存储器、一个主存储器</p>
<p>每个执行单元有各自的寄存器组、局部存储器、地址寄存器</p>
<p>向量处理器：向量寄存器，处理对象：向量；主存储器应采用“多端口同时读取”的交叉多模块存储器</p>
<p>MISD：现实中不存在</p>
<p>MIMD：线程（进程）级并行</p>
<p>共享存储多处理器系统（SMP）：多处理共享最低级Cache、主存（共享单一物理地址空间）；LOAD、STORE指令访问存储器</p>
<p>多核处理器：一个CPU有多个处理器，（片级多处理器）、共享存储器</p>
<p>多计算机系统：消息传递通信，有独立的主存（物理地址空间独立）</p>
<h2 id="硬件多线程"><a href="#硬件多线程" class="headerlink" title="硬件多线程"></a>硬件多线程</h2><p>单核CPU中使用资源重复，为多个进程分别提供不同的寄存器组，以减少切换进程的开销</p>
<p>细粒度多线程：每个时钟周期切换线程，实现指令级并行</p>
<p>粗粒度多线程：当一个线程出现较大开销阻塞时（Cache缺失），才切换线程。切换线程时需要清空流水线（开销比细粒度大），实现指令级并行</p>
<p>同时多线程：实现指令级并行和线程级并行</p>
<h1 id="数据的表示和运算"><a href="#数据的表示和运算" class="headerlink" title="数据的表示和运算"></a>数据的表示和运算</h1><p><strong>十六进制的运算使用位运算去做，不然会出错</strong></p>
<ul>
<li><p>所有大写英文字母的ASCII码值都小于小写字母”a”的ASCII码值</p>
</li>
<li><p>1PFLOPS&#x3D;每秒一千万亿（10^15）次浮点运算</p>
</li>
<li><p>10^8&#x3D;一亿</p>
</li>
<li><p>算术右移：原码补0，反码补1，补码补符号位</p>
</li>
<li><p>算数左移：反码补1；原码，补码补0</p>
</li>
<li><p>逻辑移位：补0</p>
</li>
<li><p>机器零是指浮点运算结果在最小正数到0以及最大负数到0之间的值，计算机将其当作机器零处理</p>
</li>
<li><p>定点数中的零是实在的零</p>
</li>
<li><p><code>addw %bx,%ax</code>为AT&amp;T格式，目的寄存器为ax</p>
</li>
<li><p><code>sub bx,ax</code>为Inter格式，目的寄存器为bx</p>
</li>
</ul>
<h2 id="大小端"><a href="#大小端" class="headerlink" title="大小端"></a>大小端</h2><ul>
<li><p>现代计算机都采用字节编址方式，一个操作数有可能有多个内存地址对应</p>
</li>
<li><p>小段方案：最低有效字节存储在最小位置（多字节数据存放在连续字节序列中的排列顺序）</p>
</li>
<li><p>大端方案：最高有效字节存储在最小位置</p>
</li>
</ul>
<table>
<thead>
<tr>
<th>机器数：012345H，以小（大）段方案存放在08000H处</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>地址</td>
<td>08000H</td>
<td>08001H</td>
<td>08002H</td>
</tr>
<tr>
<td>机器数（小端）</td>
<td>45H</td>
<td>23H</td>
<td>01H</td>
</tr>
<tr>
<td>机器数（大端）</td>
<td>01H</td>
<td>23H</td>
<td>45H</td>
</tr>
</tbody></table>
<ul>
<li><p>海明码可以发现两位错误并进行以为纠错（$n+k+1\le2^k$）</p>
</li>
<li><p>被校验数据位的海明位号等于该数据位的各校验位海明位号之和（D1在H3（011），则由P1、P2进行校验）</p>
</li>
<li><p>模4补码更容易检查加减运算中的溢出问题</p>
</li>
<li><p>模4补码在存储时只需一个符号位（正确的数其两个符号位相同），只有在送入ALU计算时，才在ALU中使用双符号位</p>
</li>
<li><p>双符号位的最高符号位代表真正的符号，低位符号位用于参与位移操作以判断是否发出溢出（”01”正溢出，”10”负溢出）</p>
</li>
<li><p>原码一位乘法，符号位$x_s\oplus y_s$，数值位两数绝对值相乘，累加次数n次</p>
</li>
<li><p>补码一位乘法，符号位参与运算，累加次数n+1次</p>
</li>
<li><p>实现N位（不包括符号位）补码一位乘时，乘积为2N+1位（需要再加符号位）</p>
</li>
</ul>
<h2 id="真值、原码、补码、反码转换规律"><a href="#真值、原码、补码、反码转换规律" class="headerlink" title="真值、原码、补码、反码转换规律"></a>真值、原码、补码、反码转换规律</h2><ul>
<li><ol>
<li>真值-&gt;[x]原：数值位不变，符号位正数为0，负数为1</li>
</ol>
</li>
<li><ol start="2">
<li>[x]原-&gt;[x]补：符号位不变，正数数值位不变，负数数值位取反+1</li>
</ol>
</li>
<li><ol start="3">
<li>[x]原&lt;-&gt;[x]反：符号位不变，正数数值位不变，负数数值位取反</li>
</ol>
</li>
<li><ol start="4">
<li>[x]原&lt;-&gt;[-x]原：数值位不变，符号位取反</li>
</ol>
</li>
<li><ol start="5">
<li>[x]补&lt;-&gt;[-x]补：符号位取反，数值位取反+1</li>
</ol>
</li>
<li><ol start="6">
<li>[x]补&lt;-&gt;[x]移：符号位取反</li>
</ol>
</li>
<li><p>真值的移码与补码只差一个符号位</p>
</li>
<li><p>算术位移对象是有符号数，符号位不参与位移</p>
</li>
<li><p>补码不恢复余数法中，异号相除时，够减商0，不够减商1</p>
</li>
<li><p>判断加减法溢出时，如果采用判断进位的方式（符号位进位$c_0$，最高位进位$c_1$），则产生溢出的条件为$c_0\oplus c_1&#x3D;1$</p>
</li>
<li><p>无符号数的加减计算与有符号计算相同（化为二进制按位运算）：$a-b&#x3D;a+[-b]$</p>
</li>
</ul>
<h2 id="标志位OF、SF、CF"><a href="#标志位OF、SF、CF" class="headerlink" title="标志位OF、SF、CF"></a>标志位OF、SF、CF</h2><ul>
<li><p>溢出标志位OF（Overflow Flag）（有符号数加减运算）：1表示溢出（寄存器中值不是真正结果）</p>
</li>
<li><p>OF&#x3D;最高位产生的进位$\oplus$次高位产生的进位</p>
</li>
<li><p>符号标志位SF（有符号数加减运算）：0表示结果为正数，1表示结果为负数</p>
</li>
<li><p>进位标志位CF（Carry Flag）（无符号数加减运算）：1表示最高位有进位，0表示最高位无进位（最高位是否进位与是否溢出无关）</p>
</li>
<li><p>无符号数运算时借位标志$CF&#x3D;C\oplus sub$,(C为进位输出，减法运算：sub&#x3D;1，加法运算：sub&#x3D;0，加法运算时有进位则说明溢出)</p>
</li>
<li><p>乘法运算时，如果高33位不是全0或全1，OF&#x3D;1，表示溢出</p>
</li>
<li><p>当指令执行无符号数比较跳转时，通常对两数进行减法运算，结果小于等于0时跳转（小于0借位符号CF&#x3D;1，等于0零符号ZF&#x3D;1）</p>
</li>
</ul>
<h2 id="浮点数"><a href="#浮点数" class="headerlink" title="浮点数"></a>浮点数</h2><ul>
<li><p>采用规格化浮点数最主要是为了增加数据的表示精度</p>
</li>
<li><p>在IEEE 754标准下，尾数全部为隐藏最高位”1”的正数原码(临时浮点数无隐含位)</p>
</li>
<li><p>规格化浮点数就是让尾数$\frac{1}{2}\le|M|\le1$</p>
</li>
<li><p>（基数为2时）原码规格化数的尾数最高位一定是1；补码规格化数的尾数最高位一定与尾数符号相反（0.11 1、1.011 1（最大负数形式））</p>
</li>
<li><p>基数为$2^n$时，若浮点数为正数，数值位前n位不全为0；若浮点数为负数，数值位前n位不全为1</p>
</li>
</ul>
<blockquote>
<p>根据数值x求浮点数二进制值时。先将x转换为二进制表示，再对其规格化，得到规格化尾数（IEEE 754标准下只需要把尾数转换为1.xxx形式即可）与阶数。按照IEEE 754标准将数符、移码表示的阶数、规格化尾数填入相应字段</p>
</blockquote>
<table>
<thead>
<tr>
<th>类型</th>
<th>数符</th>
<th>阶码</th>
<th>尾数数值</th>
<th>总位数</th>
<th>偏置值</th>
</tr>
</thead>
<tbody><tr>
<td>短浮点数float</td>
<td>1</td>
<td>8</td>
<td>23</td>
<td>32</td>
<td>127（7FH）</td>
</tr>
<tr>
<td>长浮点数</td>
<td>1</td>
<td>11</td>
<td>52</td>
<td>64</td>
<td>1023（3FFH）</td>
</tr>
<tr>
<td>临时浮点数</td>
<td>1</td>
<td>15</td>
<td>64</td>
<td>80</td>
<td>16383(3FFFH)</td>
</tr>
</tbody></table>
<ul>
<li><p>float类型最小正数值：$1.0\times2^{1-127}(E&#x3D;1,M&#x3D;0)$（阶码不能取到全0）</p>
</li>
<li><p>float类型最大正数值:$1.111..\times2^{254-127}&#x3D;2^{127}(2-2^{-23})\ (E&#x3D;254,M&#x3D;.111…)$</p>
</li>
<li><p>double类型最小正数值：$1.0\times2^{1-1023}(E&#x3D;1,M&#x3D;0)$</p>
</li>
<li><p>double类型最大正数值：$1.111..\times2^{2046-1023}&#x3D;2^{1023}(2-2^{-52})\ (E&#x3D;2046,M&#x3D;.111…)$</p>
</li>
<li><p>阶码全为1时表示无穷大，全为0时表示非规格化数</p>
</li>
<li><p>右规和尾数舍入过程，阶码可能上溢</p>
</li>
<li><p>浮点数乘法任何情况下最多进行一次右归，可能进行多次左归</p>
</li>
<li><p>最简单的截断方法为直接截断</p>
</li>
</ul>
<hr>
<ul>
<li><p>加法器进位信号$g&#x3D;X_i Y_i $</p>
</li>
<li><p>加法器进位传递信号$p&#x3D;X_i\oplus Y_i$</p>
</li>
<li><p>相同字长下，定点数比浮点数精度更高（浮点数部分字段用作阶码表示）；浮点数表示范围更大</p>
</li>
<li><p>unsigned类型为无符号整数，直接用二进制位对数值进行编码而得。（一般为补码表示）</p>
</li>
<li><p>原码一位乘算法过程中，所有移位均是逻辑移位</p>
</li>
</ul>
<h2 id="补码加减运算器"><a href="#补码加减运算器" class="headerlink" title="补码加减运算器"></a>补码加减运算器</h2><p> <img src="/%5Cimages%5Cimage-20220315100153529.png" alt="image-20220315100153529"></p>
<p>加减运算器处理有符号数与无符号数的处理过程是相同的，但是判断是否溢出的方法不同</p>
<p>加法器的低位进位信息，减法运算时为1（被减数取反加1），加法运算时为0</p>
<h2 id="乘法电路和除法电路的基本结构"><a href="#乘法电路和除法电路的基本结构" class="headerlink" title="乘法电路和除法电路的基本结构"></a>乘法电路和除法电路的基本结构</h2><p>部分积和被乘数X做无符号数加法时，可能产生进位，因此需要一个专门的进位位C。乘积奇存器P初始时置0。计数器C，初值为 32，每循环一次减1。 ALU 是乘法核心部件，对乘积寄存器P和被乘数奇存器X的內容做“无符号加法” 运算，运算结果送回奇存器P，进位存放在C中。每次循环都对进位位C，乘积寄存器P和乘数奇存器Y 实现同步 “逻辑右移”，此时，进位位 C移入寄存器P的最高位，寄存器Y的最低位移出。每次寄存器Y的最低位都被送到控制逻辑，以决定被乘数是否“加”到部分积上。</p>
<p><img src="/%5Cimages%5Cimage-20220315100203719.png" alt="image-20220315100203719"></p>
<p>无符号乘法溢出判断：前n比特有不是0，则发生了溢出</p>
<p>有符号乘法溢出判断：高n+1位全1或全0说明没有溢出</p>
<p>需要部件：支持加减法ALU、移位功能的寄存器（无符号乘法逻辑右移、有符号算术乘法右移、除法左移）、控制逻辑（加减法控制、左右移控制、写使能）、计数器</p>
<h1 id="存储系统"><a href="#存储系统" class="headerlink" title="存储系统"></a>存储系统</h1><p> <img src="/%5Cimages%5Cimage-20220315101255419.png" alt="image-20220315101255419"></p>
<ul>
<li><p>主存使用RAM和ROM实现，控制存储器使用ROM实现，主存主要使用DRAM，Cache主要使用SRAM</p>
</li>
<li><p>主存和控存都是按地址访问</p>
</li>
<li><p>存放一个二进制位的物理器件称为存储元，地址码相同的多个存储元构成一个存储单元（字节编址即八个存储元使用一个存储单元），若干个存储单元集合构成存储体</p>
</li>
<li><p>编址方式属于计算机组成，与机器字长（计算机体系结构）没有关系</p>
</li>
</ul>
<hr>
<ul>
<li><p>存取周期$t_RC$：存储芯片进行连续两次读、写操作所必须间隔的时间（存取时间+恢复时间）</p>
</li>
<li><p>存取时间$T_a$：执行一次读操作或写操作的时间（分有读出时间、写入时间）</p>
</li>
</ul>
<ol>
<li><p>读出时间$t_A$：从主存接收到有效地址开始，到读出所选中单元的内容并在外部数据总线上稳定地出现所需的时间（还需要一个总线周期才能将数据传入CPU）</p>
</li>
<li><p>写入时间：从主存接收到有效地址开始到数据写入被写入单元为止</p>
</li>
</ol>
<hr>
<ul>
<li>采用DMA方式传递数据时，每传递一个数据就要占用一个存取周期</li>
</ul>
<h2 id="SRAM、DRAM"><a href="#SRAM、DRAM" class="headerlink" title="SRAM、DRAM"></a>SRAM、DRAM</h2><ul>
<li><p>主存储器由DRAM实现；Cache由SRAM实现</p>
</li>
<li><p>CDROM是只读型光盘存储器，不属于只读存储器ROM，使用串行存取方式</p>
</li>
</ul>
<table>
<thead>
<tr>
<th></th>
<th>DRAM</th>
<th>SRAM</th>
</tr>
</thead>
<tbody><tr>
<td>特点</td>
<td>需要刷新、集成度高</td>
<td>速度较快、成本较高、功耗较大</td>
</tr>
<tr>
<td>存储元</td>
<td>晶体管（电容）</td>
<td>双稳态触发器（六晶体管MOS）</td>
</tr>
<tr>
<td>送行列地址</td>
<td>分两次送（地址复用）</td>
<td>同时送</td>
</tr>
</tbody></table>
<blockquote>
<p>DRAM刷新</p>
<ol>
<li><p>刷新对CPU是透明的，不依赖外部访问</p>
</li>
<li><p>刷新单位是行</p>
</li>
<li><p>刷新操作类似于读操作（将信息通过刷新放大器放大后重新存回存储单元）</p>
</li>
<li><p>一次刷新只占用一个存取周期</p>
</li>
</ol>
</blockquote>
<hr>
<ul>
<li><p>低位交叉编址中低位地址表示该字节存储的芯片编号。若是连续占多位字节的数据字（如double）即从该芯片开始存储。</p>
</li>
<li><p>一个存储周期可以对所有芯片各读取一个字节</p>
</li>
<li><p>芯片引脚数目N&#x3D;数据线$N_1$+ 地址线$N_2$（DRAM采用地址复用技术，因此地址线是原来的1&#x2F;2）+片选线（1根）+读写控制线（2根）（读写控制线有时候也可以共用一根)</p>
</li>
<li><p>地址的编码是由低位向高位编码，先将片内地址由最低位填入，再填入片选地址（最高位可以空闲）</p>
</li>
<li><p>CPU与Cache间的数据交换以字为单位；Cache与主存间的交换以块为单位</p>
</li>
</ul>
<h2 id="固态硬盘SSD"><a href="#固态硬盘SSD" class="headerlink" title="固态硬盘SSD"></a>固态硬盘SSD</h2><ol>
<li>基于闪存Flash（芯片中的一块相当于磁盘中的一条磁道，磁道中还有若干页（最基本的读写单位））</li>
<li>可以随机存储、用and门实现</li>
<li>动态磨损：偏向使用较新的内存颗粒进行擦除写入（优化不够）</li>
<li>静态磨损：将长时间不用的数据写入较老的内存颗粒中，将新数据写入新的内存颗粒（磨损均衡、主控压力较大）</li>
</ol>
<p><img src="\images\image-20220315100219089.png" alt="哎呀，图片不见了" style="zoom:100%;" /><img src="/" alt="image-20220315100219089"></p>
<h2 id="三级Cache"><a href="#三级Cache" class="headerlink" title="三级Cache"></a>三级Cache</h2><ul>
<li><p>指令Cache与数据Cache分离位于L1级，通常使用写分配法与写回法</p>
</li>
<li><p>L1对L2使用全写法，L2对L3使用写回法（非全部情况）</p>
</li>
<li><p>Cache写命中策略：全写法、写回法</p>
</li>
<li><p>Cache未命中策略：写分配法、非写分配法</p>
</li>
<li><p>全写法（write-through）：将数据同时写入Cache与主存</p>
</li>
<li><p>写回法（write-back）：在块被换出时，将更新的块写回缓存</p>
</li>
<li><p>写分配法（write-allocate）：加载主存中的块到Cache</p>
</li>
<li><p>非写分配法（not-write-allocate）：只写入主存，不调块</p>
</li>
<li><p>每行Cache对应一个标记项，项中包括有效位、标记位tag（地址总长度-块内偏移地址-Cache组偏移地址）、一致性维护位（写回法时使用）、替换算法位（替换算法时使用）</p>
</li>
<li><p>Cache缺失由硬件完成；缺页由软件完成；TLB缺失既可以用硬件也可以用软件完成</p>
</li>
<li><p>LRU算法中替换算法位长度与Cache组大小有关，2路组相联使用1位LRU位，4路组相联使用2位LRU位</p>
</li>
<li><p>语句<code>a[k]=a[k]+32</code>，需要访存两次。第一次读取a[k]的值；第二次将新值写回a[k]</p>
</li>
<li><p>Cache的效率公式：Cache访问时间&#x2F;实际访存的平均访问时间</p>
</li>
</ul>
<h2 id="CPU使用虚拟内存过程"><a href="#CPU使用虚拟内存过程" class="headerlink" title="CPU使用虚拟内存过程"></a>CPU使用虚拟内存过程</h2><p>CPU使用的是虚拟地址，由辅助硬件（TLB、页表）找到虚地址与实地址间的对应关系，并判断该虚地址对应的存储单元是否已装入主存（有效位为1）。若已在主存中，通过地址变换，CPU可直接访问主存指示的存储单元（或直接访问Cache获得数据）；若不在主存，通过缺页中断，把包含该字的一页或是一段掉入主存后再由CPU访问。若主存已满，采用替换算法置换主存中的一页或一段</p>
<hr>
<ul>
<li><p>TLB缺失则需要访存一次访问页表；页表缺失需要访问磁盘；Cache缺失需要访存一次获取数据</p>
</li>
<li><p>段页式在地址变换时需要两次查表，开销较大</p>
</li>
<li><p>虚拟存储管理系统的基础是程序访问的局部性原理，该理论的基本含义是：在程序执行的过程中，程序对主存的访问是不均匀的</p>
</li>
<li><p>Flash半导体存储器的物理结构不需要考虑寻道时间和旋转延迟，可以直接按IO请求的先后顺序服务</p>
</li>
</ul>
<h1 id="指令系统"><a href="#指令系统" class="headerlink" title="指令系统"></a>指令系统</h1><ul>
<li><p>零地址指令：空操作指令、停机指令、关中断指令、弹栈压栈指令</p>
</li>
<li><p>要注意隐操作数的情况，一地址指令中有将目的地址隐藏的双操作数指令（先按指令地址码给出的地址读操作数，另一个操作数由ACC提供，运算结果也存放在ACC中）</p>
</li>
<li><p>执行一条三地址指令需要访问4次存储器（取指令1次、取两个操作数2次、存放结果1次）</p>
</li>
<li><p>程序控制类指令功能：改变程序的执行顺序（即转移地址的指令），使程序有测试、分析、判断、循环执行的能力。如：无条件转移指令、条件转移指令、循环指令（中断隐指令指令为硬件实现，不存在于指令系统中）</p>
</li>
<li><p>微操作信号发生器的设计与寄存器数量无关</p>
</li>
<li><p>CISC控制器大多数采用微程序控制</p>
</li>
<li><p>RISC采用指令流水线技术（有利于编译程序代码优化），以硬布线控制为主</p>
</li>
<li><p>RISC只有Load\Store指令访存，其余指令的操作都在寄存器间进行</p>
</li>
</ul>
<blockquote>
<p>微程序控制中，确定一条指令的地址</p>
<ol>
<li><p>增量计数法：由$\mu PC$确定下一条微指令地址</p>
</li>
<li><p>下地址法（断定法）：由微指令的下地址字段给出后续指令地址</p>
</li>
<li><p>硬件法：由专门硬件电路或外部直接向CMAR输入微指令地址</p>
</li>
</ol>
</blockquote>
<p> 基址寻址时，程序员操作偏移地址，基址寄存器内容由操作系统控制（如在中断向量表寻找中断向量，基地址为中断向量表起始地址）</p>
<ul>
<li>变址寻址时，程序员操作变址寄存器，偏移地址（数据结构首地址）不变</li>
<li>简化地址结构的基本方法是尽量使用隐含取值</li>
<li>如果一条指令由两字节组成，每取一字节PC值+1（每次PC自加的值由指令字长决定）</li>
</ul>
<p><img src="/%5Cimages%5Cimage-20220315101701157.png" alt="image-20220315101701157"></p>
<ul>
<li>对按字寻址的机器，程序计数器取决于存储器字数、指令寄存器取决于指令字长</li>
<li>内存地址都为无符号数，寻址时对补码操作数需要一定转换</li>
<li>执行<code>cmp a,b</code>指令时，通过<code>a-b</code>实现对a与b的比较</li>
</ul>
<h1 id="中央处理器"><a href="#中央处理器" class="headerlink" title="中央处理器"></a>中央处理器</h1><ul>
<li><p>控制器的全部功能：从主存中取出指令、分析指令、并产生有关的操作控制信号</p>
</li>
<li><p>程序状态字寄存器存放计算机系统表征程序和机器状态的部件</p>
</li>
<li><p>间址周期的作用是取操作数的有效地址，间址周期结束后，MDR中的内容为操作数地址</p>
</li>
<li><p>对于间接寻址的指令，先访存一次取出有效地址，再访存取出操作数（间址周期介于取址周期与执行周期之间）</p>
</li>
<li><p>MDR通常与存储字长有关，数据字长是一次存取数据的长度（可能是存储字长的n倍）</p>
</li>
</ul>
<h2 id="数据通路"><a href="#数据通路" class="headerlink" title="数据通路"></a>数据通路</h2><ul>
<li><p>指令执行过程中数据所经过的路径，包括路径上的部件，称为数据通路</p>
</li>
<li><p>ALU、通用寄存器、状态寄存器、Cache、MMU、浮点运算逻辑、异常和中断处理逻辑等，都属于数据通路的一部分</p>
</li>
</ul>
<h2 id="多核技术"><a href="#多核技术" class="headerlink" title="多核技术"></a>多核技术</h2><ul>
<li><p>多核处理器指单芯片处理器，在一个芯片中集成两个或多个完整且并行工作的处理器核心而构成的处理</p>
</li>
<li><p>核心指指令部件、算术&#x2F;逻辑部件、寄存器堆、一二级缓存处理单元</p>
</li>
<li><p>多核处理三大技术：维持Cache一致性、核间通信技术、对软件设计的挑战</p>
</li>
<li><p>多个CPU共享统一的地址空间，且独自拥有属于自己的L1Cache</p>
</li>
</ul>
<h2 id="机器周期、指令周期、时钟周期"><a href="#机器周期、指令周期、时钟周期" class="headerlink" title="机器周期、指令周期、时钟周期"></a>机器周期、指令周期、时钟周期</h2><ul>
<li>指令周期：CPU每取出并执行一条指令所需的全部时间</li>
<li>机器周期是指令执行中每步操作（取值、存储器读、存储器写等）所需要的时间</li>
<li>每个机器周期的节拍数可以不等（同一个操作（取值、写回等）的节拍数必须相同）</li>
<li>各指令功能不同，所以各指令执行所需的机器周期数也是可变的</li>
</ul>
<hr>
<ul>
<li><p>指令的执行周期结束后进入中断周期，用来响应中断</p>
</li>
<li><p>CPU根据指令周期的不同阶段判断从存储器中取出的二进制代码是指令还是数据</p>
</li>
<li><p>CPU硬件中使用专门的MUX（二路选择器）进行PC操作</p>
</li>
<li><p>通常以存取周期作为基准周期，即内存中读取一个指令字的最短时间作为机器周期（指令字长为2倍存储字长的取值周期为2个机器周期）</p>
</li>
<li><p>控制存储器CM使用ROM存放微程序，是CPU的一部分，因此控存不属于存储系统的一部分</p>
</li>
<li><p>在组合逻辑控制器中，微操作控制信号的形成主要与指令译码信号与时钟信号有关</p>
</li>
<li><p>硬布线控制器需要结合各微操作的节拍安排，综合分析，写出逻辑表达式，再设计成逻辑电路图。因此时序系统较为复杂</p>
</li>
<li><p>微指令在直接编码方式下，其操作控制字段位数等于微命令数；在字段直接编码方式下，每个字段要预留一个不进行任何操作的状态</p>
</li>
<li><p>一个微程序周期对应一个指令周期，一个微指令周期对应一个时钟周期（一条指令对应一个微程序，一个微程序由许多微指令构成，一条微指令会发出许多不同的微命令）</p>
</li>
<li><p>流水线是时间上并行（空间并行是资源重复）</p>
</li>
<li><p>超标量流水线能结合动态调度技术提高指令执行的并行性（拓展Tomasulo算法:支持双流出超标量流水线）</p>
</li>
<li><p>一个m段流水线稳定流出时CPU的吞吐能力，与m个并行部件的CPU吞吐能力相等</p>
</li>
</ul>
<h1 id="总线"><a href="#总线" class="headerlink" title="总线"></a>总线</h1><ul>
<li><p>猝发传送：一个总线周期内传输存储地址连续的n个数据字，$t&#x3D;t_1+n\times t_2$(t_1传输地址，t_2传输一个数据字)</p>
</li>
<li><p>一个总线的时钟周期可以传输一个存储字（长度为总线位宽）</p>
</li>
<li><p>总线宽度有称总线位宽，是总线上能够同时传输的数据位数，通常是指总线的数据总线的根数</p>
</li>
<li><p>数据总线宽度由总线的功能特性定义</p>
</li>
<li><p>系统总线中地址线传输CPU想访问的存储单元或IO端口地址</p>
</li>
<li><p>采用分离事务通信方式（总线复用）可以提高总线利用率</p>
</li>
<li><p>靠近CPU的总线速度快（连接高速设备）</p>
</li>
<li><p>PCI-Express×16采用并行传输方式</p>
</li>
<li><p>USB是连接外部设备的IO总线，用于设备与设备控制器（IO接口）间互连的接口标准</p>
</li>
<li><p>USB特性：即插即用、热插拔、采用菊花链形式将众多外设连接起来（拓展坞）、可扩展性强、高速传输、串行总线（不能传输多位数据）</p>
</li>
<li><p>系统总线：ISA、EISA</p>
</li>
<li><p>局部总线：VESA、PCI、AGP、PCI-Express（取代PCI、AGP）</p>
</li>
<li><p>PCI总线是一个与处理器无关的高速外围总线，其基本传输机制是猝发式传输</p>
</li>
</ul>
<h1 id="输入、输出系统"><a href="#输入、输出系统" class="headerlink" title="输入、输出系统"></a>输入、输出系统</h1><h2 id="IO接口"><a href="#IO接口" class="headerlink" title="IO接口"></a>IO接口</h2><ul>
<li><p>IO设备通过设备控制器与主板的系统总线相连接（控制IO设备的具体动作）</p>
</li>
<li><p>IO接口：外设与主机间传输数据时进行各种协调工作的逻辑部件（速度匹配、数据缓存、电平和格式转换、地址译码、传送控制信号和状态信息）</p>
</li>
<li><p>执行IO指令时，CPU通过地址总线选择所请求的IO端口、通过数据总线在通用寄存器与IO端口之间进行数据传送</p>
</li>
<li><p>IO接口的控制线与地址线都是从CPU到IO接口的单向传送。IO接口中的命令字、状态字、中断类型号都是通过IO总线的数据线由IO接口传送到CPU</p>
</li>
<li><p>若干端口加相应的控制逻辑组成接口</p>
</li>
<li><p>IO统一编址执行速度慢；IO独立编址控制复杂</p>
</li>
</ul>
<hr>
<ul>
<li><p>磁盘驱动器是由磁头、磁盘和读写电路等组成即磁盘本身</p>
</li>
<li><p>U盘本质上来说是一种只读存储器</p>
</li>
<li><p>RAID技术将多个独立的物理磁盘组成一个独立的逻辑磁盘，提升存储性能、系统的可靠性</p>
</li>
<li><p>RAID0把连续多个数据块交替存放在不同的物理磁盘扇区中，几个磁盘交叉并行读写，不仅扩大了存储容量，而且提高了磁盘数据存取速度</p>
</li>
<li><p>RAID1使两个磁盘同时进行读写，互为备份</p>
</li>
<li><p>磁盘非格式化容量：磁盘可利用磁化单元数</p>
</li>
<li><p>磁盘格式化容量：按照某种特定的记录格式所能存储信息的总量（格式化后要比非格式化小）</p>
</li>
<li><p>采用定长数据块记录格式，直接寻址的最小单元为扇区</p>
</li>
<li><p>一个磁盘的读写操作是串行的，不可能同一时刻即读又写，也不可能同一时刻读或写两组数据</p>
</li>
<li><p>VRAM（显示存储器）：将一帧图像信息存储其中，存储容量由图像分辨率、灰度级决定</p>
</li>
<li><p>在字符显示的VRAM中存放ASCII码用以显示字符</p>
</li>
<li><p>总线仲裁中，计数器定时查询需要一根总线请求BR，和$\lceil\log_2 n\rceil$根设备地址线</p>
</li>
<li><p>总线仲裁一般是IO设备争用总线的判优方式，而中断判优方式一般是IO设备争用CPU的判优方式</p>
</li>
<li><p>通道和中断是软硬件相结合实现方式</p>
</li>
</ul>
<h2 id="中断"><a href="#中断" class="headerlink" title="中断"></a>中断</h2><ul>
<li><p>中断处理方式：在IO设备输入每个数据的过程中，无须CPU干预。仅当传输完一个数据时，才需要CPU进行中断处理。</p>
</li>
<li><p>中断数据传送是在软件控制下完成的</p>
</li>
<li><p>指令执行结构出现异常而引起的中断（运算溢出等）为程序性中断</p>
</li>
<li><p>IO中断为外部中断</p>
</li>
<li><p>机器校验中断为终止类中断</p>
</li>
<li><p>对于故障类异常，其断电是发生故障时的指令地址，要重新计算PC值</p>
</li>
<li><p>中断的断点为下条指令地址，无需重新计算</p>
</li>
<li><p>中断响应由高到低的优先级宜用访管-程序性-重新启动</p>
</li>
<li><p>主存故障引起的中断是机器校验中断，属于内中断（只能由CPU自身完成）；外中断一般指主存和CPU外的中断，如外设引起的中断。此时需要CPU从总线获取中断源的标志信息，进行下一步处理</p>
</li>
<li><p>缺页、溢出等异常事件是特定指令执行过程中产生的，中断不和任何指令相关联，不阻止任何指令的完成（在指令周期的中断周期有效）</p>
</li>
<li><p>配有通道的计算机系统中，用户程序需要输入&#x2F;输出时，引起的中断时访管中断，系统由用户态转为核心态</p>
</li>
<li><p>访管指令即为Trap指令</p>
</li>
<li><p>中断优先级包含响应优先级与处理优先级，处理优先级由屏蔽字实现（0为该中断开放、1为该中断被屏蔽）</p>
</li>
<li><p>CPU收到多个中断请求时，根据响应优先级响应中断</p>
</li>
<li><p>响应优先级为CPU响应中断的优先级，当存在中断屏蔽字情况下。可能出现的是，处理机响应D1，D1再被D2打断</p>
</li>
<li><p>中断屏蔽是在打断中断时使用，谁先得到响应要看响应优先级</p>
</li>
<li><p>中断服务程序的最后指令通常是中断返回指令（RETI），该指令在中断恢复后，此时CPU所有寄存器已经恢复到中断前的状态。因此该指令不需要进行无条件转移，只需要通知CPU开始从PC中取值，进入取值周期即可。</p>
</li>
<li><p>在多重中断中，CPU只有在检测到中断请求信号后（中断优先级更低的中断请求信号检测不到）才会进入中断响应周期</p>
</li>
</ul>
<h2 id="DMA方式"><a href="#DMA方式" class="headerlink" title="DMA方式"></a>DMA方式</h2><ul>
<li><p>DMA传送过程中，DMAC将接管CPU的地址总线、数据总线、控制总线，CPU主存控制信号被禁止使用</p>
</li>
<li><p>DMA传输的是数据块</p>
</li>
<li><p>DMA请求，请求的是总线使用权。</p>
</li>
<li><p>DMA数据传输在控制器的控制下完成的</p>
</li>
<li><p>对DMA请求的响应可以发生在每个机器周期结束时，只要CPU不占总嫌就可被响应（即DMA响应发生在一个总线事务完成后）</p>
</li>
<li><p>DMA请求不会导致被中断指令重新执行</p>
</li>
</ul>
<p><img src="/%5Cimages%5Cimage-20220315102052316.png" alt="image-20220315102052316"></p>
<h2 id="通道"><a href="#通道" class="headerlink" title="通道"></a>通道</h2><ul>
<li><p>通道程序存放在主存中，由通道从主存中取出并执行（通道执行）</p>
</li>
<li><p>CPU通过执行IO指令负责开启关闭通道，以及处理来自通道的中断实现对通道的管理</p>
</li>
</ul>
<h2 id="IO接口-1"><a href="#IO接口-1" class="headerlink" title="IO接口"></a>IO接口</h2><ul>
<li><p>IO设备通过设备控制器与主板的系统总线相连接（控制IO设备的具体动作）</p>
</li>
<li><p>IO接口：外设与主机间传输数据时进行各种协调工作的逻辑部件（速度匹配、数据缓存、电平和格式转换、地址译码、传送控制信号和状态信息）</p>
</li>
<li><p>执行IO指令时，CPU通过地址总线选择所请求的IO端口、通过数据总线在通用寄存器与IO端口之间进行数据传送</p>
</li>
<li><p>IO接口的控制线与地址线都是从CPU到IO接口的单向传送。IO接口中的命令字、状态字、中断类型号都是通过IO总线的数据线由IO接口传送到CPU</p>
</li>
<li><p>若干端口加相应的控制逻辑组成接口</p>
</li>
<li><p>IO统一编址执行速度慢；IO独立编址控制复杂</p>
</li>
<li><p>磁盘驱动器是由磁头、磁盘和读写电路等组成即磁盘本身</p>
</li>
<li><p>U盘本质上来说是一种只读存储器</p>
</li>
<li><p>RAID技术将多个独立的物理磁盘组成一个独立的逻辑磁盘，提升存储性能、系统的可靠性</p>
</li>
<li><p>RAID0把连续多个数据块交替存放在不同的物理磁盘扇区中，几个磁盘交叉并行读写，不仅扩大了存储容量，而且提高了磁盘数据存取速度</p>
</li>
<li><p>RAID1使两个磁盘同时进行读写，互为备份</p>
</li>
<li><p>磁盘非格式化容量：磁盘可利用磁化单元数</p>
</li>
<li><p>磁盘格式化容量：按照某种特定的记录格式所能存储信息的总量（格式化后要比非格式化小）</p>
</li>
<li><p>采用定长数据块记录格式，直接寻址的最小单元为扇区</p>
</li>
<li><p>一个磁盘的读写操作是串行的，不可能同一时刻即读又写，也不可能同一时刻读或写两组数据</p>
</li>
<li><p>VRAM（显示存储器）：将一帧图像信息存储其中，存储容量由图像分辨率、灰度级决定</p>
</li>
<li><p>在字符显示的VRAM中存放ASCII码用以显示字符</p>
</li>
<li><p>总线仲裁中，计数器定时查询需要一根总线请求BR，和<img src="/%5Cimages%5Cclip_image076.png" alt="img">根设备地址线</p>
</li>
<li><p>总线仲裁一般是IO设备争用总线的判优方式，而中断判优方式一般是IO设备争用CPU的判优方式</p>
</li>
<li><p>通道和中断是软硬件相结合实现方式</p>
</li>
</ul>
<h2 id="中断-1"><a href="#中断-1" class="headerlink" title="中断"></a>中断</h2><ul>
<li><p>中断处理方式：在IO设备输入每个数据的过程中，无须CPU干预。仅当传输完一个数据时，才需要CPU进行中断处理。</p>
</li>
<li><p>中断数据传送是在软件控制下完成的</p>
</li>
<li><p>指令执行结构出现异常而引起的中断（运算溢出等）为程序性中断</p>
</li>
<li><p>IO中断为外部中断</p>
</li>
<li><p>机器校验中断为终止类中断</p>
</li>
<li><p>对于故障类异常，其断电是发生故障时的指令地址，要重新计算PC值</p>
</li>
<li><p>中断的断点为下条指令地址，无需重新计算</p>
</li>
<li><p>中断响应由高到低的优先级宜用访管-程序性-重新启动</p>
</li>
<li><p>主存故障引起的中断是机器校验中断，属于内中断（只能由CPU自身完成）；外中断一般指主存和CPU外的中断，如外设引起的中断。此时需要CPU从总线获取中断源的标志信息，进行下一步处理</p>
</li>
<li><p>缺页、溢出等异常事件是特定指令执行过程中产生的，中断不和任何指令相关联，不阻止任何指令的完成（在指令周期的中断周期有效）</p>
</li>
<li><p>配有通道的计算机系统中，用户程序需要输入&#x2F;输出时，引起的中断时访管中断，系统由用户态转为核心态</p>
</li>
<li><p>访管指令即为Trap指令</p>
</li>
<li><p>中断优先级包含响应优先级与处理优先级，处理优先级由屏蔽字实现（0为该中断开放、1为该中断被屏蔽）</p>
</li>
<li><p>CPU收到多个中断请求时，根据响应优先级响应中断</p>
</li>
<li><p>响应优先级为CPU响应中断的优先级，当存在中断屏蔽字情况下。可能出现的是，处理机响应D1，D1再被D2打断</p>
</li>
<li><p>中断屏蔽是在打断中断时使用，谁先得到响应要看响应优先级</p>
</li>
<li><p>中断服务程序的最后指令通常是中断返回指令（RETI），该指令在中断恢复后，此时CPU所有寄存器已经恢复到中断前的状态。因此该指令不需要进行无条件转移，只需要通知CPU开始从PC中取值，进入取值周期即可。</p>
</li>
<li><p>在多重中断中，CPU只有在检测到中断请求信号后（中断优先级更低的中断请求信号检测不到）才会进入中断响应周期</p>
</li>
</ul>
<h2 id="DMA方式-1"><a href="#DMA方式-1" class="headerlink" title="DMA方式"></a>DMA方式</h2><ul>
<li><p>DMA传送过程中，DMAC将接管CPU的地址总线、数据总线、控制总线，CPU主存控制信号被禁止使用</p>
</li>
<li><p>DMA传输的是数据块</p>
</li>
<li><p>DMA请求，请求的是总线使用权。</p>
</li>
<li><p>DMA数据传输在控制器的控制下完成的</p>
</li>
<li><p>对DMA请求的响应可以发生在每个机器周期结束时，只要CPU不占总嫌就可被响应（即DMA响应发生在一个总线事务完成后）</p>
</li>
<li><p>DMA请求不会导致被中断指令重新执行</p>
</li>
</ul>
<p><img src="/%5Cimages%5Cclip_image079.jpg" alt="图像"></p>
<h2 id="通道-1"><a href="#通道-1" class="headerlink" title="通道"></a>通道</h2><ul>
<li><p>通道程序存放在主存中，由通道从主存中取出并执行（通道执行）</p>
</li>
<li><p>CPU通过执行IO指令负责开启关闭通道，以及处理来自通道的中断实现对通道的管理</p>
</li>
</ul>
]]></content>
      <tags>
        <tag>408</tag>
      </tags>
  </entry>
  <entry>
    <title>计算机网络</title>
    <url>/2022/03/11/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<p><strong>计算机网络是利用<a href="https://baike.baidu.com/item/%E9%80%9A%E4%BF%A1%E7%BA%BF%E8%B7%AF">通信线路</a>将地理上分散的、具有独立功能的<a href="https://baike.baidu.com/item/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F">计算机系统</a>和通信设备按不同的形式连接起来，以功能完善的网络软件及协议实现资源共享和信息传递的系统</strong><br><a href="/download/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C.xmind">Xmind下载</a></p>
<span id="more"></span>

<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><ul>
<li><p>计算机网络资源：计算机硬件、软件、数据</p>
</li>
<li><p>局域网与广域网差异不仅在覆盖范围，还主要在于所使用协议不同（局域网广播技术、广域网点对点交换技术）</p>
</li>
<li><p>局域网与广域网的互联是通过路由器来实现的</p>
</li>
<li><p>广播式网络（通常为局域网通信方式，局域网工作在数据链路层）可以不要网络层，但需要服务访问点（链路层与物理层）</p>
</li>
<li><p>定义功能执行的方法不是对网络模型进行分层的目标</p>
</li>
<li><p>OSI模型精确定义了服务、协议、接口三个概念，TCP&#x2F;IP模型没有</p>
</li>
<li><p>ISO&#x2F;OSI模型中，网络层可同时提供无连接服务与面向链接服务（TCP&#x2F;IP中传输层提供）</p>
</li>
<li><p>ISO&#x2F;OSI模型中，传输层只提供面向连接的服务</p>
</li>
<li><p>OSI模型中，表示层的功能是表示出用户看得懂的数据格式。主要完成数据字符集的转换、数据格式化和文本压缩、数据加密解密等</p>
</li>
<li><p>TCP&#x2F;IP模型中，传输层处理关于可靠性、流量控制、错误校正等问题</p>
</li>
<li><p>TCP&#x2F;IP协议族：TCP,IP,ICMP,IGMP,ARP,RARP,UDP,DNS,FTP,HTTP</p>
</li>
<li><p>以太网的MAC数据帧的首部和尾部长度为18字节</p>
</li>
<li><p>以太网前导码8字节，第一个字段7字节（前同步码）；第二个字段1字节（帧开始定界符）</p>
</li>
<li><p>IPv4常用首部长度为20字节</p>
</li>
<li><p>TCP首部开销为20字节，UDP首部开销为8字节 </p>
<img src="\images\image-20220311210327223.png" alt="哎呀，图片不见了" style="zoom:100%;" /></li>
</ul>
<h1 id="物理层"><a href="#物理层" class="headerlink" title="物理层"></a>物理层</h1><ul>
<li><p>物理层规定了电气特性，因此能够识别所传送的比特流</p>
</li>
<li><p>在数值上，波特率等于比特率与每符号所含比特数的比值</p>
</li>
<li><p>信道不等同于通信电路，一条可双向通信的电路往往包含两个信道，一条是发送信道，一条是接受信道。多个通信用户公用通信电路时，每个用户在该通信电路上都有一条信道（时分复用）</p>
</li>
<li><p>波特率等于每秒可能发生的信号变化次数</p>
</li>
<li><p>报文交换不能用于语音数据交换</p>
</li>
<li><p>虚电路对网络中的故障敏感，因此不能在出错率高（设备故障率高）的传输系统中使用（可以使用数据报方式）</p>
</li>
<li><p>电路交换不支持差错控制</p>
</li>
<li><p>电缆采用屏蔽技术的好处是减少电磁干扰辐射</p>
</li>
<li><p>描述物理层接口引脚处于高电位的含义属于功能描述</p>
</li>
<li><p>利用中继器扩大网络传输距离就是将衰弱信号进行整形再生</p>
</li>
<li><p>曼彻斯特编码的编码效率为50%，4B&#x2F;5B编码的编码效率为80%</p>
<img src="\images\image-20220311210338642.png" alt="哎呀，图片不见了" style="zoom:100%;" />

</li>
<li><p>非归零编码（NRZ）：高电平为1，低电平为0，不用归零</p>
</li>
<li><p>反向非归零编码（NRZI）：信号翻转代表0，信号保持不变代表1</p>
</li>
<li><p>曼彻斯特编码（以太网使用）：一个码元分为两个相等间隔，每个码元位中间跳变，作为时钟信号；码元为1时，前一间隔高电平，后一间隔低电平；</p>
</li>
<li><p>差分曼彻斯特编码（局域网使用）：码元为1时，前半码元电平与上一码元的后半个码元电平相同</p>
</li>
<li><p>转发器作用是放大信号</p>
</li>
<li><p>奈奎斯特定理：$2W\log_2 V$（V为每个码元离散电平数目，W理想低通信道带宽）</p>
</li>
<li><p>香农定理：$W\log_2(1+S&#x2F;N)$（S&#x2F;N为信噪比）</p>
</li>
<li><p>两个公式中取较小的一个指固定带宽后取较小速率的值为实际最高理论速率</p>
</li>
<li><p>同步TDM方式复用要求复用线路的数据率相同，对于数据传输率低的复用线路采用脉冲填充方式</p>
</li>
<li><p>语音信号需要128个量化级，每次采样需要$\log_2 128&#x3D;7bits$来标识，语音频率4kHz，每秒需采样8000次。则一路话音需要的数据传输速率为$8000\times7&#x3D;56kb&#x2F;s$</p>
</li>
</ul>
<h1 id="数据链路层"><a href="#数据链路层" class="headerlink" title="数据链路层"></a>数据链路层</h1><ul>
<li>通过提高信噪比可以减弱随机差错对数据传输的影响</li>
</ul>
<blockquote>
<p>通信信道噪声可以分为：热噪声与冲击噪声</p>
<p>热噪声：信道固有噪声，随即差错，提高信噪比可以降低它对数据传输的影响</p>
<p>冲击噪声：外界电磁干扰，突发错差</p>
</blockquote>
<ul>
<li>海明码纠错d位需要码距为$2d+1$的编码方案；检错d位需要码距为$d+1$的编码方案</li>
<li>海明码能纠一位错应该满足$2^k\ge n+k+1$（k为校验码位数、n为数据码位数）</li>
<li>循环冗余码的帧检验序列与帧数据一同发出，在接受端对这m+r位数据与多项式做模2除法</li>
<li>带r个校验位的多项式编码可以检测到所有长度小于等于r的突发性错误</li>
<li>TDM所用传输介质的性质是：介质的位速率大于单个信号的位速率</li>
<li>DBN协议中，接收端在收到不合要求的数据帧需要全部抛弃，但是会重复返回最后一个确认帧（防止已发送的ACK丢失）</li>
<li>拥塞控制中的选择重传协议接受窗口大小必须满足:发送窗口大小$W_T$≥接受窗口大小$W_R$，且$W_T+W_R\le 2^m$（m为帧序号位数）；接受窗口的最大值为$W_{RMAX}&#x3D;2^{(n-1)}$</li>
<li>信道效率：指发送方在一个发送周期（发送方开始发送数据到收到第一个ACK）内，有效发送数据所需要的时间占整个发送周期的比率</li>
<li>信道吞吐率：信道利用率×发送方发送速率</li>
<li>以太网中，当数据传输速率提高时，帧的发送时间相应地缩短。为了能有效地检测冲突，可以减少电缆介质的长度或增加最短帧长</li>
</ul>
<table>
<thead>
<tr>
<th>滑动窗口协议</th>
<th>一比特滑动窗口</th>
<th>Go back n协议</th>
<th>选择重传协议</th>
</tr>
</thead>
<tbody><tr>
<td>发送窗口大小</td>
<td>1</td>
<td>n（小于序号范围）</td>
<td>n（小于序号范围的一半）</td>
</tr>
<tr>
<td>接受窗口大小</td>
<td>1</td>
<td>1</td>
<td></td>
</tr>
<tr>
<td>ACK</td>
<td>每次都需要返回ACK</td>
<td>累计ACK</td>
<td>NAK或ACK定时器</td>
</tr>
<tr>
<td>超时重传的帧</td>
<td>未收到ACK的帧</td>
<td>i号帧以后的所有帧</td>
<td>未收到ACK的帧</td>
</tr>
</tbody></table>
<h2 id="码分复用"><a href="#码分复用" class="headerlink" title="码分复用"></a>码分复用</h2><ul>
<li><p>A、B同时向C发送数据，则A、B的码序列内积为0</p>
</li>
<li><p>C读取A发送的数据就要将接收到的数据与A的码片序列进行内积（&gt;0表示1；&lt;0表示0）</p>
</li>
</ul>
<h2 id="CSMA-x2F-CA"><a href="#CSMA-x2F-CA" class="headerlink" title="CSMA&#x2F;CA"></a>CSMA&#x2F;CA</h2><ol>
<li><p>DIFS（分布式协调IFS）：最长的IFS，优先级最低，用于异步帧竞争访问的时延。即载波监听到信道空闲，等待DIFS后发送RTS预约信道</p>
</li>
<li><p>PIFS（点协调式IFS）：PCF操作使用</p>
</li>
<li><p>SIFS（短IFS）：最短的IFS，最高优先级，目的站等待SIFS时间返回ACK</p>
</li>
</ol>
<p>A.   CSMA&#x2F;CA协议的退避算法需要等待一个DIFS，而且要进入争用窗口，计算随机退避时间以便再次试图接入信道</p>
<p>B.   CSMA&#x2F;CA协议只有当检测到信道空闲且该数据帧是要发送的第一条数据帧才不使用退避算法，当（1）发送第一帧前检测到信道忙；（2）每次重传；（3）每次成功发送后再发送下一帧时都需要使用退避算法</p>
<p>C.  CSMA&#x2F;CD协议当发生冲突后会，收发端检测到冲突后会发送干扰信号，干扰信号需要$\tau$时间在网络内传播，而退避算法是在检测到冲突后就开始执行</p>
<p>D.  以太网中可以发送数据后，还需要再等待96比特时间（最小帧间隔时间）</p>
<h2 id="CSMA-x2F-CA算法"><a href="#CSMA-x2F-CA算法" class="headerlink" title="CSMA&#x2F;CA算法"></a>CSMA&#x2F;CA算法</h2><p>I.    如果最初有数据要发送，且信道空闲，在等待DIFS后，就发送整个数据帧</p>
<p>II.   当检测到信道忙，站点执行CSMA&#x2F;CA退避算法，选取一个随机回退值。一旦检测到信道忙，退避计时器就保持不变；只要信道空闲，退避计时器就进行倒计时</p>
<p>III.  当退避计时器减到0时，站点发送整个帧并等待确认</p>
<p>IV.  发送站收到确认，说明发送帧被目的站正确接收。如果发送第二帧，就从步骤2开始</p>
<p>V.   若发送站没有收到ACK，必须重传该帧</p>
<ul>
<li>MAC帧不需要帧结束符（有IFS保障帧之间有间隙），但是以太网MAC帧，在数据链路层，帧既要加首部，也要加尾部</li>
</ul>
<table>
<thead>
<tr>
<th>参数</th>
<th>10BASE5</th>
<th>10BASE2</th>
<th>10BASE-T</th>
<th>10BASE-FL</th>
</tr>
</thead>
<tbody><tr>
<td>传输媒体</td>
<td>基带同轴电缆（粗缆）</td>
<td>基带同轴电缆（细缆）</td>
<td>非屏蔽双绞线</td>
<td>光线对（850nm）</td>
</tr>
<tr>
<td>编码</td>
<td>曼彻斯特编码</td>
<td>曼彻斯特编码</td>
<td>曼彻斯特编码</td>
<td>曼彻斯特编码</td>
</tr>
<tr>
<td>拓扑结构</td>
<td>总线形</td>
<td>总线形</td>
<td>星形</td>
<td>点对点</td>
</tr>
<tr>
<td>最大段长</td>
<td>500m</td>
<td>185m</td>
<td>100m</td>
<td>2000m</td>
</tr>
<tr>
<td>最多结点数目</td>
<td>100个</td>
<td>30个</td>
<td>2个</td>
<td>2个</td>
</tr>
<tr>
<td>使用协议</td>
<td>CSMA&#x2F;CD</td>
<td>CSMA&#x2F;CD</td>
<td>半双工方式下CSMA&#x2F;CD</td>
<td>半双工方式下CSMA&#x2F;CD</td>
</tr>
</tbody></table>
<p>A.   允许在1Gb&#x2F;s下<a href="https://baike.baidu.com/item/%E5%85%A8%E5%8F%8C%E5%B7%A5/310007">全双工</a>和<a href="https://baike.baidu.com/item/%E5%8D%8A%E5%8F%8C%E5%B7%A5/309852">半双工</a>两种方式工作</p>
<p>B.   100Base-T的设备线路传输速率为100Mbps</p>
<ul>
<li><p>如果同一局域网的两台设备具有相同的静态MAC地址，则在网络上这两个设备无法正确通信</p>
</li>
<li><p>p坚持CSMA协议适用于时隙信道</p>
</li>
<li><p>p坚持CSMA协议如果监听到信道忙会持续监听（推迟到下一个时隙再监听），直到信道空闲，并以p概率发送数据，1-p概念推迟到下一个时隙</p>
</li>
<li><p>快速以太网仍然使用CSMA&#x2F;CD协议，采用保持最短帧长不变，而将最大电缆长度减少到100m的方法，使以太网的数据传输速率提高至100Mb&#x2F;s</p>
</li>
<li><p>吉比特以太网支持流量控制机制，其中IEEE 802.3z采用光纤通道；IEEE 802.3ab采用4对UTP5类线</p>
</li>
<li><p>IEEE802.11 在MAC层使用CSMA&#x2F;CA协议</p>
</li>
<li><p>令牌环网在源站进行差错控制</p>
</li>
<li><p>广域网使用存储转发式的传输方式</p>
</li>
</ul>
<table>
<thead>
<tr>
<th>PPP协议</th>
<th>HDLC协议</th>
</tr>
</thead>
<tbody><tr>
<td>面向字节（字节填充法）</td>
<td>面向比特</td>
</tr>
<tr>
<td>提供差错控制，不使用序号和确认机制</td>
<td>信息帧使用序号和确认机制</td>
</tr>
<tr>
<td>全双工通信、点对点、面向连接</td>
<td>全双工通信、点对点、面向连接</td>
</tr>
<tr>
<td>PPP帧比HDLC帧多一个2字节的协议字段（$0x0021$表示IP数据报）</td>
<td>信息帧传输数据及其捎带确认，监督帧流量控制和差错控制，无编号帧建立链路</td>
</tr>
</tbody></table>
<ul>
<li><p>局域网交换机实现的功能主要是在物理层与数据链路层</p>
</li>
<li><p>在本地通信量较大的局域网内主要使用交换机，以降低冲突域</p>
</li>
<li><p>交换机按MAC地址转发，可以实现多端口并行传输</p>
</li>
<li><p>以太网MAC协议提供的是无连接不可靠的协议（不对发送的数据帧进行编号，不要求对方发回ACK确认）</p>
</li>
<li><p>以太网使用曼彻斯特编码，总线形拓扑，</p>
</li>
<li><p>以太网交换机直通交换，在输入端口检测到一个数据帧时，检查帧首部，获取帧的目的地址，启动内部动态查找表转换为相应的输出端口，在输入端与输出端交叉处接通，把数据帧直通到相应的端口，实现交换功能。直通交换只检查帧的目的地址（6Ｂ），最短传输延迟为$6\times8bits&#x2F;Mbps$</p>
</li>
<li><p>交换机所连主机可以同时连通多个端口，使每对相互通信的主机像独占信道一样，进行无碰撞💥的数据传输</p>
</li>
<li><p>交换机所连主机能达到的带宽是每个端口能达到的带宽最大值，集线器所连接主机所能达到的带宽是端口带宽的1&#x2F;n（n为所连主机数）</p>
</li>
</ul>
<h1 id="网络层"><a href="#网络层" class="headerlink" title="网络层"></a>网络层</h1><ul>
<li><p>在路由器连接的异构网络中，物理层、数据链路层、网络层的协议可以不相同，而网络层以上的协议必须相同（特定路由器可以连接IPv4与IPv6网络）</p>
</li>
<li><p>路由器分有直接交付与间接交付；当发送站与目的站处于同一网段时，使用直接交付；反之使用间接交付</p>
</li>
<li><p>距离-向量路由协议中，慢收敛导致路由器接受了无效的路由信息最可能导致路由回路问题</p>
</li>
</ul>
<blockquote>
<p>路由环路的产生：</p>
<p>当A路由器一侧的X网络发生故障，则A路由器收到故障信息，并把X网络设置为不可达，等待更新周期来通知相邻的B路由器。但是，如果相邻的B路由器的更新周期先来了，则A路由器将从B路由器那学习了到达X网络的路由，就是错误路由，因为此时的X网络已经损坏，而A路由器却在自己的路由表内增加了一条经过B路由器到达X网络的路由。然后A路由器还会继续把该错误路由通告给B路由器，B路由器更新路由表，认为到达X网络须经过A路由，然后继续通知相邻的路由器，至此路由环路形成，A路由器认为到达X网络经过B路由器，而B则认为到达X网络进过A路由器。</p>
</blockquote>
<ul>
<li>IPv4数据报中首部长度单位4B，总长度单位1B，片偏移单位8B（IPv6的首部长度是8B的整数倍）</li>
</ul>
<blockquote>
<h2 id="TTL"><a href="#TTL" class="headerlink" title="TTL"></a>TTL</h2><p>TTL字段由IP数据包的发送者设置，在IP数据包从源到目的的整个转发路径上，每经过一个路由器，路由器都会修改这个TTL字段值，具体的做法是把该TTL的值减1，然后再将IP包转发出去。如果在IP包到达目的IP之前，TTL减少为0，路由器将会丢弃收到的TTL&#x3D;0的IP包并向IP包的发送者发送 ICMP time exceeded消息。</p>
</blockquote>
<ul>
<li><p>ICMP报文作为IP层数据报的数据，加上IP数据报的首部，组成IP数据报发送出去（主要供网络层设备使用），即ICMP报文封装在数据链路层帧中发送</p>
</li>
<li><p>NAT表项需要管理员添加，以便控制一个内网到外网的网络连接。如果发送分组的IP：端口对在NAT表项中找不到，服务器不会转发该分组</p>
</li>
<li><p>DCHP服务器使用主机的以太网地址（MAC地址）标识主机</p>
</li>
<li><p>位于不同子网的主机相互通信时，路由器在转发IP数据报时，重新封装硬件源MAC地址（本路由器）与目的MAC地址（下一条路由器）</p>
</li>
<li><p>处于不同网段的主机必须通过路由器才能进行通信</p>
</li>
<li><p>RIP中每个网络的子网掩码必须相同，RIP2支持CIDR</p>
</li>
<li><p>OSPF协议使用Hello分组来保持与其邻居的连接</p>
</li>
<li><p>BGP交换的可达性信息是到达某个网络所经过的路径</p>
</li>
<li><p>网络地址不同的说明网络不同，需要使用路由器进行连接</p>
</li>
<li><p>IP网关等同于IP路由器</p>
</li>
<li><p>在一个广域网内的所有节点事先知道该广域网能通过的最大分组，因此广域网没有必要进行分片。但当数据报进入某个网络后，MTU可能发生变化，此时可能需要分片。</p>
</li>
<li><p>如果一台主机有两个以太网卡，那么它可以同时连接两个不同的网络（网络号不能相同，否则会发生冲突），这样它需要两个IP地址</p>
</li>
<li><p>IPv6的通信量类字段区分不同的IPv6数据报的类别或优先级（0最低），0-7表示允许延迟，8-15表示高优先级</p>
</li>
<li><p>IPv6流标号字段，所有属于同一个流的数据报都具有同样的流标号</p>
</li>
<li><p>IP组播地址映射为MAC地址方法：将IP地址的后23位转换为十六进制（第24位取0）组成后24位MAC，前24位MAC地址使用MAC地址的组播 </p>
</li>
<li><p>在设计组播路由时，为了避免路由环路，构造组播转发树</p>
</li>
<li><p>一个D类地址标志一个组播组</p>
</li>
<li><p>在组播情况下，是适配器NIC而不是CPU决定是否接受一个帧</p>
</li>
<li><p>NIC接受每一个广播帧，然后交付操作系统，由CPU决定是否接收</p>
</li>
<li><p>OSPF协议规定Area0是主干区域，区域路由器是与不同区域相连的路由器</p>
</li>
<li><p>不同局域网的两台主机若想通信，只需将数据包发给路由器即可，由路由器处理其他操作。（不用主机发送ARP报文）</p>
</li>
</ul>
<h1 id="传输层"><a href="#传输层" class="headerlink" title="传输层"></a>传输层</h1><ul>
<li>TCP拥塞控制的慢启动算法中，发送端每收到一个确认段就将拥塞窗口加一；这样在经过一个RTT后拥塞窗口的大小会加倍（cwnd大小指数式增长）</li>
<li>TCP拥塞控制的拥塞避免算法中，每经过一轮RTT只增加一个cwnd（线性增加）</li>
<li>排序工作由传输层实现；重组工作由网络层实现</li>
<li>TCP socket 建立的过程</li>
</ul>
<p><img src="https://user-images.githubusercontent.com/13389461/68817970-5e11d000-06be-11ea-85ee-5d2496d14c63.png" alt="TCP socket 建立的过程"></p>
<ul>
<li>UDP socket 建立的过程</li>
</ul>
<p><img src="https://user-images.githubusercontent.com/13389461/72334798-e3365700-36f8-11ea-9bd8-1bfd59118c01.png" alt="UDP socket 建立的过程"></p>
<h2 id="端口"><a href="#端口" class="headerlink" title="端口"></a>端口</h2><p>网络数据进入主机后，首先到达网卡，然后进入内核，由网络协议栈处理（使用socket）</p>
<p>每个进程在内核中都有一个表，保存该进程申请并占用的所有soket描述符。</p>
<p>socket：源IP+port，目的IP+port，建立起进程与端口间的联系。内核根据传输层数据报中的端口找到相应的进程实现进程间的通信</p>
<p>SOcket本质上是一种资源，理论上能够通过sendmsg将socket描述符传递到其他进程。同时，父子进程间，线程间也会进行socket共享。</p>
<hr>
<ul>
<li><p>TCP，UDP分别拥有自己的端口号，它们互不干扰，可以共存于同一台主机</p>
</li>
<li><p>TCP协议规定HTTP服务器进程的端口号为80</p>
</li>
<li><p>TCP采用对报文段确认的确认机制</p>
</li>
<li><p>TCP将收到的报文段组成字节流交给上层</p>
</li>
<li><p>TCP将应用层交付的数据看作一串字节流</p>
</li>
<li><p>MSS是TCP报文的有效载荷长度</p>
</li>
<li><p>UDP不需要处理RTT</p>
</li>
<li><p>TCP、UDP报头都有目的端口号、源端口号、校验号</p>
</li>
<li><p>TCP第三次握手如果不携带数据则不消耗序号</p>
</li>
<li><p>TCP每次收到比期望序号大的失序报文就发送一个冗余ACK</p>
</li>
<li><p>传输层分用：接收方发传输层剥去报文首部后，能把这些数据正确交付到目的进程（目的端口号是在终点交付报文时使用，源端口号是需要对方回信时使用）</p>
</li>
</ul>
<h2 id="三次握手"><a href="#三次握手" class="headerlink" title="三次握手"></a>三次握手</h2><img src="\images\image-20220315104618890.png" alt="哎呀，图片不见了" style="zoom:100%;" />

<p><strong>两次握手的问题</strong></p>
<p>第三次握手主要防止已失效的连接请求报文段突然传输到服务端。</p>
<p>当连接请求(CR)被网络延迟并重传, 导致Host2将重复的CR当做新的CR进行处理。此时服务器会为该链接分配资源，但是Host1并不知道这次连接。由此导致资源浪费</p>
<h2 id="四次握手"><a href="#四次握手" class="headerlink" title="四次握手"></a>四次握手</h2><img src="\images\image-20220315110536500.png" alt="哎呀，图片不见了" style="zoom:100%;" />

<p>四次握手是半关闭的连接释放，都是双方主动释放连接。server端收到FIN消息后，返回FIN ACK，并结束数据传输后再进行连接的释放。</p>
<p>client端的TIME-WAIT阶段需要等待2MSL时间（MSL报文最大存活时间）</p>
<ol>
<li>保证client发送的最后一个ACK报文能够达到server。如果最后一个ACK丢失，server端会在第一个MSL时间内进行超时重传，client就能在2MSL时间内收到重传的连接释放报文，并重新启动TIME-WAIT状态，确保server能够正确收到ACK报文，进入CLOSED状态</li>
<li>防止已失效的连接请求报文段出现在本次链接中。client发送最后一个ACK报文段后，如果正确达到server。再经过2MSL，本次连接产生的所有报文段（因为网络拥塞等问题在网络中留存的报文）都会从网络中消失。</li>
</ol>
<h2 id="TCP-keepalive"><a href="#TCP-keepalive" class="headerlink" title="TCP keepalive"></a>TCP keepalive</h2><p>定义一个时间段，在这个时间段内，如果没有任何连接相关的活动，TCP 保活机制会开始作用，每隔一个时间间隔，发送一个探测报文，该探测报文包含的数据非常少，如果连续几个探测报文都没有得到响应，则认为当前的 TCP 连接已经死亡，系统内核将错误信息通知给上层应用程序。在 Linux 系统中，2 小时内如果没有任何连接相关的活动，则会启动保活机制。而每次的检测间隔为75秒，检测 9 次无响应，认为对方是不可达的，从而中断本次的连接。也就是说最少需要经过 2 小时 11 分 15 秒才可以发现一个「死亡」连接。一般的，web 服务软件一般都会提供 <code>keepalive_timeout</code> 参数，用来指定 HTTP 长连接的超时时间</p>
<h1 id="应用层"><a href="#应用层" class="headerlink" title="应用层"></a>应用层</h1><table>
<thead>
<tr>
<th>DNS</th>
<th>FTP</th>
<th>电子邮件📧</th>
<th>WWW</th>
</tr>
</thead>
<tbody><tr>
<td>UDP（53端口）</td>
<td>TCP（控制连接21端口、数据连接20端口）</td>
<td>TCP（SMTP  25端口、POP3 110端口、IMAP）</td>
<td>TCP（HTTP  80端口）</td>
</tr>
<tr>
<td>递归查询、迭代查询</td>
<td>主动模式、被动模式</td>
<td></td>
<td>持久化连接、流水线</td>
</tr>
<tr>
<td>根域名服务器、顶级域名服务器、授权域名服务器、本地域名服务器</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<ul>
<li>客户机是面向用户的，服务器是面向任务的</li>
<li>多个不同的域名可以共用一个IP地址，访问一台服务器</li>
<li>CNAME记录用于将一个<a href="https://zh.wikipedia.org/wiki/%E5%9F%9F%E5%90%8D">域名</a>（同名）映射到另一个域名（真实名称），<a href="https://zh.wikipedia.org/wiki/%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90%E6%9C%8D%E5%8A%A1%E5%99%A8">域名解析服务器</a>遇到CNAME记录会以映射到的目标重新开始查询</li>
</ul>
<h2 id="VPN"><a href="#VPN" class="headerlink" title="VPN"></a>VPN</h2><p>通过VPN访问内网时，实际是在访问VPN服务器，再由VPN服务器访问内网。同时通过数据加密技术封装一条虚拟数据通信隧道，保障了网络通讯是安全私密的。也就是正向代理</p>
<h2 id="SSH"><a href="#SSH" class="headerlink" title="SSH"></a>SSH</h2><h3 id="非对称加密"><a href="#非对称加密" class="headerlink" title="非对称加密"></a>非对称加密</h3><p>client使用server提供的公钥对信息进行加密，发送给server。server使用自己独有的私钥对加密信息进行解密，获取信息。</p>
<h3 id="中间人攻击"><a href="#中间人攻击" class="headerlink" title="中间人攻击"></a>中间人攻击</h3><p>攻击者中途拦截client请求公钥的信息，并发送自己的公钥。client使用攻击者的公钥加密，攻击者使用自己的私钥解密，即可得到client的信息</p>
<h3 id="基于口令的认证"><a href="#基于口令的认证" class="headerlink" title="基于口令的认证"></a>基于口令的认证</h3><p>为了防止中间人攻击，需要对获得的公钥进行认证。在https中可以通过CA来进行公证。而在SSH中公钥私钥都是自己生成的，只能通过client端自己对公钥进行确认。host被确认后，被追加到known_host中。此时再进行登录操作</p>
<h3 id="基于公钥的认证"><a href="#基于公钥的认证" class="headerlink" title="基于公钥的认证"></a>基于公钥的认证</h3><p>Client端的public key需要Client手动Copy到Server端。Server端在authorized_keys中匹配到Client的公钥后，会生成随机数R，并用<strong>Client的公钥对该随机数进行加密</strong>，然后将加密后信息发送给Client，Client端通过私钥进行解密得到随机数R，然后对随机数R和本次会话的SessionKey利用MD5生成摘要Digest1，发送给Server端。Server端会也会对R和SessionKey利用同样摘要算法生成Digest2，最后比较Digest1和Digest2是否相同，完成认证过程。</p>
<h2 id="FTP"><a href="#FTP" class="headerlink" title="FTP"></a>FTP</h2><ul>
<li>匿名FTP访问通常使用anonymous作为用户名，该用户ID的密码可以是任何字符串</li>
</ul>
<blockquote>
<p>FTP服务器运行一个主进程，负责接收新的请求；以及若干个从属进程，负责处理单个请求。</p>
<p>服务器主进程接收新的请求后（第一次登陆请求，第二次读取文件请求），服务器使用20号端口创建数据传输进程和TCP数据连接到客户端发送的接收文件的随机端口（实际的文件传输连接）。</p>
<p>数据传输进程实际完成对文件的传输。传输完毕后关闭“数据传送连接”，并结束运行</p>
<p>FTP客户端发送的控制连接发送给服务器端控制进程（建立TCP链接到服务器的21端口，发送登录账号与密码），并在整个会话期间一直保持打开。</p>
<p>收到服务器返回的登陆成功信息后，客户端打开一个随机端口，并将该端口发送给服务器，从这个端口接收文件。</p>
</blockquote>
<h2 id="电子邮件"><a href="#电子邮件" class="headerlink" title="电子邮件"></a>电子邮件</h2><ul>
<li>POP3协议在传输层使用明文传输密码，并不对密码进行加密🔐</li>
<li>SMTP协议用于用户代理向邮件服务器发送邮件、邮件服务器间发送邮件（不支持邮件服务器向用户代理发送邮件）</li>
<li>POP3协议用于用户代理从邮件服务器读取邮件</li>
<li>SMTP协议只支持7比特的ASCII码内容</li>
<li>MIME邮件支持非ASCII码的编码规则（图片、视频等）</li>
</ul>
<h2 id="WWW"><a href="#WWW" class="headerlink" title="WWW"></a>WWW</h2><ul>
<li>请求一个万维网文档所需的时间&#x3D;该文档传输时间+两倍的RTT（一次RTT建立TCP连接，另一个RTT用于请求文档和接收文档，即主机的第三次握手携带请求信息）</li>
<li>浏览器只会在先前访问过的网站上留下Cookie</li>
<li>Cookie是服务器产生的</li>
<li>使用浏览器访问WWW服务器的第一步是DNS解析，获取IP地址</li>
</ul>
<p>客户端的 www浏览器获得 www服务器的主页并显示在客户端的屏幕上的过程如下(假设访问天勤论坛，域名为 <a href="http://www.csbji.com)：">www.csbji.com)：</a></p>
<ol>
<li>WWW 浏览器直接使用名称 <a href="http://www.csbiji.com/">www.csbiji.com</a> 访问该www 服务器，首先需要完成对该服务器的域名解析，并最终获得天勤论坛服务器对应的IP 地址 116.255.187.175。</li>
<li>www 浏览器将通过TCP 协议与服务器建立一条TCP 连接。当 TCP 连接建立之后，wwW 浏览器就向 www 服务器发送要求获取其主页的 HTTP请求。</li>
<li>www 服务器在接收到浏览器的 HTTP 请求之后，将构建所请求的 web 页面必需的各种信息，并将信息通过 Internet 传送给客户端的浏览器。</li>
<li>浏览器将收到的信息进行解释，然后将web页面显示在用户的屏幕上。</li>
</ol>
<h3 id="HTTP-报文格式"><a href="#HTTP-报文格式" class="headerlink" title="HTTP 报文格式"></a>HTTP 报文格式</h3><p>HTTP 请求由<strong>请求行、请求头部、空行和请求体</strong>四个部分组成。</p>
<ul>
<li><strong>请求行</strong>：包括请求方法，访问的资源 URL，使用的 HTTP 版本。<code>GET</code>和<code>POST</code>是最常见的 HTTP 方法，除此以外还包括<code>DELETE、HEAD、OPTIONS、PUT、TRACE</code>。</li>
<li><strong>请求头</strong>：格式为“属性名:属性值”，服务端根据请求头获取客户端的信息，主要有<code>cookie、host、connection、accept-language、accept-encoding、user-agent</code>。</li>
<li><strong>请求体</strong>：用户的请求数据如用户名、密码等。</li>
</ul>
<p>HTTP 响应也由四个部分组成，分别是：<strong>状态行、响应头、空行和响应体</strong>。</p>
<ul>
<li><strong>状态行</strong>：协议版本，状态码及状态描述。</li>
<li><strong>响应头</strong>：响应头字段主要有<code>connection、content-type、content-encoding、content-length、set-cookie、Last-Modified、Cache-Control、Expires</code>。</li>
<li><strong>响应体</strong>：服务器返回给客户端的内容。</li>
</ul>
<h3 id="HTTP1-0-和-HTTP1-1-的区别"><a href="#HTTP1-0-和-HTTP1-1-的区别" class="headerlink" title="HTTP1.0 和 HTTP1.1 的区别"></a>HTTP1.0 和 HTTP1.1 的区别</h3><ul>
<li><strong>长连接</strong>：HTTP1.0 默认使用短连接，每次请求都需要建立新的 TCP 连接，连接不能复用。<strong>HTTP1.1 支持长连接，复用 TCP 连接，允许客户端通过同一连接发送多个请求</strong>。不过，这个优化策略也存在问题，当一个队头的请求不能收到响应的资源时，它将会阻塞后面的请求。这就是“<strong>队头阻塞</strong>”问题。</li>
<li><strong>断点续传</strong>：HTTP1.0 <strong>不支持断点续传</strong>。HTTP1.1 新增了 <strong>range</strong> 字段，用来指定数据字节位置，<strong>支持断点续传</strong>。</li>
<li><strong>错误状态响应码</strong>：在 HTTP1.1 中新增了 24 个错误状态响应码，如<code>409（Conflict）</code>表示请求的资源与资源的当前状态发生冲突、<code>410（Gone）</code>表示服务器上的某个资源被永久性的地删除。</li>
<li><strong>Host 头处理</strong>：在 HTTP1.0 中认为每台服务器都绑定一个唯一的 IP 地址，因此，请求消息中的 URL 并没有传递主机名。到了 HTTP1.1 时代，虚拟主机技术发展迅速，在一台物理服务器上可以存在多个虚拟主机，并且它们共享一个 IP 地址，故 HTTP1.1 增加了 HOST 信息。</li>
</ul>
<h3 id="HTTP1-1-和-HTTP2-0-的区别"><a href="#HTTP1-1-和-HTTP2-0-的区别" class="headerlink" title="HTTP1.1 和 HTTP2.0 的区别"></a>HTTP1.1 和 HTTP2.0 的区别</h3><p>HTTP2.0 相比 HTTP1.1 支持的特性：</p>
<ul>
<li><strong>新的二进制格式</strong>：HTTP1.1 基于文本格式传输数据；HTTP2.0 采用二进制格式传输数据，解析更高效。</li>
<li><strong>多路复用</strong>：在一个连接里，允许同时发送多个请求或响应，<strong>并且这些请求或响应能够并行地传输而不被阻塞</strong>，避免 HTTP1.1 出现的“队头堵塞”问题。</li>
<li><strong>头部压缩</strong>，HTTP1.1 的 header 带有大量信息，而且每次都要重复发送；HTTP2.0 把 header 从数据中分离，并封装成头帧和数据帧，<strong>使用特定算法压缩头帧</strong>，有效减少头信息大小。并且 HTTP2.0 **在客户端和服务器端记录了之前发送的键值对，对于相同的数据，不会重复发送。**比如请求 a 发送了所有的头信息字段，请求 b 则**只需要发送差异数据**，这样可以减少冗余数据，降低开销。</li>
<li><strong>服务端推送</strong>：HTTP2.0 允许服务器向客户端推送资源，无需客户端发送请求到服务器获取。</li>
</ul>
<h3 id="HTTPS-与-HTTP-的区别"><a href="#HTTPS-与-HTTP-的区别" class="headerlink" title="HTTPS 与 HTTP 的区别"></a>HTTPS 与 HTTP 的区别</h3><ol>
<li>HTTP 是超文本传输协议，信息是<strong>明文传输</strong>；HTTPS 则是具有<strong>安全性</strong>的 ssl 加密传输协议。</li>
<li>HTTP 和 HTTPS 用的端口不一样，HTTP 端口是 80，HTTPS 是 443。</li>
<li>HTTPS 协议<strong>需要到 CA 机构申请证书</strong>，一般需要一定的费用。</li>
<li>HTTP 运行在 TCP 协议之上；HTTPS 运行在 SSL 协议之上，SSL 运行在 TCP 协议之上。</li>
</ol>
<h3 id="GET和POST的区别"><a href="#GET和POST的区别" class="headerlink" title="GET和POST的区别"></a>GET和POST的区别</h3><p>概括：</p>
<ul>
<li>对于GET方式的请求，浏览器会把http header和data一并发送出去，服务器响应200（返回数据）；</li>
<li>而对于POST，浏览器先发送header，服务器响应100 continue，浏览器再发送data，服务器响应200 ok（返回数据）<br>区别：</li>
<li>get参数通过url传递，post放在request body中。</li>
<li>get请求在url中传递的参数是有长度限制的，而post没有。</li>
<li>get比post更不安全，因为参数直接暴露在url中，所以不能用来传递敏感信息。</li>
<li>get请求只能进行url编码，而post支持多种编码方式。</li>
<li>get请求会浏览器主动cache，而post支持多种编码方式。</li>
<li>get请求参数会被完整保留在浏览历史记录里，而post中的参数不会被保留。</li>
<li>GET和POST本质上就是TCP链接，并无差别。但是由于HTTP的规定和浏览器&#x2F;服务器的限制，导致他们在应用过程中体现出一些不同。</li>
<li>GET产生一个TCP数据包；POST产生两个TCP数据包。</li>
</ul>
]]></content>
      <tags>
        <tag>408</tag>
      </tags>
  </entry>
  <entry>
    <title>离散数学</title>
    <url>/2022/03/22/%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6/</url>
    <content><![CDATA[<p><a href="/download/%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6.xmind">Xmind下载</a></p>
<span id="more"></span>
<h2 id="逻辑"><a href="#逻辑" class="headerlink" title="逻辑"></a>逻辑</h2><h3 id="命题"><a href="#命题" class="headerlink" title="命题"></a>命题</h3><ul>
<li><p>或者为真或者为假而不是两者同时成立的一条陈述句</p>
</li>
<li><p>量词</p>
<ul>
<li><p>全称量词$\forall x \quad P(x)$</p>
</li>
<li><p>存在量词$\exists x \quad P(x)$</p>
</li>
</ul>
</li>
</ul>
<img src="\images\image-20220310093258263.png" alt="哎呀，图片不见了" style="zoom:100%;" />
    > 需要掌握对嵌套量词的翻译，诸如$\forall x(C(x)\vee\exists y(C(y)\wedge F(x,y)))$：对于任意的x都有C(x),或者存在一个y，满足C(y),并且满足F(x,y)

<ul>
<li><p>逻辑运算符</p>
<ul>
<li>条件语句$p\rightarrow q$：p为真而q为假时命题为假，否则为真</li>
<li>合取$p\wedge q$（p并且q）两个命题都为真，命题为真</li>
<li>析取$p\vee q$（p或q）只要两个命题至少一个为真，命题即为真</li>
<li>异或$p\oplus q$:两个命题恰好一个为真，命题为真</li>
<li>运算优先级<img src="\images\image-20220310100101783.png" alt="哎呀，图片不见了" style="zoom:100%;" />（量词比所有的逻辑运算符都具有更高的优先级)</li>
</ul>
</li>
<li><p>命题运算</p>
</li>
</ul>
<img src="\images\image-20220310093244956.png" alt="哎呀，图片不见了" style="zoom:100%;" />


  

<ul>
<li><p>逻辑等价：所有可能情况下都有相同真值的两个复合命题是逻辑等价的</p>
</li>
<li><p>德摩根律：<img src="\images\image-20220310100536193.png" alt="哎呀，图片不见了" style="zoom:100%;" /></p>
</li>
<li><p>可满足性：如果存在一个对其变元的真赋值使其为真，则为可满足的</p>
</li>
<li><p>重言式</p>
<ul>
<li>命题变量所有可能值都为真的命题<img src="\images\image-20220310100152284.png" alt="哎呀，图片不见了" style="zoom:100%;" /></li>
</ul>
</li>
<li><p>谬论</p>
<ul>
<li>命题变量所有可能值都为假的命题</li>
</ul>
</li>
</ul>
<h3 id="证明方法"><a href="#证明方法" class="headerlink" title="证明方法"></a>证明方法</h3><ul>
<li><p>逻辑表示：$(p_1\wedge p_2\dots \wedge p_n)\Rightarrow q$</p>
<ul>
<li>q从逻辑上由p得到</li>
<li>通过证明规则推理是一个重言式证明</li>
<li>假言推理$(p\wedge(p\rightarrow q)\rightarrow q)$</li>
<li>间接方法：逆否命题与其等价$(p\rightarrow q)\leftrightarrow((\neg p)\rightarrow (\neg q))$</li>
<li>反证法$((p\rightarrow q)\wedge(\neg q))\rightarrow (\neg p)$</li>
<li>数学归纳法<img src="\images\image-20220310100216323.png" alt="哎呀，图片不见了" style="zoom:100%;" /></li>
<li>强归纳法$(p_{n_0}\wedge p_{n_0+1}\dots \wedge p_k)\Rightarrow p_{K+1}$</li>
<li><img src="\images\image-20220310102629033.png" alt="哎呀，图片不见了" style="zoom:100%;" /></li>
<li><img src="\images\image-20220310102824648.png" alt="哎呀，图片不见了" style="zoom:100%;" /></li>
<li>构造性证明：通过找到一个使得P(a)为真的元素a来给出$\exists xP(x)$的存在性证明</li>
<li>唯一性证明：分为两部分，存在性（存在某个元素具有期望的性质）与唯一性（如果$x\not{&#x3D;}y$,则y不具有该性质）</li>
</ul>
</li>
</ul>
<h2 id="基本结构"><a href="#基本结构" class="headerlink" title="基本结构"></a>基本结构</h2><h3 id="集合"><a href="#集合" class="headerlink" title="集合"></a>集合</h3><p>集合是对象的一个无序的聚集</p>
<p>当一个集合是有限的或者与正整数集具有一样的大小，则说这个集合是可数的</p>
<ul>
<li><p>空集$\emptyset$:不含有任何元素的集合</p>
<blockquote>
<p>除空集外，任何集合至少有$\emptyset$这一个元素，即为单元素集。所以任何非空集合都至少有两个子集，空集和集合S本身</p>
</blockquote>
</li>
<li><p>文氏图</p>
</li>
</ul>
<h4 id="集合的大小"><a href="#集合的大小" class="headerlink" title="集合的大小"></a>集合的大小</h4><p>若S有n个不同的元素，则称S为有限集，n为S的基数，记为|S|（$|\emptyset|&#x3D;0$）</p>
<p>集合A和集合B相同的基数，当且仅当存在从A到B的一个一一对应，写成|A|&#x3D;|B|（衡量两个集合大小的方法）</p>
<h4 id="可数集"><a href="#可数集" class="headerlink" title="可数集"></a>可数集</h4><p>一个集合或者是有限集或者与自然数集具有相同的基数，这个集合就称为可数的.如果一个无限集S是可数的，我们使用$\aleph_0$来表示S的基数，写成$|S|&#x3D;\aleph_0$</p>
<p>正奇数集、有理数集（将每个有理数表示为两个正整数之比）都是可数集合；实数集是不可数集</p>
<h5 id="SCHRODER-BERNSTEIN定理"><a href="#SCHRODER-BERNSTEIN定理" class="headerlink" title="SCHRÖDER-BERNSTEIN定理"></a>SCHRÖDER-BERNSTEIN定理</h5><p>如果|A|和|B|是集合，且|A|≤|B|和|B|≤|A|，则|A|&#x3D;|B|换言之，如果存在一对一函数f从A到B和g从B到A，则存在 A和B之间的一一对应画数。</p>
<h4 id="集合运算"><a href="#集合运算" class="headerlink" title="集合运算"></a>集合运算</h4><img src="\images\image-20220310114936537.png" alt="哎呀，图片不见了" style="zoom:100%;" />
<img src="\images\image-20220310114948919.png" alt="哎呀，图片不见了" style="zoom:100%;" />
> 证明集合相等一种方法：证明每一个是另一个的子集

<h3 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h3><p>令A和B是非空集合，从A到B的函数f是对元素的一种指派，对A的每个元素恰好指派B的一个元素，写成：$f:A\rightarrow B$。A为f的定义域，B为f的陪域</p>
<blockquote>
<p>如果两个函数的定义域、陪域、映射关系都相同，这两个函数是相等的</p>
</blockquote>
<ul>
<li>单射函数：函数是一对一的，对于定义域上的a，b有$f(a)&#x3D;f(b)\rightarrow a&#x3D;b$</li>
<li>满射函数（映上）：对每个$b\in B$有元素$a\in A$使得$f(a)&#x3D;b$</li>
<li>双射函数：函数即时一对一的，又是映上的</li>
<li>反函数$f^{-1}$：有$f(a)&#x3D;b$时$f^{-1}(b)&#x3D;a$(如果函数不是一一对应的，就无法定义反函数)</li>
<li>合成函数：有$(f\circ g)(a)&#x3D;f(g(a))$(交换律不成立)</li>
<li>部分函数：A的一个子集为f的定义域，对于A中但不在f定义域的元素没有定义</li>
</ul>
<h3 id="序列"><a href="#序列" class="headerlink" title="序列"></a>序列</h3><p>一种用来表示有序列表的离散结构</p>
<h4 id="递推关系"><a href="#递推关系" class="headerlink" title="递推关系"></a>递推关系</h4><ul>
<li>递归关系</li>
<li>初始条件</li>
</ul>
<h4 id="求和"><a href="#求和" class="headerlink" title="求和"></a>求和</h4><h2 id="代数结构"><a href="#代数结构" class="headerlink" title="代数结构"></a>代数结构</h2><p>由对象集合及运算的数学结构，称为代数系统</p>
<ul>
<li><p>特殊元（都是唯一的）</p>
<ul>
<li>幺元$e<em>x&#x3D;x</em>e&#x3D;x$</li>
<li>零元$\theta <em>x&#x3D;x</em>\theta&#x3D;\theta$</li>
<li>逆元$x*x^{-1}&#x3D;e$</li>
</ul>
</li>
</ul>
<h3 id="同态"><a href="#同态" class="headerlink" title="同态"></a>同态</h3><ul>
<li>象的积等于积的象$f(a*b)&#x3D;f(a)\oplus f(b)$</li>
</ul>
<h3 id="同构"><a href="#同构" class="headerlink" title="同构"></a>同构</h3><ul>
<li><p>当f是A到B的一个同态</p>
<ul>
<li>f为满射函数为满同态</li>
<li>f为单射函数为单同态</li>
<li>f为双射函数为同构</li>
</ul>
</li>
</ul>
<h3 id="同余关系"><a href="#同余关系" class="headerlink" title="同余关系"></a>同余关系</h3><ul>
<li>代数结构上的相等关系</li>
<li><img src="\images\image-20220311122644180.png" alt="哎呀，图片不见了" style="zoom:100%;" /></li>
</ul>
<h3 id="商集"><a href="#商集" class="headerlink" title="商集"></a>商集</h3><ul>
<li>集合A关于运算~的所有等价类作为元素组成的集合</li>
</ul>
<h3 id="自然同态"><a href="#自然同态" class="headerlink" title="自然同态"></a>自然同态</h3><ul>
<li>建立同余关系便可确定一个自然同态</li>
<li>假设R是S上一个同余关系，S&#x2F;R是对应的商半群，由f(a)&#x3D;[a]定义一个从S到S&#x2F;R的同态的满射的函数,被称为自然同态</li>
</ul>
<h3 id="同态基本定理"><a href="#同态基本定理" class="headerlink" title="同态基本定理"></a>同态基本定理</h3><ul>
<li><p>设f是S到T的一个同态，R是S上关系，对于S中的ab，a R b,当且仅当f(a)&#x3D;f(b)</p>
<ul>
<li>R是一个同余关系</li>
<li>T和商半群S&#x2F;R是同构的</li>
</ul>
</li>
<li><p>$$<br>(\bar{f}\circ f_R)(a)&#x3D;\bar{f}(f_R(a))&#x3D;\bar{f}([a])&#x3D;f(a)<br>$$</p>
</li>
<li><img src="\images\image-20220311122658572.png" alt="哎呀，图片不见了" style="zoom:100%;" /></li>
</ul>
<h2 id="计数"><a href="#计数" class="headerlink" title="计数"></a>计数</h2><h3 id="乘积法则"><a href="#乘积法则" class="headerlink" title="乘积法则"></a>乘积法则</h3><p>假设一个过程可以被分解为两个任务，如果任务一有n1种方式，任务二有n2种方式，那么完成这个过程有n1×n2种方式</p>
<p>一个有限集S的不同子集数是$2^{|S|}$</p>
<h3 id="求和法则"><a href="#求和法则" class="headerlink" title="求和法则"></a>求和法则</h3><p>如果完成任务一有n1种方式，完成任务二有n2种方式，并且这些任务不能同时执行，那么完成两个任务有n1+n2种方式</p>
<h3 id="减法法则（容斥原理）"><a href="#减法法则（容斥原理）" class="headerlink" title="减法法则（容斥原理）"></a>减法法则（容斥原理）</h3><p>如果一个任务可以通过n1种方式用方法一完成、或者通过n2种方式用方法二完成，那么执行这个任务的方法数是n1+n2减去两类方法中执行这个任务相同的方法$|A_1 \bigcup A_2|$</p>
<h3 id="除法法则"><a href="#除法法则" class="headerlink" title="除法法则"></a>除法法则</h3><p>如果一个任务能由一个可以用n种方式完成的过程实现，而对于每种完成任务的方式w，在n种方式中正好有d种与之对应，那么完成这个任务的方法数为n&#x2F;d</p>
<h3 id="鸽巢原理"><a href="#鸽巢原理" class="headerlink" title="鸽巢原理"></a>鸽巢原理</h3><ul>
<li>如果n个鸽子被分配到m个鸽巢中且m＜n，那么至少有一个鸽巢含有两个或更多的鸽子</li>
<li>推广鸽巢原理：如果N个物体放入k个盒子，那么至少有一个盒子包含了至少$\lceil N&#x2F;k\rceil$</li>
</ul>
<h3 id="排列"><a href="#排列" class="headerlink" title="排列"></a>排列</h3><p>具有n个不同元素的集合的r排列数为：$P(n,r)&#x3D;n(n-1)(n-2)\dots(n-r+1)&#x3D;\frac{n!}{(n-r)!}$</p>
<h3 id="组合"><a href="#组合" class="headerlink" title="组合"></a>组合</h3><p>具有n个不同元素的集合的r组合是从该集合中无序选取r个元素，r组合数记为C(n,r)$C(n,r)&#x3D;\frac{n!}{r!(n-r)!}$</p>
<h2 id="关系"><a href="#关系" class="headerlink" title="关系"></a>关系</h2><p>可以使用两个相关元素构成的有序对来表达两个集合元素之间的关系，A到B的二元关系是A出B的子集</p>
<h3 id="二元运算"><a href="#二元运算" class="headerlink" title="二元运算"></a>二元运算</h3><ul>
<li>结合、分配、吸收、幂等、交换、封闭</li>
</ul>
<h3 id="n元关系"><a href="#n元关系" class="headerlink" title="n元关系"></a>n元关系</h3><h4 id="投影"><a href="#投影" class="headerlink" title="投影"></a>投影</h4><p>将n元组映射到m元组，即删除了n元组的n-m个分量</p>
<h4 id="连接"><a href="#连接" class="headerlink" title="连接"></a>连接</h4><p>$J_p(R,S)$是m+n-p元关系，即将m元组的后p个分量与n元组的前p个分量相同的第一个关系中所有m元组和第二个关系的所有n元组组合起来产生的一个新的关系</p>
<h3 id="笛卡尔积"><a href="#笛卡尔积" class="headerlink" title="笛卡尔积"></a>笛卡尔积</h3><ul>
<li><p>有序对（a,b）：对象ab以指定次序出现的一个列举</p>
</li>
<li><p>笛卡尔积A×B：A与B中元素a,b的所有有序对(a,b)组成的集合</p>
</li>
<li><p>A的划分：A的非空子集的一个集合</p>
<ul>
<li>A的每个元素属于划分中的某个集合</li>
<li>如果A1和A2是划分中的不同元素，A1∩A2为空集</li>
</ul>
</li>
</ul>
<h3 id="关系R"><a href="#关系R" class="headerlink" title="关系R"></a>关系R</h3><ul>
<li><p>笛卡尔积的子集</p>
</li>
<li><p>关系矩阵M <img src="\images\image-20220311095540877.png" alt="哎呀，图片不见了" style="zoom:100%;" /></p>
</li>
<li><p>关系有向图</p>
<ul>
<li><p>道路</p>
<ul>
<li>R中从a到b长度为n的道路指：从a开始到b结束且具有$a R x_1，x_1 R x_2，\dots$的一条有限序列</li>
<li>$R^n$表示R中存在从x到y长度为n的一条道路</li>
<li>$R^{\infty}$表示R的连通关系</li>
</ul>
 <img src="\images\image-20220311095553648.png" alt="哎呀，图片不见了" style="zoom:100%;" />

<h4 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h4></li>
<li><p>自反和非自反</p>
<ul>
<li><p>自反：$对于\forall a\in A,有(a,a)\in R$</p>
<ul>
<li>Dom(R)&#x3D;Ran(R)&#x3D;A</li>
</ul>
</li>
<li><p>非自反：$对于\forall a\in A,有(a,a)\notin R$</p>
</li>
</ul>
</li>
<li><p>对称和非对称</p>
<ul>
<li><p>对称：$只要(a,b)\in R，就有(b,a)\in R$</p>
<ul>
<li>不是对称的：$\exists (a,b)\in R,但(b,a)\notin R$</li>
</ul>
</li>
<li><p>非对称：$如果(a,b)\in R，那么(b,a)\notin R$</p>
<ul>
<li>不是非对称的：$\exists (a,b)\in R且(b,a)\in R$</li>
</ul>
</li>
<li><p>反对称：$如果(a,b)\in R且(b,a)\in R，则a&#x3D;b$</p>
<ul>
<li>不是反对称的：$\exists a\neq b,(a,b)\in R,(b,a)\in R$</li>
</ul>
</li>
</ul>
</li>
<li><p>传递关系$(a,b)\in R,(b,c)\in R\rightarrow(a,c)\in R$</p>
<ul>
<li>推广：$R是传递的当且仅当对于所有n\ge 1，R^n\subseteq R$</li>
</ul>
</li>
</ul>
<h4 id="等价关系"><a href="#等价关系" class="headerlink" title="等价关系"></a>等价关系</h4><ul>
<li><p>如果一个关系是自反的、对称的、传递的</p>
</li>
<li><p>在等价关系中，如果两个元素有关联，就可以说它们是等价的</p>
</li>
<li><p>等价类</p>
<ul>
<li>与a有关系的所有元素为a的等价类，记作$[a]_R$</li>
<li>如果$b\in [a]_R$,b叫做这个等价类的代表元</li>
</ul>
</li>
<li><p>如果R是等价关系，下面三条语句是等价的：</p>
</li>
</ul>
<p>$$<br>(1)aRb;\quad (2)[a]_R\cap[b]_R\neq\empty;\quad(3)[a]_R&#x3D;[b]_R<br>$$</p>
<h4 id="划分"><a href="#划分" class="headerlink" title="划分"></a>划分</h4><p>等价类构成A的划分，它们将A分成不相交的子集。集合S的划分是S的不相交的非空子集构成的集合<br>$$<br>\bigcup_{a\in A}[a]_R&#x3D;A\<br>[a]_R\neq[b]_R时，[a]_R\cap[b]_R&#x3D;\empty<br>$$</p>
<blockquote>
<p>如果F是集合A的一个划分，那么F可以构造A上的一个等价关系</p>
</blockquote>
<h4 id="闭包"><a href="#闭包" class="headerlink" title="闭包"></a>闭包</h4><ul>
<li><p>包含关系R且满足某种性质的关系S</p>
</li>
<li><p>闭包S为所有包含R且具有性质P的关系的子集</p>
</li>
<li><p>自反闭包$R\cup \Delta  \quad(\Delta&#x3D;{(a,a)|a\in A})$</p>
</li>
<li><p>对称闭包$R\cup R^{-1}$</p>
</li>
<li><p>传递闭包$R^{\infty}$</p>
<ul>
<li>沃舍尔算法：将第k行加到第k列有1的行上</li>
</ul>
<img src="\images\image-20220311104511020.png" alt="哎呀，图片不见了" style="zoom:100%;" /></li>
</ul>
</li>
<li><p>合成$\cup^n_{i&#x3D;1}(a_{ij}\cap b_{ij})$</p>
</li>
</ul>
<h2 id="序"><a href="#序" class="headerlink" title="序"></a>序</h2><h3 id="偏序集"><a href="#偏序集" class="headerlink" title="偏序集"></a>偏序集</h3><ul>
<li><p>偏序R：自反、反对称（需要对元素进行排序）、传递（如≥、≤）</p>
</li>
<li><p>a,b可比：偏序集中每一对元素不必都是可比的</p>
</li>
<li><p>全序集：如果一个偏序集中每一对元素都是可比的</p>
</li>
<li><p>良序集：如果R是全序的，并且S的每个非空子集都有一个最小元素</p>
<ul>
<li>良序归纳原理：设S是一个良序集。如果对所有$y\in S$ $P(y)$为真，且对所有$x\in S$有$xRy$，那么$P(x)$对所有的$x\in S$为真</li>
</ul>
</li>
<li><p>偏序的有向图中没有长度比1大的环</p>
</li>
<li><p>哈塞图</p>
<ul>
<li>移除自反的环以及移除所有由于传递性必须出现的边</li>
<li>若$xRy$且不存在$z\in S$使得$xRzRy$，则称元素y覆盖元素x，y覆盖x的有序对(x,y)的集合称为偏序集上的覆盖关系</li>
</ul>
</li>
<li><p>拓扑排序</p>
<ul>
<li>相容：只要$aRb$就有$a\preccurlyeq b$则称全序$\preccurlyeq $和偏序$R$是相容的</li>
<li>构造一个全序的过程</li>
<li><img src="\images\image-20220311115219022.png" alt="哎呀，图片不见了" style="zoom:100%;" /></li>
</ul>
</li>
<li><p>极值元</p>
<ul>
<li>极大元：A中不存在比它大的元</li>
<li>极小元：A中不存在比它小的元</li>
<li>最大元：存在一个元素大于每个其他元素</li>
<li>最小元：存在一个元素小于每个其他元素</li>
<li>一个偏序集可以有多个极大元和多个极小元，而最小元和最大元是唯一的</li>
<li>上界：一个大于等于子集A中所有元素的元素</li>
<li>下界：一个小于等于子集A中所有元素的元素</li>
<li>最小上界：</li>
<li>最大下界</li>
</ul>
</li>
</ul>
<h3 id="格"><a href="#格" class="headerlink" title="格"></a>格</h3><ul>
<li>集合中任意两个元素所组成的子集都有最小上界和最大下界的一个偏序集</li>
<li>同构的格<img src="\images\image-20220311122731102.png" alt="哎呀，图片不见了" style="zoom:100%;" /></li>
</ul>
<h3 id="布尔代数"><a href="#布尔代数" class="headerlink" title="布尔代数"></a>布尔代数</h3><img src="\images\image-20220311122750308.png" alt="哎呀，图片不见了" style="zoom:100%;" />

<h2 id="图"><a href="#图" class="headerlink" title="图"></a>图</h2><h3 id="图的同构"><a href="#图的同构" class="headerlink" title="图的同构"></a>图的同构</h3><p>两个图具有完全相同的形式，从某种意义上就是两个图的顶点间存在着一一对应，这个边应保持边的对应关系。即如果存在一对一的和映上的从V1到V2的函数f，且f具有这样的性质：对V1中所有的a，b在G1中相邻当且仅当f(a)和f(b)在G2中相邻，则称G1、G2是同构的</p>
<ul>
<li>图形不变量：同构的简单图必须具有相同的顶点数、边数，并且对应顶点的度也必须相同</li>
</ul>
<h3 id="性质-1"><a href="#性质-1" class="headerlink" title="性质"></a>性质</h3><ul>
<li>握手定理：设m条边的无向图中$2m &#x3D; \sum_{v\in V}deg(v)$，图中顶点度的总和是边数的两倍</li>
<li>无向图有偶数个度为奇数的顶点</li>
<li>$\sum_{v\in V}deg^{-}(v)&#x3D;\sum_{v\in V}deg^{+}(v)&#x3D;|E|$</li>
</ul>
<h3 id="连通性"><a href="#连通性" class="headerlink" title="连通性"></a>连通性</h3><ul>
<li>通路：边的序列</li>
<li>连通分支：图的一个最大连通子图</li>
<li>割点：删除图中一个点和它关联的边后，产生比原图更多的连通分支的子图</li>
</ul>
<h3 id="欧拉通路"><a href="#欧拉通路" class="headerlink" title="欧拉通路"></a>欧拉通路</h3><ul>
<li><p>欧拉回路是包含G的每条边的简单回路</p>
</li>
<li><p>欧拉通路是包含G的每条边的简单通路</p>
</li>
<li><p>若连通图有欧拉回路，则每个顶点必有偶数度</p>
</li>
<li><p>含有至少两个顶点的连通多重图具有欧拉回路当且仅当它的每个顶点的度都为偶数</p>
</li>
<li><img src="\images\image-20220311161327975.png" alt="哎呀，图片不见了" style="zoom:100%;" /></li>
<li><p>连通多重图具有欧拉通路但无欧拉回路当且仅当它恰有两个度为奇数的点</p>
</li>
</ul>
<h3 id="哈密顿通路"><a href="#哈密顿通路" class="headerlink" title="哈密顿通路"></a>哈密顿通路</h3><ul>
<li>哈密顿通路：经过G中每一个顶点恰好一次的简单通路</li>
<li>哈密顿回路：经过G中每一个顶点恰好一次的简单回路</li>
<li>狄拉克定理：如果G是有n个顶点的简单图，其中n≥3，并且G中每个顶点的度都至少为n&#x2F;2，则G有哈密顿回路</li>
<li>欧尔定理：如果G是有n个顶点的简单图，其中n≥3，并且对于G中每一对不相邻的顶点u和v，都有deg(u)+deg(v)≥n，则G有哈密顿回路</li>
</ul>
<h3 id="最短通路"><a href="#最短通路" class="headerlink" title="最短通路"></a>最短通路</h3><img src="\images\image-20220311163044209.png" alt="哎呀，图片不见了" style="zoom:100%;" />

<img src="\images\image-20220311163101748.png" alt="哎呀，图片不见了" style="zoom:100%;" />


<h3 id="平面图"><a href="#平面图" class="headerlink" title="平面图"></a>平面图</h3><p>在平面中画出且边没有任何交叉的图</p>
<h4 id="欧拉公式"><a href="#欧拉公式" class="headerlink" title="欧拉公式"></a>欧拉公式</h4><p>$$<br>r&#x3D;e-v+2（r为平面图中面数、e为边数、v为顶点数）<br>$$</p>
<ul>
<li>若v≥3，有$e\le3v-6$</li>
<li>G有不超过5的顶点</li>
<li>若v≥3并且没有长度为3的回路，则$e\le2v-4$</li>
</ul>
<h4 id="初等细分"><a href="#初等细分" class="headerlink" title="初等细分"></a>初等细分</h4><p>若一个图是平面图，则通过删除一条边并添加一个新顶点和两条边获得的任何图也是平面图，该操作称为初等细分</p>
<h4 id="同胚"><a href="#同胚" class="headerlink" title="同胚"></a>同胚</h4><p>若相同的图通过一系列初等细分获得G1、G2，则称它们是同胚的</p>
<img src="\images\image-20220311164606633.png" alt="哎呀，图片不见了" style="zoom:100%;" />

<h4 id="库拉图斯基定理"><a href="#库拉图斯基定理" class="headerlink" title="库拉图斯基定理"></a>库拉图斯基定理</h4><p>一个图是非平面图当且仅当它包含一个同胚与$K_{3,3}或K_5$的子图</p>
<h3 id="图着色"><a href="#图着色" class="headerlink" title="图着色"></a>图着色</h3><ul>
<li><p>简单图的着色是对该图的每个顶点都指定一个颜色，使得两个相邻的顶点颜色相同</p>
</li>
<li><p>四色定理：平面图的着色不超过4</p>
</li>
</ul>
<h2 id="群与半群"><a href="#群与半群" class="headerlink" title="群与半群"></a>群与半群</h2><h3 id="半群"><a href="#半群" class="headerlink" title="半群"></a>半群</h3><ul>
<li><p>非空集合S以及一个定义在S上的可结合的二元运算*</p>
<ul>
<li>如果*是一个交换运算，则称半群为交换半群</li>
</ul>
</li>
<li><p>幺半群</p>
<ul>
<li>具有单位元的半群</li>
</ul>
</li>
<li><p>子半群</p>
<ul>
<li>T为S的一个非空子集，ab属于T，a*b也属于T</li>
</ul>
</li>
<li><p>半群的积</p>
<ul>
<li>$(s_1,t_1)<em>‘’(s_2,t_2)&#x3D;(s_1</em>s_2,t_1*’t_2)$</li>
<li>同余关系$a \ R \ a’,b \ R \ b’\rightarrow (a*b)\ R\ (a’*b’)$</li>
</ul>
</li>
<li><p>同态象</p>
<ul>
<li>f为一个满射的S到T的同态，T是S的同态象</li>
</ul>
</li>
</ul>
<h4 id="半群的同构"><a href="#半群的同构" class="headerlink" title="半群的同构"></a>半群的同构</h4><ul>
<li><p>设$(S,<em>)和(T,</em>‘)$是两个半群，如果函数$f:S\rightarrow T$是从S到T的一个<strong>一一对应</strong>（单射、满射），并且对S中的所有a和b有$f(a<em>b)&#x3D;f(a)</em>‘f(b)$，则称$f:S\rightarrow T$是从$(S,<em>)到(T,</em>‘)$的一个同构</p>
</li>
<li><img src="\images\image-20220312100013154.png" alt="哎呀，图片不见了" style="zoom:100%;" />
</li>
<li><p>如果f是从交换半群$(S,<em>)到半群(T,</em>‘)$的一个同构，分别有单位元e，e’，那么$f(e)&#x3D;e’$</p>
</li>
<li><p>如果f是从交换半群$(S,<em>)到半群(T,</em>‘)$的一个满同态，分别有单位元e，e’，那么$f(e)&#x3D;e’$</p>
</li>
<li><p>如果f是从交换半群$(S,<em>)到半群(T,</em>‘)$的一个同态，S’是S的一个子半群，那么f下S’的象是T的子半群</p>
</li>
<li><p>如果f是从交换半群$(S,<em>)到半群(T,</em>‘)$的一个满同态，那么$(T,*’)$也是交换半群</p>
</li>
</ul>
<h3 id="群G"><a href="#群G" class="headerlink" title="群G"></a>群G</h3><ul>
<li><p>具有单位元e的幺半群$a \ R \ a’,b \ R \ b’\rightarrow (a*b)\ R\ (a’*b’)$</p>
</li>
<li><p>阿贝尔群</p>
<ul>
<li>满足交换律的群</li>
</ul>
</li>
<li><p>性质</p>
<ul>
<li>G中每个元素a在G中仅有一个逆元$aa^{-1}&#x3D;a^{-1}a&#x3D;e$</li>
<li>消去性质</li>
</ul>
<p>$$<br>\begin{cases}<br>ab&#x3D;ac\Rightarrow b&#x3D;c \<br>ba&#x3D;ca\Rightarrow b&#x3D;c<br>\end{cases}<br>$$</p>
<ul>
<li>$(a^{-1})^{-1}&#x3D;a,\quad (ab)^{-1}&#x3D;b^{-1}a^{-1}$</li>
<li>ax&#x3D;b,ya&#x3D;b在G中有唯一解（a,b为G中元素）</li>
<li>阶不相等的两个群不可能是同构的</li>
</ul>
</li>
<li><p>乘法表</p>
<ul>
<li>任意行列中不能存在重复的元素，必须在空格中填e</li>
</ul>
</li>
<li><p>三角形对称群</p>
<ul>
<li><img src="\images\image-20220312100153392.png" alt="哎呀，图片不见了" style="zoom:100%;" /></li>
<li>不是阿贝尔群</li>
<li>n个元素的所有置换的集合在合成运算下是阶为n！的群，称为n个字母的对称群（$S_n$）</li>
<li>元素是集合的置换的集合</li>
<li>运算为“依次序定义”</li>
</ul>
</li>
</ul>
<h3 id="子群H"><a href="#子群H" class="headerlink" title="子群H"></a>子群H</h3><ul>
<li><p>条件</p>
<ul>
<li>包含G的幺元</li>
<li>a，b属于H，ab也属于H</li>
<li>元素a的逆元也属于H</li>
</ul>
</li>
<li><p>正规子群</p>
<ul>
<li><p>定义:对于G的所有元素a，有左陪集等于右陪集</p>
</li>
<li><p>设R是G上的一个同余关系，$H&#x3D;[e]$（包含单位元的等价类）,H为G的一个正规子群</p>
<ul>
<li>$[a]&#x3D;aH&#x3D;Ha$</li>
</ul>
</li>
<li><p>若G是任意一个群，G上的同余关系的等价类总是G的某个正规子群的陪集，反之，G的任意一个正规子群的陪集恰好是关于G上某个同余关系的等价类</p>
</li>
</ul>
</li>
<li><p>核ker(f)</p>
<ul>
<li>G上的同余关系的等价类总是G的某个正规子群的陪集</li>
<li>若f是G到$G’$的满同态，f的核为$ker(f)&#x3D;{a\in G|f(a)&#x3D;e’}$<ul>
<li>$ker(f)$是G的一个正规子群</li>
<li>商群$G&#x2F;ker(f)$与$G’$是同构的</li>
<li>$ker(f)&#x3D;[e]$</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="编码"><a href="#编码" class="headerlink" title="编码"></a>编码</h3><ul>
<li><p>概念</p>
<ul>
<li><img src="\images\image-20220312100322891.png" alt="哎呀，图片不见了" style="zoom:100%;" /></li>
<li><p>报文$B^m$</p>
<ul>
<li>一个有限字母表中字符的有限序列</li>
<li>运算$\oplus:(x_1,x_2,\dots,x_m)\oplus(y_1,y_2,\dots,y_m)&#x3D;<br>(x_1+y_1,x_2+y_2,\dots,x_m+y_m)$</li>
<li>阶数：$2^m$</li>
<li>单位元：$(0,0,……,0)$</li>
<li>每个元素都是自己 的逆元</li>
</ul>
</li>
<li><p>编码函数$e:B^m\rightarrow B^n$</p>
<ul>
<li>代码字e(b)</li>
<li>编码函数e是单射的</li>
</ul>
</li>
<li><p>Hamming距离</p>
<ul>
<li><p>x，y不相同的位置数</p>
</li>
<li><p>编码函数最短距离</p>
<ul>
<li>所有不同对代码字间距离的最小值</li>
<li>设e为一个群码，e的最短距离是非零代码字的最小权</li>
</ul>
</li>
<li><p>一个编码函数能够检测k个或更少错误当且仅当它的最短距离至少是k+1</p>
</li>
</ul>
</li>
<li><p>群码</p>
<ul>
<li><p>如果$e(B^m)&#x3D;{e(b)|e\in B^m}&#x3D;Ran(e)为B^n的一个子群$</p>
<ul>
<li>$B_n$的每个子群都是一个阿贝尔群</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>奇偶校验码H</p>
<ul>
<li><img src="\images\image-20220312100341481.png" alt="哎呀，图片不见了" style="zoom:100%;" />
</li>
<li><p>模2布尔积</p>
<ul>
<li>如果奇数个相应对包含两个1，$C_{ij}$为1；如果偶数个相应对包含两个1，$C_{ij}$为0</li>
<li><img src="\images\image-20220312100350972.png" alt="哎呀，图片不见了" style="zoom:100%;" /></li>
<li>分配性质$(D\oplus E)<em>F&#x3D;(D</em>F)\oplus(E*F)$</li>
</ul>
</li>
<li><p>同态$f_H(x)&#x3D;x*H,\quad x\in B^n$</p>
<ul>
<li>$N&#x3D;{x\in B^n|x*H&#x3D;\bar{0}}$是$Bn$的一个正规子群</li>
</ul>
</li>
<li><p>编码函数$e_H:B^m\rightarrow B^n$</p>
<ul>
<li>$x&#x3D;e_H(b)&#x3D;b_1b_2\dotsb_mx_1x_2\dots x_r$</li>
<li>设$x&#x3D;y_1y_2\dots y_mx_1\dots x_r \in B^n,那么x*H&#x3D;\bar{0}当且仅当对某个b\in B^m,x&#x3D;e_h(b)$</li>
<li>推论：$e_H(B^m)&#x3D;{e_H(b)|b\in B^m}是B^n的一个子群$</li>
</ul>
</li>
</ul>
</li>
<li><p>译码与纠错</p>
<ul>
<li><p>译码函数$d:B^n\rightarrow B^m$</p>
<ul>
<li><p>满射函数</p>
</li>
<li><p>若b为报文,有$(d\circ e)(b)&#x3D;d(e(b))&#x3D;b$</p>
</li>
<li><p>代码字x，接受字$x_t$</p>
<ul>
<li><p>$x_t$的左陪集$x_t\oplus N&#x3D;{\varepsilon_1,\varepsilon_2,\dots,\varepsilon_{x^m}}\quad(\varepsilon_i&#x3D;x_t\oplus x^{(i)})$</p>
<ul>
<li><p>$x_t$到代码字的距离恰好为$\varepsilon_i$的权</p>
</li>
<li><p>$\varepsilon_i$是有最小权的陪集成员是$x^{(i)}$是最接近$x_t$的代码字</p>
</li>
<li><p>一个有最小权的元素称作陪集首部</p>
<ul>
<li>不一定是唯一的</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>最大似然方法</p>
<ul>
<li><p>根据编码函数e确定译码函数d</p>
</li>
<li><p>步骤</p>
<ul>
<li><p>确定$B_n$中$N&#x3D;e(B_m)$的所有左陪集</p>
</li>
<li><p>对于每个陪集，求陪集首部（最小权的字）</p>
</li>
<li><p>确定接受字$x_t$属于N的陪集</p>
<ul>
<li>$B_n$中存在$2^n&#x2F;2^m&#x3D;2^r个N的不同陪集$</li>
<li>N的陪集形成$B_n$的一个划分</li>
</ul>
</li>
</ul>
</li>
<li><p>由3得到的陪集首部</p>
<ul>
<li>计算$x&#x3D;x_t\oplus \varepsilon$,若$x&#x3D;e(b),那么d(x_t)&#x3D;b$，则$x_t$的译码为b</li>
</ul>
</li>
</ul>
</li>
<li><p>陪集表（译码表）</p>
<ul>
<li><p>译码过程</p>
<ul>
<li><p>当一个字$x_t$被接收，确定包含它的行，求该行陪集首部并且把它与$x_t$相加，即可得到最接近$x_t$的代码字</p>
</li>
<li><p>构造</p>
<ul>
<li>第一行为N，其余行为不属于N的其他元素与N异或(N为代码字集合)</li>
</ul>
</li>
</ul>
</li>
<li><p>简化形式</p>
<ul>
<li><p>x*H为x的校验子</p>
<ul>
<li><p>计算每个陪集首部的校验子</p>
<ul>
<li>计算接受字的校验子</li>
</ul>
</li>
<li><p>求接受字所在陪集</p>
<ul>
<li>计算$x&#x3D;x_t\oplus \varepsilon$,若$x&#x3D;e(b),那么d(x_t)&#x3D;b$，则$x_t$的译码为b</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>Cooper</title>
    <url>/2022/03/25/Cooper/</url>
    <content><![CDATA[<p>论文：<a href="https://ieeexplore.ieee.org/abstract/document/8885377/">https://ieeexplore.ieee.org/abstract/document/8885377/</a></p>
<span id="more"></span>
<h4 id="前人的工作"><a href="#前人的工作" class="headerlink" title="前人的工作"></a>前人的工作</h4><p>数据融合可以分为数据级融合、特征级融合、对象级融合。数据级融合就是你将没有经过任何处理的原始数据进行融合，特征融合是先将原始数据的特征提取出来再进行融合，对象级融合将每辆车的检测结果进行融合。对象级融合只依赖于单一车辆的传感器，只有当两辆车共享一个参考对象时才会起作用。因此这并不能解决发现之前未被检测到的物体。</p>
<h4 id="论文概述"><a href="#论文概述" class="headerlink" title="论文概述"></a>论文概述</h4><p>本篇文献是使用数据级融合技术。这首先要解决两个问题，车辆间共享数据的格式、用于融合的数据总量。同时由于接收到的数据是在不同的位置和角度拍摄的，因此车辆需要对接受到的数据进行<strong>选择与重构</strong>。</p>
<h5 id="数据选择"><a href="#数据选择" class="headerlink" title="数据选择"></a>数据选择</h5><p>首先，需要了解哪种类型的感知数据适合于合作感知，接着使用SPOD方法克服点云过于稀疏而无法检测到目标的缺点，将两个传感器的数据优先用于协同传感。由于激光点云带有位置信息，所以优先选择激光点云数据。而由于在感知系统中，图像和点云数据是对齐的，所以在某些情况下，还需要在协同感知的过程中提取图像数据。</p>
<h5 id="数据重构"><a href="#数据重构" class="headerlink" title="数据重构"></a>数据重构</h5><p>通过对点云数据的合并，可以直观的重建局部环境。重建局部环境过程中，车辆交换信息的包由激光雷达传感器安装信息和其GPS读取信息组成，确定每一帧点云的中心点位置。车辆的IMU（惯性测量单元）也是必要的，用来表示车辆的朝向α、倾角β、翻滚角γ度等。旋转矩阵R表示为<br>$$<br>R&#x3D;R_z(\alpha)R_y(\beta)R_x(\gamma)\<br>R_z(\alpha)&#x3D;\begin{bmatrix} \cos \alpha &amp; -\sin\alpha &amp; 0 \ \sin\alpha &amp; \cos\alpha &amp; 0\0&amp;0&amp;1 \end{bmatrix}\quad R_y(\beta)&#x3D;\begin{bmatrix} \cos \beta &amp; 0&amp; \sin\beta  \0&amp;1&amp;0 \ -\sin\beta  &amp; 0&amp; \cos\beta\end{bmatrix}\quad R_x(\gamma)&#x3D;\begin{bmatrix} 1&amp;0&amp;0\ 0 &amp;\cos\gamma &amp; -\sin\gamma \0&amp; \sin \gamma &amp; \cos\gamma\end{bmatrix}<br>$$<br>当连通车辆交换信息时，协同感知系统根据下式生成新的一帧(此前需要对传来的数据进行一定的转换，转换到同一坐标系下)<br>$$<br>\begin{bmatrix} X\Y\Z \end{bmatrix}&#x3D;\begin{bmatrix} X_R\Y_R\Z_R \end{bmatrix}\bigcup\begin{bmatrix} X_R’\Y_R’\Z_R’\end{bmatrix}<br>$$</p>
<h5 id="SPOD"><a href="#SPOD" class="headerlink" title="SPOD"></a>SPOD</h5> <img src="\images\image-20220127125511528.png" alt="哎呀，图片不见了" style="zoom:100%;" />

<p>作者选择16束激光雷达，既可以输出稀疏数据，并且与32束64束相比具有价格优势。借助SECOND网络思想，作者提出SPOD网络，一个适用于低密度点云数据的目标检测算法。</p>
<p>点数据参数化表示为x,y,z与反射率r。</p>
<ol>
<li>在预处理模块，为了得到更紧凑的表示，使用<a href="https://ieeexplore.ieee.org/abstract/document/8462926/%E6%96%B9%E6%B3%95%E5%B0%86%E7%82%B9%E4%BA%91%E6%8A%95%E5%BD%B1%E5%88%B0%E7%90%83%E4%BD%93%E4%B8%8A%EF%BC%8C%E7%94%9F%E6%88%90%E5%AF%86%E9%9B%86%E8%A1%A8%E7%A4%BA%E3%80%82">https://ieeexplore.ieee.org/abstract/document/8462926/方法将点云投影到球体上，生成密集表示。</a></li>
<li>voxel特征提取模块，以点云数据作为输入，将提取的voxel-wise特征输入到voxel特征编码层。然后使用稀疏卷积中间层，大大减少卷积过程的计算开销。</li>
<li>RPN模块使用SSD结构，将卷积层输出的特征图作为输入，并连接成一个特征映射进行预测。</li>
</ol>
<h4 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h4> <img src="\images\image-20220128203821445.png" alt="哎呀，图片不见了" style="zoom:100%;" />


<h5 id="T-amp-J数据集"><a href="#T-amp-J数据集" class="headerlink" title="T&amp;J数据集"></a>T&amp;J数据集</h5><p>T&amp;J数据集是作者构建的适合于车辆协作的数据集。车辆传感器有：两个前景摄像头、四个四周鱼眼摄像头、一个惯性GPS传感器、一个前景120°雷达、一个V elodyne VLP-16 360°激光雷达、一个英伟达PX2.</p>
<p>在测试中，使用GeForce GTX 1080 Ti GPU[1]在计算机上执行用于3D汽车检测的SPOD模型。融合数据使用5毫米，一个很小检测时间增加就可以带来检测数量和准确率上很大提升。</p>
<h5 id="融合的鲁棒性"><a href="#融合的鲁棒性" class="headerlink" title="融合的鲁棒性"></a>融合的鲁棒性</h5><p>为了解决位置偏移对于检测结果的影响，作者对GPS读数进行了程序性的人工倾斜。</p>
<ul>
<li>将x和y坐标倾斜到已知GPS位置偏移的最大范围</li>
<li>只倾斜一个轴到GPS偏移的极限</li>
<li>通过将GPS最大偏移量加倍来模拟异常情况</li>
</ul>
<p>这种倾斜会使得一部分结果提升分数，但是也会使得两个实例检测失败。</p>
<h5 id="网络要求"><a href="#网络要求" class="headerlink" title="网络要求"></a>网络要求</h5><p>作者使用一种策略提取ROI来减少传输时的数据量，诸如建筑、树木等背景数据都被减去，因为这些信息可以在每辆车自己构建出来。并且传输率为每秒1帧。<br> <img src="\images\image-20220128212610332.png" alt="哎呀，图片不见了" style="zoom:100%;" /></p>
<ul>
<li>情况1下，两辆车已经非常接近。此时，车辆需要尽可能多的信息，因此会传输激光雷达下所有数据，每车每帧1.8Mb。</li>
<li>情况2下，车辆横向靠近彼此，就像在路口一样。这种情况下，ROI通常是司机视角，120°是最小要求，并且两辆车都需要交换这些信息。</li>
<li>情况3下，后车需要知道前车的视野，这种情况需要交换的信息量最少</li>
</ul>
<h4 id="做出的贡献"><a href="#做出的贡献" class="headerlink" title="做出的贡献"></a>做出的贡献</h4><ol>
<li>提出稀疏点云目标检测SPOD方法来在低密度点云数据中检测目标</li>
<li>展示数据融合技术在感知范围与提升检测准确率上面的突出表现</li>
<li>展示了利用现有的车辆网络，促进激光雷达数据在车辆间的传输，实现协同感知是可行</li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>F-COOPER</title>
    <url>/2022/03/25/F-COOPER/</url>
    <content><![CDATA[<p>论文：<a href="https://dl.acm.org/doi/abs/10.1145/3318216.3363300">F-cooper | Proceedings of the 4th ACM&#x2F;IEEE Symposium on Edge Computing</a></p>
<span id="more"></span>
<h4 id="前人的工作"><a href="#前人的工作" class="headerlink" title="前人的工作"></a>前人的工作</h4><h5 id="卷积特征图"><a href="#卷积特征图" class="headerlink" title="卷积特征图"></a>卷积特征图</h5><p>在目标检测领域，目前流行是使用CNN网络提取特征。首先将原始数据作为输入送入带有filter的卷积层，每个filter产生一个特征图。生成的特征图会递归作为下一层的输入。</p>
<h5 id="特征融合"><a href="#特征融合" class="headerlink" title="特征融合"></a>特征融合</h5><p>由于CNN网络生成的特征图可解释性较差，在使用特征图时，需要确保处理一致性的问题。即将其他代理生成的特征图转换为自己所用时，需要付出一定的运行成本。</p>
<h6 id="融合特点"><a href="#融合特点" class="headerlink" title="融合特点"></a>融合特点</h6><p>FPN与RCNN的成功说明了融合不同层生成的特征图检测目标是可行的。同时要保证所有提取出来的特征图都是以相同的格式和数据类型获得的。接下来，由于从3D点云中提取的特征图也包含位置数据，所以只要不同的自动驾驶汽车之间存在单一的重叠点，我们就可以对不同的自动驾驶汽车的特征图进行融合。在融合的过程中，需要调整融合算法来适应等距位置对齐的问题。具体来说就是让每辆车发送其GPS和IMU数据，以便进行点云融合的转换计算，也就是说，将发送者看到的视图转换为接收者看到的视图。</p>
<h6 id="压缩与传输"><a href="#压缩与传输" class="headerlink" title="压缩与传输"></a>压缩与传输</h6><p>不同车辆由于传感器的不同获取的原始数据有可能不同，而将原始数据使用CNN网络处理，多余的数据将被过滤，只留下对于目标检测有用的信息。同时，数据量也将大量减少。对于一个典型的LiDAR传感器来说，每一帧LiDAR包含大约100,000个点，约为4MB。如此庞大的数据量对于任何现有的无线网络基础设施来说都是一个严重的负担。与大量的原始LiDAR数据形成鲜明对比的是，在使用压缩技术后，CNN生成的特征大小可以低至200 Kb。通过无损压缩，数据大小的优势可以进一步增强。加上稀疏矩阵的特性，可以实现不超过1MB的特征数据压缩</p>
<h6 id="一般和内在属性"><a href="#一般和内在属性" class="headerlink" title="一般和内在属性"></a>一般和内在属性</h6><p>所有的自动驾驶汽车都必须根据传感器产生的数据做出决定。原始数据由车辆上的物理传感器生成，然后转发到车载计算设备。然后，原始数据通过基于CNN的深度学习网络进行处理，最终做出驾驶决策。</p>
<p>由于到目前为止几乎所有已知的自动驾驶车辆都使用基于CNN的网络，因此特征提取是通用的，在融合前不需要进一步处理。</p>
<h4 id="论文概述"><a href="#论文概述" class="headerlink" title="论文概述"></a>论文概述</h4><p>相互联系的自动驾驶汽车CAV可以为提升道路安全提供一种新的解决方案，单车传感器受限于视野和范围的限制，往往无法为自动驾驶提供足够的安全保障，搭配边缘计算的多代理数据融合技术可以实现更好的情况感知，并且特征级的数据融合可以在低带宽的情况下实现实时的数据分析，这可以缓解数据融合中存在的数据量过大与计算性能要求较高的困境。</p>
<p>自动驾驶汽车边缘计算系统由三层组成:车辆、边缘和云。每辆自动驾驶汽车都配备了车载边缘设备，这些设备集成了自动驾驶所需的功能模块，包括定位、感知、路径规划、车辆控制等。自动驾驶汽车可以与路边的边缘服务器通信，并最终通过无线网络到达云端。正是在这里，数据被融合和处理，以进一步扩大每个车辆的感知范围;超出视线范围和视野范围。</p>
<p>本篇论文提出的基于特征的合作感知框架是端到端的特征级数据融合，并且提出两个不同的融合方案：voxel特征融合、空间特征融合。二者的检测性能都要强于单源目标检测。虽然空间特征融合数据可以动态调整压缩大小，比体素特征融合数据更小，但后者能够在原始数据级融合的水平上改进检测，每种策略都有其独到的优势。<br><img src="\images\image-20220122114435549.png" alt="哎呀，图片不见了" style="zoom:100%;" /></p>
<h5 id="voxel特征融合（VFF）"><a href="#voxel特征融合（VFF）" class="headerlink" title="voxel特征融合（VFF）"></a>voxel特征融合（VFF）</h5><p>将点云数据分割为一个一个的voxel，每个voxel至少含有1给点，至多含有35个点，再通过VoxelNet的VFE层生成voxel特征（128维）。即所有的非空voxel都通过一系列完整的连接层转换为一个长度为128的固定大小向量（特征图）。</p>
<p>为了使得计算和内存使用更加高效，作者将非空voxel的特征都保存到一个哈希表中，其中voxel的坐标被用作哈希键。对于自动驾驶的实际场景，一般会保存几千个voxel，这样的计算开销是可以接受的。</p>
<p>在VFF中，将两辆车的voxel特征进行融合。虽然两辆车位于不同的物理位置，但它们共享相同的校准3D空间，不同的偏移量表示每辆车在3D校准空间中的相对物理位置。最终使用element-wise maxout方案（从两个特征的特征向量中每个维度只取最大值）来融合同一位置上的特征。这种方法可以提取明显的特征，同时抑制3D空间中不利于检测的特征，从而实现更小的数据量。此外，由于maxout是一个简单的浮点运算，它没有引入额外的参数。</p>
<p>但是在现实中，即使出现了voxel间微小的偏差也会使得检测结果明显的不匹配。</p>
<h5 id="空间特征融合（SFF）"><a href="#空间特征融合（SFF）" class="headerlink" title="空间特征融合（SFF）"></a>空间特征融合（SFF）</h5><img src="\images\image-20220123165217177.png" alt="哎呀，图片不见了" style="zoom:100%;" />


<p>与VFF不同的是，通过对每辆车的体素特征进行预处理，得到空间特征。然后将两个源空间特征进行融合，并将融合后的空间特征转发到RPN中进行区域建议和目标检测。</p>
<p>通过特征学习网络生成激光雷达框架的空间特征图，其输出是一个稀疏张量，尺寸为128×10×400×352.为了融合所有voxel特征，通过三层三维卷积层处理后大小为64×2×400×352,将其重塑为128×400×352后再发送给RPN。在SFF中检测范围要比特征图大小要大。然后，在不重叠区域保留原有特征的情况下，对重叠区域进行融合。SFF也采用maxout来融合重叠的空间特征。最终在RPN上找出目标区域。</p>
<h5 id="融合特征进行目标检测"><a href="#融合特征进行目标检测" class="headerlink" title="融合特征进行目标检测"></a>融合特征进行目标检测</h5><p>为了检测车辆，将合成的特征映射提供给区域建议网络(RPN)来提出目标建议。然后利用损失函数对网络进行训练。</p>
<h6 id="RPN网络"><a href="#RPN网络" class="headerlink" title="RPN网络"></a>RPN网络</h6><p>经过RPN网络后，可以得到一个由损失函数产生的两个输出：一个预测得分p，以及建议区域P参数化表示</p>
<h6 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h6><p>设G为中心的box<br>$$<br>\Delta x&#x3D;\frac{G_x-P_x}{P_d},<br>\Delta y&#x3D;\frac{G_y-P_y}{P_d},<br>\Delta z&#x3D;\frac{G_z-P_z}{P_h}\quad(P_d&#x3D;\sqrt{(P_l)^2+(P_w)^2});\<br>\Delta l&#x3D;\log{\frac{G_l}{P_l}},<br>\Delta w&#x3D;\log{\frac{G_w}{P_w}},<br>\Delta h&#x3D;\log{\frac{G_h}{P_h}},<br>\Delta\theta&#x3D;G_\theta-P_\theta;<br>$$<br>损失函数：<br>$$<br>L&#x3D;\alpha\frac{1}{N_{neg}}\sum^{N_{neg}}<em>{i&#x3D;1}L</em>{cls}(p_{neg}^i,0)+\beta\frac{1}{N_{neg}}\sum^{N_{pos}}<em>{i&#x3D;1}L</em>{cls}(p_{pos}^i,1)+\frac{1}{N_{neg}}\sum^{N_{neg}}<em>{i&#x3D;1}L</em>{reg}(p^i,G^i)<br>$$<br>p为预测锚点，G为ground truth，对于分类损失和SmoothL1损失使用二元交叉熵损失</p>
<h4 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h4><img src="\images\image-20220123171816401.png" alt="哎呀，图片不见了" style="zoom:100%;" />


<p>由于KITTI数据是单车记录的，作者通过使用同一记录的不同时间段来模拟两辆车生成的数据。</p>
<p>实验表明VFF在精度上与原始数据融合相似，但是SFF对于近距离的检测表现比VFF更好。同时，VFF对于车辆位置的偏差并不如SFF那么敏感。而且对于小目标的检测而言，其更依赖于位置距离的影响，并且小目标的特征容易在提取时被过滤导致缺失。因此，需要一种新的提取特征方法来像外科手术一般提取特征，避免检测冲突。</p>
]]></content>
      <tags>
        <tag>论文阅读</tag>
        <tag>deep learning</tag>
        <tag>Object Detection</tag>
        <tag>Data Fuse</tag>
      </tags>
  </entry>
</search>
